---
view-count: 9
---

### 一、 核心逻辑
**RAG 最小闭环**：`文档分块` -> `向量化 (Embedding)` -> `向量检索 (Similarity Search)`。

### 二、 极简技术栈
- **模型**：`sentence-transformers` (all-MiniLM-L6-v2 / 90MB)
- **检索**：`FAISS` (高并发) 或 `Numpy` (单机极简)
- **解析**：`PyPDF2` (PDF), `python-docx` (Word)

### 三、 核心代码 (Minimal Implementation)
```python
from sentence_transformers import SentenceTransformer, util

# 1. 加载轻量模型
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. 准备数据 (Chunks)
chunks = ["文本片段1...", "文本片段2...", "核心语义内容..."]
chunk_embeddings = model.encode(chunks, convert_to_tensor=True)

# 3. 执行语义检索
def search(query, top_k=3):
    query_embedding = model.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, chunk_embeddings, top_k=top_k)
    return [chunks[i['corpus_id']] for i in hits[0]]
```

### 四、 选型矩阵
| 需求 | 推荐模型 | 特点 |
| :--- | :--- | :--- |
| **极致速度** | `mxbai-edge-colbert-v0` | 17MB, CPU 友好 |
| **平衡性能** | `all-MiniLM-L6-v2` | 90MB, 语义准确度高 |
| **中文强化** | `paraphrase-multilingual-MiniLM-L12-v2` | 120MB, 多语言优化 |

### 五、 关键优化
- **量化**：使用 `load_in_8bit=True` 减少 50% 内存占用。
- **持久化**：将 `chunk_embeddings` 保存为 `.npy` 文件，避免重复编码。
- **分块策略**：`Chunk Size: 300-500` 字符，`Overlap: 10%` 保持上下文。

### 六、 适用场景
- **隐私优先**：完全离线，不经过任何 API。
- **资源受限**：老旧 PC、嵌入式设备。
- **垂直检索**：个人笔记、公司手册、论文库。