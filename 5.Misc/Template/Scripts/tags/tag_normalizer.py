#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡ç­¾è§„èŒƒåŒ–è„šæœ¬
ç”¨äºæ‰¹é‡æ›¿æ¢ Obsidian ç¬”è®°ä¸­çš„æ ‡ç­¾

GitHub Issue #1: https://github.com/duanbiao2000/obsidianDoc/issues/1

ä½¿ç”¨æ–¹æ³•:
    python tag_normalizer.py --dry-run           # é¢„è§ˆæ¨¡å¼ï¼ˆä¸ä¿®æ”¹æ–‡ä»¶ï¼‰
    python tag_normalizer.py --phase invalid    # åˆ é™¤æ— æ•ˆæ ‡ç­¾
    python tag_normalizer.py --phase high       # æ›¿æ¢é«˜é¢‘æ ‡ç­¾
    python tag_normalizer.py --phase all        # æ‰§è¡Œæ‰€æœ‰æ›¿æ¢
"""

import re
import json
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Tuple

# Windows ç¼–ç æ”¯æŒ
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ==================== é…ç½®åŒº ====================

# ç¬”è®°ç›®å½•åˆ—è¡¨
NOTE_DIRECTORIES = [
    "0.DailyNotes",
    "1.Projects",
    "2.Topics",
    "3.Resources",
    "4.Archives",
    "6.Calendar",
]

# ==================== æ ‡ç­¾æ˜ å°„è¡¨ ====================

# Phase 1: åˆ é™¤æ— æ•ˆ/å ä½ç¬¦æ ‡ç­¾
INVALID_TAGS = [
    "#Domain/<SubDomain>",
    "#Status/<State>",
    "#Type/<ContentType>",
    "#Domain/",
    "#Type/",
    "#Status/",
    "#6f9",
    "#333",
    "#SpecDriven",
    "#javascriptnodejs-testing-best-practices-guide-advanced-testing-techniques",
]

# Phase 2: é«˜é¢‘æ ‡ç­¾æ˜ å°„ï¼ˆä¼˜å…ˆçº§ï¼šé«˜ï¼‰
HIGH_PRIORITY_MAPPINGS = {
    # Domain æ ‡ç­¾
    "#AI": "#Domain/AI",
    "#Domain/Cognition": "#Domain/Cognitive",

    # Status æ ‡ç­¾
    "#todo": "#Status/TODO",
    "#done": "#Status/Done",

    # Type æ ‡ç­¾
    "#note": "#Type/Note",
    "#Project": "#Type/Project",
    "#MOC": "#Type/MOC",
    "#reference": "#Type/Reference",
    "#permanent-note": "#Type/Note",
}

# Phase 3: ä¸­é¢‘æ ‡ç­¾æ˜ å°„ï¼ˆä¼˜å…ˆçº§ï¼šä¸­ï¼‰
MEDIUM_PRIORITY_MAPPINGS = {
    # Domain æ ‡ç­¾
    "#Domain/CognitiveSystem": "#Domain/Cognitive",
    "#Domain/ContentCreation": "#Domain/Content",
    "#SubDomain/IELTS": "#Domain/Language/IELTS",
    "#ContentCreation": "#Domain/Content",
    "#card": "#Type/Card",
}

# Phase 4: ä½é¢‘æ ‡ç­¾æ˜ å°„ï¼ˆä¼˜å…ˆçº§ï¼šä½ï¼‰
LOW_PRIORITY_MAPPINGS = {
    # Domain æ ‡ç­¾
    "#Python": "#Domain/Technology/Python",
    "#Architecture": "#Domain/TechStack/SystemDesign",
    "#SystemDesign": "#Domain/TechStack/SystemDesign",
    "#Tech/AI": "#Domain/AI",
    "#OpenSource": "#Domain/Technology/OpenSource",
    "#Domain/Psychology": "#Domain/Cognitive/Psychology",
    "#Domain/MentalModel": "#Domain/Cognitive/MentalModel",
    "#Concurrency": "#Domain/TechStack/Concurrency",
    "#CloudNative": "#Domain/TechStack/CloudNative",
    "#CareerPlanning": "#Domain/Career/Planning",
    "#EngineeringMindset": "#Domain/Career/Engineering",
}

# åˆå¹¶æ‰€æœ‰æ˜ å°„
ALL_MAPPINGS = {
    **HIGH_PRIORITY_MAPPINGS,
    **MEDIUM_PRIORITY_MAPPINGS,
    **LOW_PRIORITY_MAPPINGS,
}

# ==================== å·¥å…·å‡½æ•° ====================

def create_backup_path(file_path: Path) -> Path:
    """åˆ›å»ºå¤‡ä»½æ–‡ä»¶è·¯å¾„"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return file_path.with_suffix(f".md.{timestamp}.bak")


def normalize_tags_in_content(content: str, phase: str = "all") -> Tuple[str, Dict]:
    """
    è§„èŒƒåŒ–å†…å®¹ä¸­çš„æ ‡ç­¾

    Args:
        content: æ–‡ä»¶å†…å®¹
        phase: æ‰§è¡Œé˜¶æ®µ ('invalid', 'high', 'medium', 'low', 'all')

    Returns:
        (å¤„ç†åçš„å†…å®¹, ç»Ÿè®¡ä¿¡æ¯)
    """
    original_content = content
    stats = {
        "deleted_tags": 0,
        "replaced_tags": 0,
        "modifications": [],
    }

    # Phase 1: åˆ é™¤æ— æ•ˆæ ‡ç­¾
    if phase in ["invalid", "all"]:
        for tag in INVALID_TAGS:
            pattern = r'\b' + re.escape(tag) + r'\b'
            matches = re.findall(pattern, content)
            if matches:
                content = re.sub(pattern, '', content)
                stats["deleted_tags"] += len(matches)
                stats["modifications"].append({
                    "action": "delete",
                    "tag": tag,
                    "count": len(matches),
                })

    # Phase 2: é«˜é¢‘æ ‡ç­¾æ›¿æ¢
    if phase in ["high", "all"]:
        for old_tag, new_tag in HIGH_PRIORITY_MAPPINGS.items():
            pattern = r'\b' + re.escape(old_tag) + r'\b'
            matches = re.findall(pattern, content)
            if matches:
                content = re.sub(pattern, new_tag, content)
                stats["replaced_tags"] += len(matches)
                stats["modifications"].append({
                    "action": "replace",
                    "old_tag": old_tag,
                    "new_tag": new_tag,
                    "count": len(matches),
                })

    # Phase 3: ä¸­é¢‘æ ‡ç­¾æ›¿æ¢
    if phase in ["medium", "all"]:
        for old_tag, new_tag in MEDIUM_PRIORITY_MAPPINGS.items():
            pattern = r'\b' + re.escape(old_tag) + r'\b'
            matches = re.findall(pattern, content)
            if matches:
                content = re.sub(pattern, new_tag, content)
                stats["replaced_tags"] += len(matches)
                stats["modifications"].append({
                    "action": "replace",
                    "old_tag": old_tag,
                    "new_tag": new_tag,
                    "count": len(matches),
                })

    # Phase 4: ä½é¢‘æ ‡ç­¾æ›¿æ¢
    if phase in ["low", "all"]:
        for old_tag, new_tag in LOW_PRIORITY_MAPPINGS.items():
            pattern = r'\b' + re.escape(old_tag) + r'\b'
            matches = re.findall(pattern, content)
            if matches:
                content = re.sub(pattern, new_tag, content)
                stats["replaced_tags"] += len(matches)
                stats["modifications"].append({
                    "action": "replace",
                    "old_tag": old_tag,
                    "new_tag": new_tag,
                    "count": len(matches),
                })

    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆåˆ é™¤æ ‡ç­¾åå¯èƒ½äº§ç”Ÿçš„ç©ºè¡Œï¼‰
    content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)

    return content, stats


def process_file(file_path: Path, phase: str = "all", dry_run: bool = False) -> Dict:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        phase: æ‰§è¡Œé˜¶æ®µ
        dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼

    Returns:
        å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        content = file_path.read_text(encoding='utf-8')
    except Exception as e:
        return {"error": str(e)}

    new_content, stats = normalize_tags_in_content(content, phase)

    # å¦‚æœæ²¡æœ‰ä¿®æ”¹ï¼Œè¿”å›ç©ºç»Ÿè®¡
    if not stats["modifications"]:
        return {}

    # å¦‚æœæ˜¯é¢„è§ˆæ¨¡å¼ï¼Œåªè¿”å›ç»Ÿè®¡ä¿¡æ¯
    if dry_run:
        return stats

    # åˆ›å»ºå¤‡ä»½
    backup_path = create_backup_path(file_path)
    try:
        file_path.rename(backup_path)
    except Exception as e:
        return {"error": f"å¤‡ä»½å¤±è´¥: {str(e)}"}

    # å†™å…¥æ–°å†…å®¹
    try:
        file_path.write_text(new_content, encoding='utf-8')
    except Exception as e:
        # å¦‚æœå†™å…¥å¤±è´¥ï¼Œæ¢å¤å¤‡ä»½
        backup_path.rename(file_path)
        return {"error": f"å†™å…¥å¤±è´¥: {str(e)}"}

    return stats


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ ‡ç­¾è§„èŒƒåŒ–è„šæœ¬ - GitHub Issue #1",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s --dry-run           # é¢„è§ˆæ¨¡å¼ï¼ˆä¸ä¿®æ”¹æ–‡ä»¶ï¼‰
  %(prog)s --phase invalid    # åˆ é™¤æ— æ•ˆæ ‡ç­¾
  %(prog)s --phase high       # æ›¿æ¢é«˜é¢‘æ ‡ç­¾
  %(prog)s --phase all        # æ‰§è¡Œæ‰€æœ‰æ›¿æ¢
        """
    )

    parser.add_argument(
        "--phase",
        choices=["invalid", "high", "medium", "low", "all"],
        default="all",
        help="æ‰§è¡Œé˜¶æ®µ (é»˜è®¤: all)"
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿®æ”¹æ–‡ä»¶"
    )

    parser.add_argument(
        "--dir",
        choices=NOTE_DIRECTORIES,
        help="åªå¤„ç†æŒ‡å®šç›®å½•ï¼ˆé»˜è®¤å¤„ç†æ‰€æœ‰ç›®å½•ï¼‰"
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"
    )

    args = parser.parse_args()

    # ç¡®å®šè¦å¤„ç†çš„ç›®å½•
    if args.dir:
        directories = [Path(args.dir)]
    else:
        directories = [Path(d) for d in NOTE_DIRECTORIES]

    # ç»Ÿè®¡ä¿¡æ¯
    total_stats = {
        "total_files": 0,
        "updated_files": 0,
        "deleted_tags": 0,
        "replaced_tags": 0,
        "errors": [],
    }

    print(f"ğŸš€ å¼€å§‹æ ‡ç­¾è§„èŒƒåŒ– (Phase: {args.phase})")
    print(f"ğŸ“ å¤„ç†ç›®å½•: {[str(d) for d in directories]}")
    print(f"ğŸ” é¢„è§ˆæ¨¡å¼: {'æ˜¯' if args.dry_run else 'å¦'}")
    print("=" * 60)

    # å¤„ç†æ¯ä¸ªç›®å½•
    for note_dir in directories:
        if not note_dir.exists():
            print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„ç›®å½•: {note_dir}")
            continue

        print(f"\nğŸ“‚ æ‰«æç›®å½•: {note_dir}")

        # æŸ¥æ‰¾æ‰€æœ‰ Markdown æ–‡ä»¶
        for md_file in note_dir.rglob("*.md"):
            total_stats["total_files"] += 1

            # å¤„ç†æ–‡ä»¶
            result = process_file(md_file, phase=args.phase, dry_run=args.dry_run)

            # å¤„ç†é”™è¯¯
            if "error" in result:
                total_stats["errors"].append({
                    "file": str(md_file),
                    "error": result["error"],
                })
                if args.verbose:
                    print(f"âŒ é”™è¯¯: {md_file}")
                    print(f"   {result['error']}")
                continue

            # å¦‚æœæ²¡æœ‰ä¿®æ”¹ï¼Œè·³è¿‡
            if not result:
                continue

            total_stats["updated_files"] += 1
            total_stats["deleted_tags"] += result.get("deleted_tags", 0)
            total_stats["replaced_tags"] += result.get("replaced_tags", 0)

            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            if args.verbose:
                print(f"âœ… {md_file}")
                for mod in result["modifications"]:
                    if mod["action"] == "delete":
                        print(f"   åˆ é™¤: {mod['tag']} ({mod['count']}æ¬¡)")
                    else:
                        print(f"   æ›¿æ¢: {mod['old_tag']} â†’ {mod['new_tag']} ({mod['count']}æ¬¡)")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ‰«ææ–‡ä»¶: {total_stats['total_files']}")
    print(f"  æ›´æ–°æ–‡ä»¶: {total_stats['updated_files']}")
    print(f"  åˆ é™¤æ ‡ç­¾: {total_stats['deleted_tags']} æ¬¡")
    print(f"  æ›¿æ¢æ ‡ç­¾: {total_stats['replaced_tags']} æ¬¡")

    if total_stats['errors']:
        print(f"  é”™è¯¯æ•°é‡: {len(total_stats['errors'])}")
        if args.verbose:
            print("\nâŒ é”™è¯¯è¯¦æƒ…:")
            for err in total_stats['errors']:
                print(f"  {err['file']}")
                print(f"  {err['error']}")

    # æ›´æ–°ç‡
    if total_stats['total_files'] > 0:
        update_rate = total_stats['updated_files'] / total_stats['total_files'] * 100
        print(f"  æ›´æ–°ç‡: {update_rate:.1f}%")

    print("=" * 60)

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° JSON
    stats_file = Path("tag_normalization_stats.json")
    stats_data = {
        "timestamp": datetime.now().isoformat(),
        "phase": args.phase,
        "dry_run": args.dry_run,
        "stats": total_stats,
    }
    stats_file.write_text(json.dumps(stats_data, indent=2, ensure_ascii=False), encoding='utf-8')
    print(f"ğŸ’¾ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")

    if not args.dry_run:
        print("\nâœ… å®Œæˆï¼è¯·æ£€æŸ¥ä¿®æ”¹åçš„æ–‡ä»¶ã€‚")
        print("ğŸ’¡ æç¤º: å¤‡ä»½æ–‡ä»¶ä½¿ç”¨ .bak åç¼€ï¼Œç¡®è®¤æ— è¯¯åå¯åˆ é™¤ã€‚")
    else:
        print("\nâš ï¸  é¢„è§ˆæ¨¡å¼å®Œæˆï¼Œæœªä¿®æ”¹ä»»ä½•æ–‡ä»¶ã€‚")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ --phase all æ‰§è¡Œå®é™…æ›¿æ¢ã€‚")


if __name__ == "__main__":
    main()
