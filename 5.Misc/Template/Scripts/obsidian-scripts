#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Obsidian çŸ¥è¯†åº“è‡ªåŠ¨åŒ–å·¥å…· - ç»Ÿä¸€CLIå…¥å£

æä¾›ç»Ÿä¸€çš„å‘½ä»¤è¡Œæ¥å£ï¼Œæ•´åˆæ‰€æœ‰è‡ªåŠ¨åŒ–è„šæœ¬åŠŸèƒ½ï¼š
- æ ‡ç­¾ç®¡ç†
- é“¾æ¥åˆ†æ
- å…ƒæ•°æ®éªŒè¯
- å·¥ä½œæµç¼–æ’

ä½œè€…: Claude Sonnet 4.5
åˆ›å»ºæ—¶é—´: 2026-01-25
ç›¸å…³: Issue #4 - è‡ªåŠ¨åŒ–è„šæœ¬ä½“ç³»ä¼˜åŒ–
"""

import argparse
import sys
from pathlib import Path

# å¯¼å…¥æ ¸å¿ƒåº“
from obsidian_scripts.core.encoding import auto_setup
from obsidian_scripts.core.config import Config
from obsidian_scripts.plugins.tags import TagManager
from obsidian_scripts.plugins.links import LinkAnalyzer

# è‡ªåŠ¨è®¾ç½®UTF-8ç¼–ç 
auto_setup()


def create_parser():
    """åˆ›å»ºCLIå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="ObsidiançŸ¥è¯†åº“è‡ªåŠ¨åŒ–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  æ ‡ç­¾ç®¡ç†:
    obsidian-scripts tags add-domain --dry-run
    obsidian-scripts tags cleanup --verbose
    obsidian-scripts tags normalize --phase all

  é“¾æ¥åˆ†æ:
    obsidian-scripts links find-orphans --threshold 0 --output orphans.md
    obsidian-scripts links analyze --stats
    obsidian-scripts links density

  å…ƒæ•°æ®éªŒè¯:
    obsidian-scripts validate metadata --include-domains --fix --dry-run

  å·¥ä½œæµ:
    obsidian-scripts workflow daily-maintenance
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # ==================== tags å­å‘½ä»¤ ====================
    tags_parser = subparsers.add_parser('tags', help='æ ‡ç­¾ç®¡ç†')
    tags_subparsers = tags_parser.add_subparsers(dest='tags_command')

    # tags add-domain
    add_domain_parser = tags_subparsers.add_parser('add-domain', help='æ·»åŠ Domainæ ‡ç­¾')
    add_domain_parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼')
    add_domain_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # tags cleanup
    cleanup_parser = tags_subparsers.add_parser('cleanup', help='æ¸…ç†é‡å¤æ ‡ç­¾')
    cleanup_parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼')
    cleanup_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # tags normalize
    normalize_parser = tags_subparsers.add_parser('normalize', help='æ ‡ç­¾è§„èŒƒåŒ–')
    normalize_parser.add_argument('--phase', choices=['invalid', 'high', 'medium', 'low', 'all'],
                                default='all', help='æ‰§è¡Œé˜¶æ®µ')
    normalize_parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼')
    normalize_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # ==================== links å­å‘½ä»¤ ====================
    links_parser = subparsers.add_parser('links', help='é“¾æ¥åˆ†æ')
    links_subparsers = links_parser.add_subparsers(dest='links_command')

    # links find-orphans
    orphans_parser = links_subparsers.add_parser('find-orphans', help='æŸ¥æ‰¾å­¤ç«‹ç¬”è®°')
    orphans_parser.add_argument('--threshold', type=int, default=0, help='é“¾æ¥é˜ˆå€¼ï¼ˆé»˜è®¤0ï¼‰')
    orphans_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    orphans_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # links analyze
    analyze_parser = links_subparsers.add_parser('analyze', help='åˆ†æé“¾æ¥è¿é€šæ€§')
    analyze_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # links density
    density_parser = links_subparsers.add_parser('density', help='é“¾æ¥å¯†åº¦ç»Ÿè®¡')
    density_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # ==================== validate å­å‘½ä»¤ ====================
    validate_parser = subparsers.add_parser('validate', help='å…ƒæ•°æ®éªŒè¯')
    validate_subparsers = validate_parser.add_subparsers(dest='validate_command')

    # validate metadata
    metadata_parser = validate_subparsers.add_parser('metadata', help='å…ƒæ•°æ®éªŒè¯')
    metadata_parser.add_argument('--include-domains', action='store_true', help='éªŒè¯Domainæ ‡ç­¾')
    metadata_parser.add_argument('--include-types', action='store_true', help='éªŒè¯Typeæ ‡ç­¾')
    metadata_parser.add_argument('--include-statuses', action='store_true', help='éªŒè¯Statusæ ‡ç­¾')
    metadata_parser.add_argument('--fix', action='store_true', help='è‡ªåŠ¨ä¿®å¤æ¨¡å¼')
    metadata_parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼')
    metadata_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    # ==================== workflow å­å‘½ä»¤ ====================
    workflow_parser = subparsers.add_parser('workflow', help='å·¥ä½œæµç¼–æ’')
    workflow_subparsers = workflow_parser.add_subparsers(dest='workflow_command')

    # workflow daily-maintenance
    daily_parser = workflow_subparsers.add_parser('daily-maintenance', help='æ—¥å¸¸ç»´æŠ¤å·¥ä½œæµ')
    daily_parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼')
    daily_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    return parser


def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šå‘½ä»¤ï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not args.command:
        parser.print_help()
        return

    config = Config()

    print("=" * 70)
    print(f"Obsidian çŸ¥è¯†åº“è‡ªåŠ¨åŒ–å·¥å…·")
    print(f"Vault: {config.vault_root}")
    print("=" * 70)
    print()

    # ==================== æ ‡ç­¾ç®¡ç† ====================
    if args.command == 'tags':
        tag_manager = TagManager(config)

        if args.tags_command == 'add-domain':
            print("ğŸ“Œ æ‰¹é‡æ·»åŠ  Domain æ ‡ç­¾")
            print()
            stats = tag_manager.add_domain_tags(
                dry_run=args.dry_run,
                verbose=args.verbose
            )
            print()
            print("âœ… å®Œæˆï¼")
            print(f"  å¤„ç†: {stats['processed']} ä¸ªæ–‡ä»¶")
            print(f"  è·³è¿‡: {stats['skipped']} ä¸ªæ–‡ä»¶")

        elif args.tags_command == 'cleanup':
            print("ğŸ§¹ æ¸…ç†é‡å¤æ ‡ç­¾")
            print()
            stats = tag_manager.cleanup_duplicates(
                dry_run=args.dry_run,
                verbose=args.verbose
            )
            print()
            print("âœ… å®Œæˆï¼")
            print(f"  å¤„ç†: {stats['processed']} ä¸ªæ–‡ä»¶")
            print(f"  æ¸…ç†: {stats['duplicates_removed']} ä¸ªé‡å¤æ ‡ç­¾")

        elif args.tags_command == 'normalize':
            print("ğŸ”§ æ ‡ç­¾è§„èŒƒåŒ–")
            print()
            stats = tag_manager.normalize_tags(
                phase=args.phase,
                dry_run=args.dry_run,
                verbose=args.verbose
            )
            print()
            print("âœ… å®Œæˆï¼")
            print(f"  åˆ é™¤: {stats['deleted']} ä¸ªæ— æ•ˆæ ‡ç­¾")
            print(f"  æ›¿æ¢: {stats['replaced']} ä¸ªæ ‡ç­¾")
            print(f"  ä¿®æ”¹: {stats['files_modified']} ä¸ªæ–‡ä»¶")

    # ==================== é“¾æ¥åˆ†æ ====================
    elif args.command == 'links':
        link_analyzer = LinkAnalyzer(config)

        if args.links_command == 'find-orphans':
            print("ğŸ” æŸ¥æ‰¾å­¤ç«‹ç¬”è®°")
            print()
            output_file = Path(args.output) if args.output else None
            orphans = link_analyzer.find_orphan_notes(
                link_threshold=args.threshold,
                output_file=output_file
            )
            print()
            print("âœ… å®Œæˆï¼")
            print(f"  æ‰¾åˆ° {len(orphans)} ä¸ªå­¤ç«‹ç¬”è®°ï¼ˆé“¾æ¥æ•°<={args.threshold}ï¼‰")
            if output_file:
                print(f"  æŠ¥å‘Š: {output_file}")

        elif args.links_command == 'analyze':
            print("ğŸ“Š åˆ†æé“¾æ¥è¿é€šæ€§")
            print()
            connectivity = link_analyzer.analyze_connectivity()

            # æ’åºå¹¶æ˜¾ç¤ºå‰20ä¸ª
            sorted_items = sorted(
                connectivity.items(),
                key=lambda x: len(x[1]['in_links']),
                reverse=True
            )

            print("å‰ 20 ä¸ªé«˜è¿é€šæ€§ç¬”è®°ï¼š")
            for i, (title, links) in enumerate(sorted_items[:20], 1):
                in_count = len(links['in_links'])
                out_count = len(links['out_links'])
                print(f"  {i}. {title}: {in_count} â† â†’ {out_count}")

            print()
            print("âœ… åˆ†æå®Œæˆï¼")

        elif args.links_command == 'density':
            print("ğŸ“ˆ é“¾æ¥å¯†åº¦ç»Ÿè®¡")
            print()
            stats = link_analyzer.get_link_density_stats()
            print(f"  æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
            print(f"  æ€»é“¾æ¥æ•°: {stats['total_links']}")
            print(f"  å¹³å‡é“¾æ¥æ•°/æ–‡ä»¶: {stats['avg_links_per_file']}")
            print(f"  å­¤ç«‹ç¬”è®° (0é“¾æ¥): {stats['orphan_count']}")
            print(f"  ä½è¿é€šæ€§ (1-2é“¾æ¥): {stats['low_connectivity_count']}")
            print(f"  é«˜è¿é€šæ€§ (>2é“¾æ¥): {stats['high_connectivity_count']}")
            print()
            print("âœ… ç»Ÿè®¡å®Œæˆï¼")

    # ==================== å…ƒæ•°æ®éªŒè¯ ====================
    elif args.command == 'validate':
        print("ğŸ” å…ƒæ•°æ®éªŒè¯")
        print()
        print("âš ï¸  æ­¤åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")
        print("   ç›¸å…³: Issue #3 é¡¹ç›®å…ƒæ•°æ®å®¡è®¡")

    # ==================== å·¥ä½œæµ ====================
    elif args.command == 'workflow':
        print("âš™ï¸  å·¥ä½œæµç¼–æ’")
        print()

        if args.workflow_command == 'daily-maintenance':
            print("ğŸ“… æ—¥å¸¸ç»´æŠ¤å·¥ä½œæµ")
            print()
            print("âš ï¸  æ­¤åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")
            print("   å°†åŒ…æ‹¬ï¼šæ ‡ç­¾æ¸…ç† + å­¤ç«‹ç¬”è®°æ£€æµ‹ + å…ƒæ•°æ®éªŒè¯")

    print()
    print("=" * 70)


if __name__ == "__main__":
    main()
