#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸…ç†é‡å¤æ ‡ç­¾è„šæœ¬
ä¿®å¤ batch_add_domain_tags.py é€ æˆçš„æ ‡ç­¾é‡å¤é—®é¢˜
"""

import re
import sys
from pathlib import Path
from datetime import datetime

# Windows ç¼–ç æ”¯æŒ
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ç¬”è®°ç›®å½•
NOTE_DIRS = [
    "0.DailyNotes",
    "1.Projects",
    "2.Topics",
    "3.Resources",
    "4.Archives",
    "6.Calendar",
]

def cleanup_duplicate_tags(file_path, dry_run=True):
    """
    æ¸…ç†æ–‡ä»¶ä¸­çš„é‡å¤æ ‡ç­¾

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼

    Returns:
        (æ˜¯å¦ä¿®æ”¹, ç»Ÿè®¡ä¿¡æ¯)
    """
    try:
        content = file_path.read_text(encoding='utf-8')
        original_content = content

        # æŸ¥æ‰¾ YAML tags éƒ¨åˆ†
        tags_match = re.search(r'^tags:\s*\n((?:[ \t]+-.+\n?)+)', content, re.MULTILINE)

        if not tags_match:
            return False, {}

        tags_text = tags_match.group(1)
        tags_start = tags_match.start(1)
        tags_end = tags_match.end(1)

        # æå–æ‰€æœ‰æ ‡ç­¾
        tags = re.findall(r'^[ \t]+-+(.+)$', tags_text, re.MULTILINE)

        if len(tags) <= 1:
            return False, {}

        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_tags = []
        for tag in tags:
            tag = tag.strip()
            if tag not in seen:
                seen.add(tag)
                unique_tags.append(tag)

        # å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å›
        if len(unique_tags) == len(tags):
            return False, {}

        # ç”Ÿæˆæ–°çš„ tags æ–‡æœ¬
        indent = '  '
        new_tags_text = '\n'.join([f"{indent}- {tag}" for tag in unique_tags]) + '\n'

        # æ›¿æ¢å†…å®¹
        new_content = content[:tags_start] + new_tags_text + content[tags_end:]

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_tags': len(tags),
            'unique_tags': len(unique_tags),
            'duplicates_removed': len(tags) - len(unique_tags),
            'tags_list': unique_tags
        }

        if not dry_run:
            # åˆ›å»ºå¤‡ä»½
            backup_path = file_path.with_suffix('.md.dup_bak')
            if not backup_path.exists():
                file_path.rename(backup_path)

            # å†™å…¥æ–°å†…å®¹
            file_path.write_text(new_content, encoding='utf-8')

        return True, stats

    except Exception as e:
        print(f"é”™è¯¯å¤„ç† {file_path}: {e}")
        return False, {'error': str(e)}

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="æ¸…ç†é‡å¤æ ‡ç­¾",
        formatter_class=argparse.RawDescriptionHelpFormatter
 )

    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼ˆä¸ä¿®æ”¹æ–‡ä»¶ï¼‰')
    parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    args = parser.parse_args()

    repo_root = Path(r"d:\è¿…é›·ä¸‹è½½\@åŒæ­¥æ–‡ä»¶\OneDrive\obsidianDoc")

    print("=" * 70)
    print("æ ‡ç­¾é‡å¤æ¸…ç†å·¥å…·")
    print("=" * 70)
    print(f"æ¨¡å¼: {'é¢„è§ˆï¼ˆä¸ä¿®æ”¹æ–‡ä»¶ï¼‰' if args.dry_run else 'æ‰§è¡Œæ¨¡å¼ï¼ˆå°†ä¿®æ”¹æ–‡ä»¶ï¼‰'}")
    print(f"å·¥ä½œç›®å½•: {repo_root}")
    print("=" * 70)
    print()

    total_files = 0
    modified_files = 0
    total_duplicates = 0
    errors = []

    for directory in NOTE_DIRS:
        dir_path = repo_root / directory
        if not dir_path.exists():
            continue

        print(f"æ‰«æç›®å½•: {directory}")

        for md_file in dir_path.rglob("*.md"):
            total_files += 1

            modified, stats = cleanup_duplicate_tags(md_file, dry_run=args.dry_run)

            if modified and 'error' not in stats:
                modified_files += 1
                duplicates = stats['duplicates_removed']
                total_duplicates += duplicates

                if args.verbose:
                    print(f"  âœ“ {md_file.relative_to(repo_root)}")
                    print(f"    æ€»æ ‡ç­¾: {stats['total_tags']}, å”¯ä¸€: {stats['unique_tags']}, æ¸…ç†: {duplicates}")

            elif 'error' in stats:
                errors.append({'file': str(md_file), 'error': stats['error']})

    print()
    print("=" * 70)
    print("æ¸…ç†ç»Ÿè®¡:")
    print("-" * 70)
    print(f"æ‰«ææ–‡ä»¶: {total_files}")
    print(f"éœ€è¦æ¸…ç†: {modified_files}")
    print(f"æ€»é‡å¤æ¬¡æ•°: {total_duplicates}")
    print(f"æ¸…ç†ç‡: {total_duplicates}/{total_files} = {total_duplicates/total_files*100:.1f} æ¬¡/æ–‡ä»¶")

    if errors:
        print(f"é”™è¯¯: {len(errors)}")

    print("=" * 70)

    if not args.dry_run:
        print()
        print("âœ… æ¸…ç†å®Œæˆï¼")
        print("ğŸ’¡ æç¤º: å¤‡ä»½æ–‡ä»¶ä½¿ç”¨ .dup_bak åç¼€")
        print("ğŸ’¡ ç¡®è®¤æ— è¯¯åå¯åˆ é™¤å¤‡ä»½æ–‡ä»¶")
    else:
        print()
        print("âš ï¸  é¢„è§ˆæ¨¡å¼å®Œæˆï¼Œæœªä¿®æ”¹ä»»ä½•æ–‡ä»¶")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ä¸å¸¦ --dry-run çš„å‚æ•°æ‰§è¡Œå®é™…æ¸…ç†")

if __name__ == "__main__":
    main()
