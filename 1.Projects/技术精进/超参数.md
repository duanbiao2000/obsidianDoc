在[[机器学习模型]]中，[[超参数]]（Hyperparameters）是指在模型训练过程开始之前由[[机器学习工程师]]或数据科学家手动设置的参数，而非模型在训练过程中从数据中学习到的参数（例如[[线性回归]]中的系数、[[神经网络]]中的权重和偏置）。[[超参数]]直接控制着模型的学习过程、结构复杂性以及最终的性能表现。

**[[超参数]]的特性与重要性：**

1.  **预设性**：[[超参数]]在模型训练开始前就已确定，它们决定了模型将如何学习数据。
2.  **非学习性**：模型无法通过常规的优化算法（如梯度下降）从数据中学习和调整[[超参数]]的值。
3.  **影响学习过程与模型结构**：
    *   它们可以控制学习算法的行为（例如，[[学习率]]决定了模型参数更新的步长）。
    *   它们可以定义模型的架构（例如，[[神经网络]]的层数、每层的神经元数量）。
    *   它们也可以影响模型的[[正则化]]强度（例如，L1/L2[[正则化]]系数，用于防止[[过拟合]]）。
4.  **对模型性能的关键影响**：[[超参数]]的选择对模型的最终性能具有决定性影响。不恰当的[[超参数]]可能导致模型[[过拟合]]（在训练数据上表现极好，但在新数据上表现差）或欠拟合（模型未能充分学习数据中的模式），从而影响其[[泛化能力]]。

**常见的[[超参数]]示例：**

*   **[[决策树]]/[[随机森林]]**：
    *   `max_depth`：树的最大深度。
    *   `min_samples_split`：分裂一个节点所需的最小样本数。
    *   `n_estimators`：[[随机森林]]中树的数量。
*   **[[支持向量机]] (SVM)**：
    *   `C`：[[正则化]]参数，控制[[模型复杂度]]和训练误差之间的权衡。
    *   `gamma`：核函数的参数，影响单个训练样本的影响范围。
    *   `kernel`：核函数的类型（如线性、径向基函数RBF）。
*   **[[神经网络]] (Neural Networks)**：
    *   `learning_rate`：[[学习率]]，控制每次参数更新的幅度。
    *   `num_layers`：隐藏层的数量。
    *   `num_units`：每层神经元的数量。
    *   `activation_function`：激活函数的类型（如ReLU, Sigmoid）。
    *   `batch_size`：每次梯度更新使用的样本数量。
    *   `dropout_rate`：[[Dropout正则化]]的比例。
*   **[[K-Means聚类]]**：
    *   `n_clusters`：簇的数量。
    *   `init`：初始化中心的方法。

**[[超参数]]的设定方法：**

由于[[超参数]]无法自动学习，其最佳值通常需要通过实验来确定，这个过程称为[[超参数调整]]（Hyperparameter Tuning 或 Hyperparameter Optimization）。常用的调整策略包括：
*   [[网格搜索]] (Grid Search)
*   [[随机搜索]] (Random Search)
*   [[贝叶斯优化]] (Bayesian Optimization)
*   遗传算法等。

选择合适的[[超参数]]是[[机器学习模型]]成功的关键一步，也是[[机器学习工程师]]专业技能的重要体现。