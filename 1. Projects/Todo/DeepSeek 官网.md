---
aliases: 
createdAt: 2025-02-24 11:18
updateAt: 
categories:
  - Mindset
tags:
  - Mindset/Reflection
  - Tech/Code
  - Action/Writing
---
[DeepSeek](https://www.deepseek.com/)

## 关键术语解释

- **MoE (Mixture of Experts)**: 混合专家模型，一种神经网络架构，通过将不同的“专家”网络组合起来处理不同的输入，从而提高模型性能。
- **Dense**: 密集连接层，神经网络中最常见的层类型，每个神经元都与前一层的所有神经元相连。
- **Activated Params**: 被激活的参数数量，指模型在运行过程中实际参与计算的参数数量。MoE 模型通常只有一部分参数被激活。
- **F1 (F1-score)**: F1 分数，精确率 (precision) 和召回率 (recall) 的调和平均值，用于综合评估模型的性能。
- **Acc. (Accuracy)**: 准确率，指模型预测正确的比例。
- **COT (Chain-of-Thought)**: 思维链，一种提示技术，通过引导模型生成一系列中间推理步骤来提高解决复杂问题的能力。

## AI 核心概念

- **模型架构**: 指模型的网络结构，例如 MoE 或 Dense。不同的架构适用于不同的任务和数据。
- **语言理解**: 指模型理解自然语言的能力，例如 MMLU、DROP 等测试集考察的就是模型在这方面的能力。
- **GPQA-Diamond (Pass@1)**: GPQA-Diamond 可能是某个问答测试集，专注于考察模型在特定领域的知识。Pass@1 指的是模型在一次尝试中答对的比例。






