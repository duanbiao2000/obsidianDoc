---
aliases: 
date: 2025-02-24 11:18
update: 
categories:
  - Mindset
tags:
  - Mindset/Reflection
  - Tech/Code
  - Action/Writing
---
[DeepSeek](https://www.deepseek.com/)

**卡片 1**

**ID:** 202502241118A
**標籤:** #AI #模型架构 #MoE #Dense #神经网络 #效率
**標題:** **模型架构对比：混合专家 (MoE) 与密集连接 (Dense)** ^moevsdense

**內容:**
理解 AI 模型的基础是了解其架构，如同了解建筑的蓝图。常见的有 MoE 和 Dense 两种设计思路：

1.  **Dense (密集连接)**: 这是传统且基础的方式，想象一个“全员大会”，网络中的每个神经元都与上一层的所有神经元连接。
    *   **特点**: 信息充分交流，但计算量大，所有参数都可能参与每一次运算。
2.  **MoE (混合专家)**: 这更像一个“专家小组会议”。系统内部有多个“专家”（子网络），每次处理输入时，会根据任务需求，智能地选择并激活一部分专家来工作。
    *   **特点**: 并非所有参数 (Activated Params) 都需要同时激活，这使得模型在保持（甚至提升）性能的同时，可能在推理时更有效率，计算成本相对较低。其核心在于有一个“门控机制”来决定“请”哪些专家出马。

简单来说，Dense 是“人人都参与”，MoE 是“术业有专攻，按需调用”。选择哪种架构取决于任务需求、性能目标和计算资源。

**連結:** [[#^activatedparams]] [[神经网络基础]] [[模型效率优化]]

---

**卡片 2**

**ID:** 202502241118B
**標籤:** #AI #模型评估 #F1分数 #准确率 #机器学习 #评测指标
**標題:** **衡量 AI 模型表现：理解 F1 分数与准确率 (Acc.)** ^evalmetrics

**內容:**
要判断一个 AI 模型是“好学生”还是“差学生”，我们需要用尺子来量，也就是评估指标。笔记中提到了两个关键指标：

1.  **准确率 (Accuracy, Acc.)**: 这是最直观的指标，告诉你模型预测对了多少。计算方法是：**预测正确的数量 / 总数量**。
    *   **例子**: 100 道题，模型答对了 90 道，准确率就是 90%。
    *   **局限性 (费曼思考)**: 如果 99 道题是 A，1 道题是 B，模型全部猜 A，准确率高达 99%，但这能说明模型真正理解了吗？在类别不均衡时，准确率可能会有误导性。
2.  **F1 分数 (F1-score)**: 这个指标更全面，它试图平衡 **“查准率” (Precision)** 和 **“查全率” (Recall)**。
    *   **Precision**: 模型预测为“是”的结果中，有多少是真的“是”？（别冤枉好人）
    *   **Recall**: 所有真的“是”的结果中，有多少被模型找出来了？（别漏掉坏人）
    *   F1 是这两者的调和平均值，特别适用于需要同时关注“宁可错杀，不可放过”和“不可错杀一个好人”的场景，或者数据不平衡的情况。

理解这些指标，才能更客观、深入地评价一个模型在特定任务上的真实能力。

**連結:** [[精确率与召回率]] [[模型选择的标准]] [[数据不平衡问题]]

---

**卡片 3**

**ID:** 202502241118C
**標籤:** #AI #提示工程 #COT #推理 #大语言模型 #费曼学习法
**標題:** **提升 AI 推理能力：思维链 (Chain-of-Thought, COT)** ^cotprompt

**內容:**
如何引导 AI 模型像人类一样进行复杂思考，而不是仅仅给出最终答案？思维链 (COT) 是一种重要的提示 (Prompting) 技术。

1.  **核心理念**: 不直接问最终结果，而是引导模型 **展示其思考过程**，一步一步地推导出答案。如同解数学题要写步骤一样。
2.  **运作方式**: 在向模型提问时，或者在提供示例 (few-shot learning) 时，明确地包含解决问题的中间步骤和逻辑链条。
3.  **效果 (类比费曼学习法)**:
    *   **强制分解**: 迫使模型将复杂问题分解为更小、更易于管理的部分。
    *   **显化思考**: 让模型的“思考”过程可见，便于检查和纠正。
    *   **提升可靠性**: 通过模拟人类的逐步推理，减少模型直接“猜”答案的可能性，提高在逻辑、数学、多步问答等任务上的准确性。这与费曼学习法中“用简单的语言解释给别人听”能加深自身理解的原理有相似之处——输出促进理解和修正。

COT 是让 AI 从“知道答案”进化到“理解为何是这个答案”的关键一步。

**連結:** [[费曼学习法]] [[大语言模型提示技巧]] [[复杂问题分解]] [[AI的可解释性]]

---
**卡片 4**

**ID:** 202502241118D
**標籤:** #AI #模型评估 #语言理解 #基准测试 #MMLU #GPQA
**標題:** **AI 语言理解能力的基准测试** ^langbench

**內容:**
衡量 AI 模型是否真正“理解”人类语言，需要标准化的考试，也就是基准测试集 (Benchmark)。笔记中提及了几个：

1.  **通用语言理解**:
    *   **MMLU (Massive Multitask Language Understanding)**: 涵盖广泛学科知识的大规模多任务语言理解测试。
    *   **DROP (Discrete Reasoning Over Paragraphs)**: 考察模型在阅读段落后进行离散推理的能力（如计数、查找特定信息等）。
2.  **特定领域知识**:
    *   **GPQA-Diamond (Pass@1)**: 名字暗示这可能是一个高难度的研究生级别的问答测试集 (Graduate-Level Google-Proof Q&A)，专注于需要深度知识和推理的问题。"Pass@1" 指模型一次尝试就通过（答对）的比例。

这些测试集如同不同科目的考试，从不同维度考察模型的语言理解、知识储备和推理能力，是评价和比较不同 AI 模型能力的重要依据。

**連結:** [[#^evalmetrics]] [[自然语言处理(NLP)]] [[AI能力边界]]






