通过以下函数提取{activeNote}中的高熵成分,只输出结果,不做步骤解释
```python
import re
import math
from collections import defaultdict
import jieba  # 中文分词
import spacy  # 英文句法分析

# 初始化英文NLP模型
nlp = spacy.load("en_core_web_sm")

def extract_high_entropy_components(text, top_n=10):
    # 第一阶段：字符级熵计算（识别高熵片段）
    def char_entropy(text_segment):
        freq = defaultdict(int)
        for char in text_segment:
            freq[char] += 1
        entropy = 0.0
        total = len(text_segment)
        for count in freq.values():
            p = count / total
            entropy -= p * math.log2(p) if p > 0 else 0
        return entropy

    # 第二阶段：提取候选成分
    candidates = []
    
    # 中文处理：提取专业术语和长词
    chinese_words = [word for word in jieba.cut(text) if len(word) >= 3 and '\u4e00' <= word[0] <= '\u9fff']
    for word in set(chinese_words):
        candidates.append((word, char_entropy(word), "CH_TERM"))
    
    # 英文处理：提取名词短语和复杂词
    doc = nlp(re.sub(r'[^\x00-\x7F]+', ' ', text))  # 过滤非ASCII字符
    for chunk in doc.noun_chunks:
        if len(chunk.text) >= 12:  # 长名词短语
            candidates.append((chunk.text, char_entropy(chunk.text), "EN_PHRASE"))
    for token in doc:
        if token.pos_ in ("NOUN", "PROPN") and len(token.text) >= 8:  # 长专业名词
            candidates.append((token.text, char_entropy(token.text), "EN_NOUN"))
    
    # 第三阶段：筛选高熵成分
    candidates.sort(key=lambda x: x[1], reverse=True)
    high_entropy = []
    seen = set()
    for item in candidates:
        if item[0] not in seen and item[1] > 3.5:  # 熵值阈值
            high_entropy.append(item)
            seen.add(item[0])
            if len(high_entropy) >= top_n:
                break
    return high_entropy

# 示例使用
text = """您的文本内容..."""
results = extract_high_entropy_components(text)
for item in results:
    print(f"{item[0]} (熵值:{item[1]:.2f}, 类型:{item[2]})")
```