
[[../白话深度学习]]
# 基础概念：


- 书中首先介绍了深度学习的基本概念，包括神经网络、激活函数、损失函数等基础知识。

- 强调了前馈神经网络的结构和工作原理。

### 激活函数：
- 解释了激活函数的作用，如何引入非线性变换，从而使神经网络能够学习更复杂的函数。

- 常见的激活函数如sigmoid、ReLU（Rectified Linear Unit）等被介绍，并讨论它们的特点和适用场景。
### 损失函数：
- 介绍了损失函数的概念，用于衡量模型预测输出与真实标签之间的差距。

- 讨论了不同任务和模型结构下选择合适损失函数的重要性。

# 神经网络：

- 介绍了神经网络的基本结构和工作原理。神经网络是由神经元组成的层次结构，包括输入层、隐藏层和输出层。

- 强调了神经网络作为深度学习模型的基础，并通过不同层次的权重和偏差进行学习。


强调神经网络作为深度学习模型的基础，通过不同层次的权重和偏差进行学习是深度学习的核心概念。以下是关于这一概念的更详细说明：

### 神经网络学习过程：

1. **权重和偏差：**
   - 在神经网络中，权重（weights）和偏差（biases）是模型的参数。
   - 每一层的神经元都与一组权重和一个偏差相关联。
   - 权重表示输入的重要性，偏差用于调整模型的灵活性。

2. **模型学习：**
   - 在训练过程中，神经网络通过学习适应输入和目标之间的关系。
   - 通过调整权重和偏差，模型逐渐提高对训练数据的拟合能力。

3. **反向传播：**
   - 反向传播是一种优化算法，用于调整网络参数以最小化预测输出与实际目标的差距（损失）。
   - 通过计算损失梯度，反向传播将误差逐层传递回网络，更新权重和偏差。

4. **深度学习的层次结构：**
   - 深度学习强调多层次的结构，即深层神经网络。每一层都可以学习不同层次的抽象特征。
   - 通过多层次的权重和偏差，神经网络能够学习从原始输入到最终输出的复杂映射。

5. **激活函数的作用：**
   - 激活函数引入非线性，使得神经网络能够学习更复杂的函数。它们使网络能够捕捉输入之间的非线性关系。

### 数学公式：

神经网络的前向传播可以用以下数学公式表示：

- **线性变换：**
$Z^{[l]} = W^{[l]} \cdot A^{[l-1]} + b^{[l]}$

- **激活函数：**
$A^{[l]} = f^{[l]}(Z^{[l]})$

其中，$A^{[0]}$ 即为输入层的输入数据。

这些公式展示了神经网络如何在不同层次上进行权重和偏差的学习，以及如何通过激活函数引入非线性。整个学习过程涉及通过训练数据不断优化这些参数，使得网络能够更好地拟合数据。


##  前馈神经网络（Feedforward Neural Networks）：


- 这是一种最基本的神经网络结构，信息在网络中单向传播，没有循环连接。

- 描述了前馈神经网络如何通过层与层之间的权重进行前向传播，从而实现对输入数据的映射。
### 前馈神经网络结构：

前馈神经网络是一种基本的神经网络结构，其中信息从输入层流向输出层，不涉及反馈。一个简单的前馈神经网络可以表示为：

![](https://files.mdnice.com/user/55803/ef4d9d42-bde0-4de4-8ab9-dc7f96cda33c.png)

- 输入层（Input Layer）
- 隐藏层（Hidden Layers）
- 输出层（Output Layer）

### 前馈神经网络工作原理：

- **线性变换：**
  前馈神经网络中每一层的神经元执行线性变换，计算加权输入。
$Z^{[l]} = W^{[l]} \cdot A^{[l-1]} + b^{[l]}$

- **激活函数：**
  在线性变换后，激活函数引入非线性，使得网络能够学习更复杂的函数关系。
$A^{[l]} = f^{[l]}(Z^{[l]})$

- **前向传播：**
  通过一系列的线性变换和激活函数，信息从输入层流向输出层，完成一次前向传播。
$\hat{Y} = A^{[L]} = f^{[L]}(W^{[L]} \cdot f^{[L-1]}ldots f^{[1]}(W^{[1]} \cdot X + b^{[1]}) \ldots) + b^{[L]}$

### 代码示例：

以下是一个简单的Python代码示例，演示了一个具有单隐藏层的前馈神经网络的前向传播过程：

```python
import numpy as np

# 定义激活函数（例如ReLU）
def relu(x):
return np.maximum(0, x)

# 定义前馈神经网络前向传播
def forward_propagation(X, parameters):
# 从输入层到隐藏层的线性变换和激活
Z1 = np.dot(parameters['W1'], X) + parameters['b1']
A1 = relu(Z1)

# 从隐藏层到输出层的线性变换和激活
Z2 = np.dot(parameters['W2'], A1) + parameters['b2']
A2 = relu(Z2)

return A2

# 示例数据
X = np.array([[1.0], [2.0]])  # 输入数据

# 示例参数
parameters = {
'W1': np.array([[0.1, 0.2], [0.3, 0.4]]),
'b1': np.array([[0.5], [0.6]]),
'W2': np.array([[0.7, 0.8], [0.9, 1.0]]),
'b2': np.array([[1.1], [1.2]])
}

# 进行前向传播
output = forward_propagation(X, parameters)
print("神经网络输出:", output)
```

这个简单的例子中，使用了ReLU作为激活函数，并演示了从输入层到输出层的前向传播过程。这样的前馈神经网络结构可以扩展到更深层次，其中有多个隐藏层，以构建更复杂的模型。

### 前向传播基本步骤
前馈神经网络通过层与层之间的权重进行前向传播，这是神经网络中信息从输入层流向输出层的过程。

1. **输入层：**

   - 输入层包含网络接收的原始数据，例如特征向量。每个输入节点对应一个特征。

2. **权重和偏置：**

   - 每个连接输入层和下一隐藏层的节点都有一个权重。权重表示了每个特征对下一层节点的影响程度。

   - 每个隐藏层节点都有一个偏置，它可以看作是对应节点的触发阈值。
  
3. **线性变换：**

   - 对于每个隐藏层节点，进行线性变换。对于第 $i$ 个隐藏层节点，线性变换的结果 $z_i$ 计算如下：

$z_i = \sum_{j=1}^{n} w_{ij} \cdot x_j + b_i$

 其中，$w_{ij}$ 是连接输入节点 $j$ 和隐藏节点 $i$ 的权重， $x_j$ 是输入节点 $j$ 的值， $b_i$ 是隐藏节点 $i$ 的偏置。

4. **激活函数：**

   - 将线性变换的结果 $z_i$ 通过激活函数进行非线性映射，得到隐藏层节点的激活输出 $a_i$。

$a_i = f(z_i)$

 其中，$f$cdot)$ 是激活函数。

5. **重复步骤：**

   - 重复上述步骤，将隐藏层的输出作为下一层的输入，直到达到输出层。

6. **输出层：**

   - 输出层的节点产生最终的模型输出。输出节点的数量通常取决于任务类型（分类、回归等）
  
7. **总体表达式：**

   - 整个前向传播的过程可以通过矩阵运算表示：

$A^{(l)} = f(W^{(l)} \cdot A^{(l-1)} + B^{(l)})$

 其中，$A^{(l)}$ 是第 $l$ 层的激活输出，$W^{(l)}$ 是第 $l$ 层到第 $l+1$ 层的权重矩阵，$B^{(l)}$ 是第 $l$ 层的偏置，$f$cdot)$ 是激活函数。 

这个过程中，每一层的权重和偏置在训练过程中通过反向传播进行优化，以最小化模型的预测误差。整个前向传播的过程可以在神经网络中传递信息，从而实现对输入数据的映射和模式识别。

# 多层感知机（MLP）：


- 作者详细解释了多层感知机，这是一种常见的深度学习模型，包含多个隐藏层。

- 讨论了MLP的训练过程，使用梯度下降等优化算法进行参数更新。

多层感知机（Multilayer Perceptron，MLP）是一种常见的深度学习模型，它包含多个隐藏层，每个隐藏层都由多个神经元组成。作者详细解释了多层感知机的训练过程，其中使用梯度下降等优化算法进行参数更新。以下是关于这些概念的更详细说明：

### 多层感知机（MLP）：

1. **基本结构：**
   - MLP包含输入层、多个隐藏层和输出层。
   - 输入层接受输入数据，每个隐藏层都包含多个神经元，输出层产生最终的预测结果。

2. **隐藏层和神经元：**
   - 每个隐藏层包含多个神经元，每个神经元都连接到前一层和后一层的神经元。
   - 每个连接都有一个权重，每个神经元有一个偏差。

3. **激活函数：**
   - 在每个隐藏层和输出层，激活函数引入非线性，允许模型学习更复杂的函数。
   - 常用的激活函数包括ReLU、Sigmoid和Tanh。

### MLP的训练过程：

1. **前向传播：**
   - 输入数据通过网络的前向传播，从输入层到输出层。
   - 在每个隐藏层和输出层，进行线性变换和激活函数操作。

2. **损失函数：**
   - 训练过程中，需要定义一个损失函数来度量模型预测与实际标签之间的差距。
   - 常用的损失函数包括均方误差（MSE）和交叉熵损失。

3. **反向传播：**
   - 通过反向传播算法，计算损失对每个参数的梯度。
   - 这些梯度用于更新参数，以最小化损失。

4. **优化算法：**
   - 使用梯度下降等优化算法来更新参数。
   - 常见的优化算法包括随机梯度下降（SGD）、Adam等。

### 数学公式
以下是与多层感知机（MLP）训练过程相关的数学公式：

前向传播：

1. **线性变换：**
 $Z^{[l]} = W^{[l]} \cdot A^{[l-1]} + b^{[l]}$

2. **激活函数（ReLU）：**
 $A^{[l]} = \text{ReLU}(Z^{[l]})$

3. **输出层的线性变换：**
 $Z^{[L]} = W^{[L]} \cdot A^{[L-1]} + b^{[L]}$

4. **输出层的激活函数：**
 $A^{[L]} = Z^{[L]}$

损失函数：

1. **均方误差（MSE）：**
 $J = \frac{1}{m} \sum_{i=1}^{m} (y_{\text{true}}^{(i)} - y_{\text{pred}}^{(i)})^2$

   其中，$m$ 是样本数量，$y_{\text{true}}^{(i)}$ 是第$i$ 个样本的实际标签，$y_{\text{pred}}^{(i)}$ 是对应的预测标签。

反向传播：

1. **输出层的误差项（对于MSE损失）：**
 $\delta^{[L]} = \frac{\partial J}{\partial Z^{[L]}} = 2 \cdot (A^{[L]} - y_{\text{true}})$

2. **隐藏层的误差项（递归计算）：**
 $\delta^{[l]} = \frac{\partial J}{\partial Z^{[l]}} = (W^{[l+1]})^T \cdot \delta^{[l+1]} \cdot \text{ReLU'}(Z^{[l]})$

3. **梯度计算（权重和偏差）：**
 $\frac{\partial J}{\partial W^{[l]}} = \delta^{[l]} \cdot (A^{[l-1]})^T$
 $\frac{\partial J}{\partial b^{[l]}} = \delta^{[l]}$

参数更新：

1. **梯度下降（或其他优化算法）：**
 $W^{[l]} = W^{[l]} - \alpha \cdot \frac{\partial J}{\partial W^{[l]}}$
 $b^{[l]} = b^{[l]} - \alpha \cdot \frac{\partial J}{\partial b^{[l]}}$

其中，$\alpha$ 是学习率。



### 代码示例：

以下是一个简化的Python代码片段，演示了一个简单的MLP的训练过程：

```python
import numpy as np

# 定义激活函数（例如ReLU）
def relu(x):
    return np.maximum(0, x)

# 定义均方误差损失函数
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

# 前向传播
def forward_propagation(X, parameters):
    # 线性变换和激活函数（多个隐藏层）
    hidden1 = relu(np.dot(parameters['W1'], X) + parameters['b1'])
    hidden2 = relu(np.dot(parameters['W2'], hidden1) + parameters['b2'])
    output = np.dot(parameters['W3'], hidden2) + parameters['b3']

    return output

# 示例数据
X = np.array([[1.0, 2.0]])
y_true = np.array([[0.5]])

# 示例参数
parameters = {
    'W1': np.random.randn(4, 2),
    'b1': np.zeros((4, 1)),
    'W2': np.random.randn(3, 4),
    'b2': np.zeros((3, 1)),
    'W3': np.random.randn(1, 3),
    'b3': np.zeros((1, 1))
}

# 前向传播
y_pred = forward_propagation(X.T, parameters)

# 计算损失
loss = mse_loss(y_true.T, y_pred)
print("损失:", loss)
```

这个简单的MLP包含多个隐藏层，使用ReLU作为激活函数，并通过均方误差损失来衡量预测与实际标签的差距。在实际应用中，深度学习框架（如TensorFlow、PyTorch）提供了更强大的工具来构建和训练MLP。
# 卷积神经网络（CNN）：


- 介绍了卷积神经网络，专门用于处理图像数据。解释了卷积层、池化层等核心组件。

- 讨论了CNN在图像分类和物体检测中的应用。
卷积神经网络（Convolutional Neural Network，CNN）是专门用于处理图像数据的深度学习模型。它包含卷积层、池化层等核心组件，通过局部感受野和权值共享等机制提取图像中的特征。以下是关于CNN的核心组件和应用的详细说明：

### 卷积神经网络（CNN）：

1. **卷积层（Convolutional Layer）：**
   - 卷积层使用卷积操作对输入图像进行特征提取。卷积核在图像上滑动，通过局部感受野计算特征映射。
   - 卷积操作利用权值共享的方式，减少参数数量，增加模型的可训练性。

2. **池化层（Pooling Layer）：**


![步幅为2，池化窗口为$2\times 2$的最大池化层](https://files.mdnice.com/user/55803/12ffd381-1ae3-46e9-9868-67cdecd2523c.png)

   - 池化层用于降采样，减小特征映射的尺寸，减轻计算负担。常见的池化操作包括最大池化和平均池化。

3. **激活函数（Activation Function）：**
   - 在卷积层和池化层之后，通常会应用激活函数引入非线性，使网络能够学习更复杂的特征。

4. **全连接层（Fully Connected Layer）：**
   - 在卷积层和池化层之后，通常会添加全连接层用于分类。全连接层将卷积层输出拉平，并连接到输出层。

### CNN在图像分类和物体检测中的应用：

1. **图像分类：**
   - CNN在图像分类任务中取得了巨大成功。通过学习图像的局部特征和层次结构，CNN能够准确地将图像分类为不同的类别。

2. **物体检测：**
   - CNN也广泛应用于物体检测任务。一些流行的物体检测框架，如Faster R-CNN、YOLO（You Only Look Once）等，使用CNN来识别图像中的物体并定位其位置。

3. **迁移学习：**
   - 由于CNN在大规模图像数据上的学习能力，迁移学习在CNN中得到了广泛应用。在训练好的模型基础上，可以在不同的任务上进行微调，提高模型在新任务上的性能。

4. **语义分割：**
   - CNN还用于语义分割任务，即将图像中的每个像素分配到相应的类别，实现对图像的像素级别理解。

### 示例代码片段：

以下是一个简化的Python代码片段，演示了使用卷积神经网络进行图像分类的基本流程（使用TensorFlow和Keras）：

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# 构建卷积神经网络模型
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

# 编译模型
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 训练模型（示例数据）
model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(val_images, val_labels))
```

这个简单的CNN模型包含卷积层、池化层和全连接层，用于图像分类任务。在实际应用中，模型结构和参数需要根据具体任务进行调整。

# 循环神经网络（RNN）：


- 讲解了循环神经网络，适用于处理序列数据，如自然语言处理任务。

- 强调了RNN中的短时记忆和长时记忆的概念，以及LSTM（长短时记忆网络）的改进。
循环神经网络（Recurrent Neural Network，RNN）是一种适用于处理序列数据的深度学习模型，特别在自然语言处理（NLP）任务中得到广泛应用。在讲解RNN时，通常强调了短时记忆和长时记忆的概念，并介绍了LSTM（长短时记忆网络）作为对RNN的改进。

### 循环神经网络（RNN）：

1. **基本结构：**
   - RNN具有循环连接，允许信息在网络中传递，并且能够处理不同长度的序列数据。
   - 每个时间步，RNN接受输入和前一个时间步的隐藏状态，产生输出和当前时间步的隐藏状态。

2. **短时记忆和长时记忆：**
   - RNN中存在短时记忆的问题，即对于离当前时间步较远的信息，模型难以捕捉。
   - 这是由于梯度消失或梯度爆炸问题，导致长序列的信息难以传递。

### 长短时记忆网络（LSTM）：

1. **解决短时记忆问题：**
   - LSTM是对RNN的改进，通过引入门控机制，能够更有效地处理长序列。
   - 包括输入门、遗忘门、输出门，通过控制信息的流动，实现对短时记忆和长时记忆的有效管理。

2. **门控机制：**
![](https://files.mdnice.com/user/55803/b646183f-1e2f-4757-bd27-590761860635.png)
   - 输入门：控制新信息的输入。
   - 遗忘门：控制过去信息的保留。
   - 输出门：决定输出哪一部分信息。

3. **公式表示：**
   - 输入门：$i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi})$
   - 遗忘门：$f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf})$
   - 输出门：$o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})$
   - 更新记忆：$g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg})$
   - 记忆更新：$c_t = f_t \cdot c_{t-1} + i_t \cdot g_t$
   - 隐藏状态：$h_t = o_t \cdot \tanh(c_t)$

### 示例代码片段：

以下是一个简化的Python代码片段，演示了使用LSTM进行序列任务（使用TensorFlow和Keras）：

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# 构建LSTM模型
model = Sequential()
model.add(LSTM(64, input_shape=(seq_length, feature_dim)))
model.add(Dense(output_dim, activation='softmax'))

# 编译模型
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 训练模型（示例数据）
model.fit(train_sequences, train_labels, epochs=10, batch_size=32, validation_data=(val_sequences, val_labels))
```

这个简单的LSTM模型用于处理序列数据，其中 `seq_length` 是序列长度，`feature_dim` 是每个时间步的特征维度。在实际应用中，需要根据具体任务和数据调整模型结构和参数。

# 深度学习框架：


![](https://files.mdnice.com/user/55803/297aa6f7-74bd-4f9f-b978-0bc710e94c6c.png)

- 书中使用MXNet框架作为实现深度学习模型的工具，但也有对其他框架如TensorFlow和PyTorch的提及。

- 提供了实际代码示例，帮助读者理解和实践所学的概念。

MXNet（Apache MXNet）是一个开源的深度学习框架，它在深度学习中有一些重点应用。以下是MXNet在深度学习中的一些主要应用方面：

1. **多领域的深度学习任务：**
   - MXNet被广泛用于多个领域的深度学习任务，包括计算机视觉、自然语言处理、语音识别等。它提供了灵活性和可扩展性，适用于不同类型的深度学习应用。

2. **分布式深度学习训练：**
   - MXNet设计时考虑了分布式训练的需求，支持在多个设备、多台机器上进行分布式深度学习训练。这使得MXNet适用于大规模数据和复杂模型的训练。

3. **动态计算图：**
   - MXNet采用动态计算图的方式，与静态计算图的框架有所不同。这使得MXNet更适合处理变长序列等需要动态构建计算图的任务。

4. **混合精度训练：**
   - MXNet支持混合精度训练，即在训练过程中同时使用32位和16位浮点数。这有助于提高训练速度和减少内存占用。

5. **模型部署：**
   - MXNet提供了用于模型导出和部署的工具，使得在生产环境中部署深度学习模型变得更加容易。支持多种平台，包括服务器端、移动端和边缘设备。

6. **Gluon API：**
   - MXNet引入了Gluon API，这是一个动态神经网络API，使得模型的定义更加简单和直观。Gluon API支持深度学习的快速迭代和实验。
### Gluon API

Gluon API 在 MXNet 框架中被广泛应用于各种深度学习任务。

1. **计算机视觉（Computer Vision）：**
   - Gluon API 被用于构建和训练图像分类、目标检测、图像分割等计算机视觉任务的深度学习模型。Gluon 的简洁模型定义和动态计算图使得在处理图像数据时更加方便。

2. **自然语言处理（Natural Language Processing，NLP）：**
   - 在处理文本数据和自然语言处理任务时，Gluon API 提供了一个直观的接口，可用于构建文本分类、命名实体识别、机器翻译等深度学习模型。

3. **语音识别（Speech Recognition）：**
   - Gluon API 可以用于构建语音识别模型，处理音频数据。其动态计算图和高层 API 简化了声音数据的建模和训练过程。

4. **推荐系统（Recommendation Systems）：**
   - 在推荐系统中，Gluon API 用于构建用于个性化推荐的深度学习模型。其灵活性和易用性使得对用户和商品的复杂关系进行建模更为容易。

5. **时间序列预测（Time Series Prediction）：**
   - 对于时间序列数据，如股票价格预测、天气预测等任务，Gluon API 提供了一个方便的接口，支持动态计算图和简洁的模型定义。

6. **迁移学习（Transfer Learning）：**
   - Gluon API 可以用于迁移学习任务，通过加载预训练的模型并微调以适应新的任务。其参数管理功能和简洁的模型定义对于迁移学习非常有用。

# 应用实例：


- 通过实际案例和项目，帮助读者将理论知识应用到实际问题中，加深对深度学习概念的理解。

# 批评

对深度学习的主要批评是许多方法缺乏理论支撑。大多数深度结构仅仅是梯度下降的某些变式。尽管梯度下降法已经被充分地研究，但理论涉及的其他算法，例如对比分歧算法，并没有获得充分的研究，其收敛性等问题仍不明确。深度学习方法常常被视为黑盒，大多数的结论确认都由经验而非理论来确定。

也有学者认为，深度学习应当被视为通向真正人工智能的一条途径，而不是一种包罗万象的解决方案。尽管深度学习的能力很强，但和真正的人工智能相比，仍然缺乏诸多重要的能力。理论心理学家加里·马库斯指出：

>就现实而言，深度学习只是建造智能机器这一更大挑战中的一部分。这些技术缺乏表达因果关系的手段……缺乏进行逻辑推理的方法，而且远没有具备集成抽象知识，例如物品属性、代表和典型用途的信息。最为强大的人工智能系统，例如IBM的人工智能系统沃森，仅仅把深度学习作为一个包含从贝叶斯推理和演绎推理等技术的复杂技术集合中的组成部分[67]。

C8 循环神经网络




[(99+ 封私信 / 96 条消息) 自学深度学习是怎样一种体验？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/475262041/answer/2274710236)

首先掌握线性代数里面的：向量、矩阵、运算、范数、[特征向量](https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)和特征值；

【用Python做数据分析】这本书，不用全都看，看重点章节就可以；当然全看了，也很快，因为这本书本身学习起来就很简单；这本书的内容，在我们往后的机器学习和深度学习关系很密切，因为我们在构建模型之前，需要很多操作去处理数据，用到这本书介绍的这两个api包；

[[机器学习理论入门]]
对于机器学习理论算法，我推荐一本书籍和一个博客和一个Python包
**书籍是：李航的统计学习**，主要，不是全都看，这本书一共是分为了11章，你只需要去看其中的六章内容，分别是：1，2，4，5，6，8
**博客是[刘建平](https://www.zhihu.com/search?q=%E5%88%98%E5%BB%BA%E5%B9%B3&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)老师的博客，Python包是sklearn；**
作者：DASOU  
链接：https://www.zhihu.com/question/475262041/answer/2274710236  

第一章是统计学习概论；这章是在学习整个机器学习的一些基础概念，比如说什么是回归问题，什么是分类问题；什么是正则化，什么是[交叉验证](https://www.zhihu.com/search?q=%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)，什么是[过拟合](https://www.zhihu.com/search?q=%E8%BF%87%E6%8B%9F%E5%90%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)等等基础概念；必须掌握，没有商量的余地；

第二章是感知机，是最简单机器学习模型，也和后面的神经网络有关系，必须掌握

第三章是K近邻算法，这个你现在不需要看，跳过它；

第四章是[朴素贝叶斯](https://www.zhihu.com/search?q=%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)算法，这个非常重要，里面的概念比如说后验概率，[极大似然估计](https://www.zhihu.com/search?q=%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)之类的，必须掌握

第五章是[决策树](https://www.zhihu.com/search?q=%E5%86%B3%E7%AD%96%E6%A0%91&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)：这很简单，就是如何特征选择，两个[决策树算法](https://www.zhihu.com/search?q=%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)；也要掌握

第六章是逻辑回归和最大熵；要看

第七章[支持向量机](https://www.zhihu.com/search?q=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)，我说一下我的观念哈，我认为这章不需要看；为什们呢？首先在我自己的工作中，几乎没用过支持向量机；而且现在，在今天，如果你在面试深度学习岗位的时候，有的面试官还在让你手推SVM公式的话，我认为这个面试官是不合格的，这个公司可能未必是你很好的一个选择；

第八章提升树，必看，这个提升树算法非常重要；

第九章第十章第十一章，都不需要看；对于隐马尔科夫和[条件随机场](https://www.zhihu.com/search?q=%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)，之后你如果想深入学NLP，再来看；对于EM算法，入门之后你碰到的时候再去看；

我刚才谈到，对于重点算法必须能够手推公式，哪几个重要呢？不多，[逻辑回归](https://www.zhihu.com/search?q=%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)，朴素贝叶斯，以及提升树里的xgboost算法；别的算法，你能够自己复述一遍讲出来，就够了；
我刚才还谈到一个准则，是不要去从零造论文实现算法，因为sklearn可以很好的帮助你；

在这个过程中，你要去搞清楚这个算法输入数据，输出数据，每个参数的含义是什么；可以自己自己调一下参数，看看不同参数下最终效果有什么不同；但是在这里不要花费太大精力在调参上，因为你现在代码实现的是一个demo，数据量很小，调参没什么意义；什么调参呢？我一会会讲到；

<font color="#d831a8">整个机器学习理论部分，如果你真的认真去学习，三周时间，你肯定能搞定；你想啊，总共看6章，每章你看四天，这四天，你期中三天看理论部分，一天用代码跑一遍熟悉一下感觉；</font>

<font color="#d831a8">其实代码这块要跑起来，很快，都不需要一天，两三个小时就可以；四天搞定一章，三周看完一点问题没有；</font>


也就是我要谈的机器学习竞赛代码实战：在这里，我只推荐一本书，叫做：

<font color="#d831a8">**阿里云天池大赛赛题解析——机器学习篇；**</font>

记住啊，是机器学习篇，不是深度学习篇；
  
天池是一个竞赛平台，这本书里面它包含了四个实战型的任务：

工业蒸汽预测

天猫用户重复购买预测

O2O优惠券预测

阿里云安全恶意程序检测

我来告诉大家怎么看这本书：

有四个任务是吧，你挑其中的一个或者两个，不需要都看，没必要；

怎么确定把这一个或者两个任务吃透呢？

<font color="#d831a8">七个步骤：赛题理解、数据探索、[特征工程](https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)、模型训练、模型验证、特征优化、模型融合7个步骤</font>

开源代码的链接我放在了思维导图的备注；

就像我说的，四个任务中挑一个或者两个，在一周，七天，三天看一个，七天看两个，或者七天你就看一个，比如第一个，把它吃透就够了；

看完之后，你会对之前学习的统计学习书籍里面机器学习算法有一个非常清楚的认识；

所以整个机器学习的理论和代码时间，花费时间为1个月；


### 4.深度学习理论学习

我把深度学习的入门仿照机器学习，也分为两个部分，先学理论，再实战打比赛；

其实说心里话，深度学习入门比机器学习入门要简单的多；

我们都知道深度这块主要就是分为NLP和CV；

NLP任务上大概可以分为四种：文本分类 文本匹配 序列标注 文本生成，

CV任务大致也可以分为图像理解和生成：理解这块大致可以分为：分类、检测、分割、追踪； 生成这块基本就是GAN模型

对于入门来说，我们不用学这么多，我们只需要学籍基础的神经网络，然后通过文本分类和图片分类任务去熟悉掌握整个[徐娜林](https://www.zhihu.com/search?q=%E5%BE%90%E5%A8%9C%E6%9E%97&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)和预测流程，比如数据处理，模型搭建等呢吧；

所以我推荐的这两门课程也是很出名的：

就是大家常说的[cs231n](https://www.zhihu.com/search?q=cs231n&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D) 和CS224n；

我来告诉大家怎么看这两个视频，同样不是全部都看；

1. **推荐的视频cs231n；**

B站视频链接见思维导图；

整个视频在B站是分为了33讲，作为入门来说，主要是学习p1-p22;

也就是从第一讲课程介绍-计算机视觉概述到循环神经网络；

我们来打开看一眼：

然后这个视频不是让你一直看，看完一部分之后，去完成对应作业；

它的作业有三个，我把实现代码链接放在这里：见思维导图备注

你去做前两个，实现图像分类任务，实现[卷积神经网络](https://www.zhihu.com/search?q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)：bn，dropout，cnn 都要看一下；

第三个作业比较复杂，大家不用去看，只需要做前两个；

**注意，不需要自己从零去做这个作业，直接看给的代码仓库，去看人家怎么实现的，当然你如果有自信而且想要锻炼自己，没问题，可以从零去实现。但是对于大部分人，你去对照着代码一行行的看，去理解为什么这儿写，输出输入是什么；**

在这个过程，就会涉及到一点，就是框架的学习，我推荐大家使用Pytorch；

框架框架学习，我这里我后面会讲到，我先在这里插一句，就是大家可以去看B站刘二大人，地址在这里：见思维导图备注

它这个pytorch学习曲线比较平滑，大家在在看计算机视觉视频之后，完成代码的部分，如果有不懂的地方，穿插着去看这个刘二大人的视频；

因为刘二大人这个视频会涉及到CNN和RNN，所以如果你一开始就看，可能会有点费劲；

我举个例子吧，比如说你看完CNN网络，然后你去完成第二个作业，突然你发现里面有些不懂，不知道为什么这么弄，然后你去看刘二大人对应的视频讲CNN代码的；是这么个顺序啊；

整个计算机市局视频和代码学习完之后，你必须要掌握到什么程度呢？

必须要把下面这些完全掌握：

反向传播梯度回传，[损失函数](https://www.zhihu.com/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)，优化算法，多层感知机，卷积神经网络，普通的循环神经网络，以及一些dropout和BN掌握住；

**2. [自然语言处理](https://www.zhihu.com/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)：**

推荐一个视频，非常经典的 CS224n：

链接：见思维导图备注

**这个课程不是需要都看，要有选择的看：**

**在B站的官方主页，它包含了18讲的内容；在入门阶段，你只需要看P1-P5和P8，P9,P11；**

通过看这个视频你要能够达到什么地步呢？

其实这个视频和cs231n在基础部分是重叠的，对于基础部分，大家可以都看，两者兼学会更好

必须熟悉的掌握：[反向传播](https://www.zhihu.com/search?q=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)，词向量，RNN，GRU，Lstm，Seq2Seq以及attention机制；初步了解卷积神经网络；

有作业，一定要认真做，自己写不出来，仿照着别人的写:见思维导图备注

**作业也不是都写：重点看a1，a2，a4，a5；其实a5这个不做的话，也没问题，把前面给的这个三个一定自己走一遍；**

作业涉及到词向量和机器翻译；

有的朋友常常会和我反应，不知道att这种细节是如何实现的，其实这些都是最基础的东西，一定要从零看代码，有余力的话，可以自己实现一遍，非常有帮助；

在学习这两个视频的过程中，视频是英文的，而且涉及到的一些经典概念，不太容易理解，那么必须要看这本书：


$f(x)=3x_1^2 +5e^{x_2}$

**[邱锡鹏](https://www.zhihu.com/search?q=%E9%82%B1%E9%94%A1%E9%B9%8F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)**

代码的学习过程中，不用去过度的关注调参之类的，而是关注代码是怎么写的；因为调参这块tricks后面我会有专门的部分提升；

3.Pytorch框架学习

pytorch框架的学习：其实这个[pytorch](https://www.zhihu.com/search?q=pytorch&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)学习应该是融合在上面这个计算机视觉学习中的；可以在看完视频只有，写作业之前，先刷一遍这个Pytorch教学视频；

B站的刘二大人：《PyTorch深度学习实践》完结合集 [https://www.bilibili.com/video/BV1Y7411d7Ys?from=search&seid=1631997590037031874&spm_id_from=333.337.0.0](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1Y7411d7Ys%3Ffrom%3Dsearch%26seid%3D1631997590037031874%26spm_id_from%3D333.337.0.0)

但是它好像没有源代码，评论区有小伙伴手敲了代码，地址在这：[https://blog.csdn.net/bit452/category_10569531.html](https://link.zhihu.com/?target=https%3A//blog.csdn.net/bit452/category_10569531.html)

### 5.深度学习竞赛实战：

重点来了，上面谈到的这些深度学习的东西，都是在给你打基础；

但是要记住，我们学习深度学习是为了实战：我给大家准备了两个学习曲线非常平滑的实战项目；

**一个是新闻分类项目，一个是街景字符识别，也就是图片分类项目，有的人可能会认为这两个项目非常简单，但是我认为千万不要小瞧这两个项目，扎扎实实做完这两个项目，对你的帮助绝对比你想象的要大；**

**先说NLP的新闻文本分类任务；地址在这里**：见思维导图备注

就像我所说的，这个任务是一个NLP中一个基础任务-[文本分类](https://www.zhihu.com/search?q=%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)任务；这也是绝大部分从业的业务型NLP工程师日常工作最常见的工作需求；所以掌握好这个任务非常关键；

那么怎么掌握呢？在天池上，有开源的赛题解析，我挑选几个我认为很好的notebook给到大家；

task1：赛题理解：

jupyter notebook 链接，见思维导图备注

就是仿照你工作的时候，运营人员怎么给你提的需求，你听完需求要去分析它是什么问题，是个分类问题，回归问题，NLP问题，CV问题，[多模态](https://www.zhihu.com/search?q=%E5%A4%9A%E6%A8%A1%E6%80%81&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)问题？

task2：分析数据：去看字符分布，最大长度，链接思维导图备注

task3：基于机器学习的文本分类任务：先做一个baseline出来，不是先搞大模型复杂东西出来；

链接见思维导图备注

task4：不同深度学习模型：

fastext：它是一种词向量，也是一种文本分类模型：对应的论文链接在这里：对应的我的博客解读，在这里，链接见思维导图备注

w2C:在视频有介绍对应的论文链接对应的我的博客解读在这里

[textcnn](https://www.zhihu.com/search?q=textcnn&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2274710236%7D)：也就是用CNN模型来做，链接见思维导图备注

textrnn：使用RNN做，链接见思维导图备注

之前深度学习视频学了，CNN，RNN等基础网络，这里你就去实战这些模型；

bert；这个可以先不看，等你入了深度学习的门，认为自己想搞NLP这个方向了，你再去看相关的论文；我把链接放在这里吧：[https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.24.6406111aE3Lglg&postId=118259](https://link.zhihu.com/?target=https%3A//tianchi.aliyun.com/notebook-ai/detail%3Fspm%3D5176.12586969.1002.24.6406111aE3Lglg%26postId%3D118259)

  

**第二个任务是CV任务：图片分类任务： 街景字符编码识别**

**链接：见**思维导图备注

task1 赛题理解

链接见思维导图备注

task数据读取与数据扩增

链接见思维导图备注

task3构建

链接思维导图备注

task4模型的训练，链接思维导图备注

task4模型的集成：

链接见思维导图备注

**做完这个任务，你会对在CV领域，如果加载自己的图片数据集，如何构建CV模型，增强数据，模型验证都有一个很清晰的了解；**

**在这两个任务实施的时候，大家可以尽情的调参，尝试各种各样的tricks提升自己的成绩；**

整个深度学习

面试题：

百面机器学习；视频最后面我会提供给大家 这本书非常好，真的非常好；
  
作者：DASOU  
链接：https://www.zhihu.com/question/475262041/answer/2274710236  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  
作者：DASOU  
链接：https://www.zhihu.com/question/475262041/answer/2274710236  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



# [[动手学深度学习复习回顾]]