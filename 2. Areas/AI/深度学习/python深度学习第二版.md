---
date created: 2024-07-16 09:55
aliases: 
theme: 
original: 
url: 
author: 
date updated: 2024-07-16 10:04
type: 
high priority: false
---
# 特征工程

特征工程（Feature Engineering）是指在机器学习任务中使用原始数据构建新的特征或对现有特征进行转换，以提高模型的性能。良好的特征工程可以使模型更好地捕捉数据的模式和关系，从而提高模型的准确性和泛化能力。

以下是特征工程的一些常见技术和方法：

1. **缺失值处理：** 处理数据中的缺失值，可以通过填充均值、中位数、众数或使用其他插补方法来处理。

2. **标准化和归一化：** 将特征的值缩放到相似的范围，以防止某些特征对模型的影响过大。标准化通常基于特征的均值和标准差，而归一化将特征缩放到 [0, 1] 的范围。

3. **数值化类别特征：** 将类别型特征转换为数值型特征，例如使用独热编码（One-Hot Encoding）或标签编码（Label Encoding）。

4. **多项式特征：** 对特征进行多项式扩展，以考虑特征之间的高阶关系。

5. **交叉特征：** 创建新的特征，通过组合不同的特征，以捕捉它们之间的交互关系。

6. **时间特征：** 对包含时间信息的特征进行处理，例如提取年、月、日、小时等信息。

7. **文本特征处理：** 对文本数据进行词袋模型、TF-IDF（Term Frequency-Inverse Document Frequency）等处理，以提取文本特征。

8. **特征选择：** 选择对目标变量有较大影响的特征，剔除不相关或冗余的特征。

9. **降维：** 使用降维技术（如主成分分析 PCA）来减少特征的数量，同时保留大部分信息。

10. **处理异常值：** 处理数据中的异常值，可以通过删除、替换或调整来处理。

良好的特征工程需要深入理解数据和问题领域，以便更好地选择和构建特征，从而提高机器学习模型的性能。

# 张量

前面例子使用的数据存储在多维 NumPy 数组中，也叫张量（tensor）。一般来说，当前所有机器学习系统都使用张量作为基本数据结构。张量对这个领域非常重要，重要到 Google 的TensorFlow 都以它来命名。那么什么是张量？
张量这一概念的核心在于，它是一个数据容器。它包含的数据几乎总是数值数据，因此它是数字的容器。你可能对矩阵很熟悉，它是二维张量。张量是矩阵向任意维度的推广［注意，张量的维度（dimension）通常叫作轴（axis）］。

让我们逐步解释张量、张量运算、微分和梯度下降的概念：

### 张量（Tensor）：

- **定义：** 张量是数学和物理学中的一种广义概念，可以表示在不同坐标系中的任意向量、矩阵或更高维度的数组。
- **在深度学习中：** 在深度学习领域，张量通常指的是多维数组，是神经网络中存储和处理数据的基本结构。标量是零维张量，向量是一维张量，矩阵是二维张量，以此类推。

### 张量运算：

- **定义：** 张量运算是在张量上执行的各种数学操作，例如加法、减法、乘法、除法等。
- **在深度学习中：** 在深度学习模型中，张量运算是神经网络层之间的核心操作。通过不同的张量运算，可以实现模型的前向传播和反向传播。

### 微分（Differentiation）：

- **定义：** 微分是求函数在某一点的导数，描述了函数在该点的变化率。
- **在深度学习中：** 微分在训练神经网络中起着关键作用。通过对损失函数进行微分，可以得到梯度，进而使用梯度下降等优化算法来更新模型的参数，使得模型逐渐收敛到最优解。

### 梯度下降（Gradient Descent）：

- **定义：** 梯度下降是一种优化算法，用于最小化（或最大化）目标函数。它通过迭代地调整模型参数，沿着目标函数的梯度方向逐渐逼近最优解。
- **在深度学习中：** 在深度学习中，梯度下降被广泛用于训练神经网络。通过计算损失函数对模型参数的梯度，可以确定参数的调整方向，并利用梯度下降的迭代过程来不断更新模型参数，使得损失函数逐渐降低，模型性能逐渐提高。

综合起来，张量是深度学习中的基本数据结构，张量运算是神经网络中的核心操作，微分和梯度下降是优化算法中的关键概念，用于训练模型。这些概念共同构成了深度学习领域的基础知识。

# fit方法

在Keras中，使用 `fit` 方法是训练神经网络模型的主要步骤。`fit` 方法通常被用于将模型与训练数据进行拟合，从而学习模型的参数。以下是使用 Keras 中的 `fit` 方法的基本示例：

```python
from keras.models import Sequential
from keras.layers import Dense
import numpy as np

# 创建一个简单的Sequential模型
model = Sequential()

# 添加一个全连接层
model.add(Dense(units=1, input_dim=1))

# 编译模型
model.compile(optimizer='sgd', loss='mean_squared_error')

# 准备训练数据
X_train = np.array([1, 2, 3, 4, 5])
y_train = np.array([2, 4, 6, 8, 10])

# 使用fit方法进行模型训练
model.fit(X_train, y_train, epochs=100, verbose=2)
```

上述代码创建了一个简单的单层神经网络，用于学习输入和输出之间的线性关系。关键步骤包括：

1. 创建一个 `Sequential` 模型。
2. 向模型添加一个全连接层。
3. 编译模型，指定优化器（这里使用随机梯度下降 'sgd'）和损失函数（均方误差 'mean_squared_error'）。
4. 准备训练数据 `X_train` 和 `y_train`。
5. 使用 `fit` 方法进行模型训练，其中 `epochs` 参数指定训练轮数。

在实际应用中，训练数据通常会更复杂，模型结构也会更加深入。`fit` 方法的参数还包括批量大小（`batch_size`）、验证数据（`validation_data`）等。根据具体问题和数据，可以调整这些参数以优化模型的训练过程。

# 第1-3章 前置基础

### 1.2.7 机器学习现状

### 1.3.4 新一轮投资热潮

## 数学基础

### 2.4.3 随机梯度下降

### 2.4.4 链式求导:反向传播算法

### 2.5.4 完整的训练循环

# 第3章 Keras和TensorFlow入门

## 3.0 本章小结

### 3.4.2 使用Colaboratory

## 3.5 TensorFlow入门

## 3.6 核心Keras API

# 第4章 NN入门:分类与回归

## 4.0 本章小结

## 4.1 二分类:影评

## 4.2 多分类:新闻

## 4.3 标量回归问题:预房价

# 第5章 机器学习基础

## 5.0 本章小结

## 5.1 泛化

## 5.1 评估模型

## 5.2 改进模型拟合

# 第6章 ML通用工作流程

## 5.0 本章小结

- 定义任务(定义,收集,理解,指标)
- 开发模型(数据,方法,基准,扩大规模,正则化与调参)
- 部署模型(解释&设定预期,部署推断模型,监控性能,维护)

**任务类型**：分类、回归、聚类等不同类型的任务可能需要不同类型的神经网络架构。例如，对于图像分类任务，卷积神经网络（CNN）通常是首选；对于序列建模任务，如自然语言处理或时间序列预测，循环神经网络（RNN）或Transformer可能更合适。

**数据特性**：数据的大小、维度和分布也会影响模型架构的选择。例如，如果数据量很大且维度高，可能需要使用深度学习模型如卷积神经网络或Transformer。如果数据是序列数据，可能需要使用循环神经网络或Transformer。

为模型正确选择最后一层激活函数和损失函数

#### 超参数自动调节软件

#### 部署模型

#### 监控模型在真实环境中的性能

# 第7章 深入Keras

渐进式呈现复杂性 progressive disclosure of complexity

## 本章总结

## Keras工作流程

## 构建模型的不同方法

## 内置训练循环和评估循环

## 编写自定义

# 第8章 计算机视觉深度学习入门

## 本章总结

## 卷积神经网络

## 小数据集

## 使用预训练模型

# 第9章 计算机视觉深度学习进阶

## 本章小结

## 三项基本计算机视觉任务

## 图像分割示例

## 架构模式

## 解释

# 第10章 深度学习处理时间序列

## 本章总结

## 不同类型的时间序列任务

## 温度预测?

## 理解RNN

## RNN高级用法

# 第11章 深度学习处理文本

## 本章总结

## 自然语言处理概述

## 文本数据

## 单词组:集合/序列

## Tranformer架构

## 超越:序列到序列学习

# 第12章 生成式深度学习

## 本章总结

## 文本生成

## DeepDream

## 神经风格迁移

## 变分自编码器

## 对抗网络入门

# 第13章 当前最佳实践

## 本章总结

## 模型性能

## 加速模型训练

# 第14章 总结

## 14.1 重点概念回顾

## 14.2 深度学习的局限性

## 14.3 如何更加通用

## 14.4 缺失的内容

## 14.5 未来

## 14.6 最近进展

## [[动手学深度学习 (一刷)笔记]]
