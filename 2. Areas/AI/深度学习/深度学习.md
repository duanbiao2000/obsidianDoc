

[[python深度学习第二版 1]]

1. 前向传播（Forward Pass）： 输入数据通过模型进行前向传播，得到模型的预测输出。
2. 计算损失（Compute Loss）： 使用损失函数计算模型输出与实际标签之间的差距。
3. 反向传播（Backward Pass）： 使用自动微分（autograd）系统，计算损失函数关于模型参数的梯度。这就是 `l.backward()` 所完成的任务。
4. 参数更新（Update Parameters）： 利用计算得到的梯度，使用优化算法来更新模型的参数，以减小损失函数。

```python
import torch

# 假设有一个模型和损失函数
model = ...  # 定义模型
criterion = torch.nn.CrossEntropyLoss()  # 选择损失函数

# 前向传播
outputs = model(inputs)
# 计算损失
loss = criterion(outputs, labels)

# 反向传播
loss.backward()

# 参数更新，例如使用梯度下降
learning_rate = 0.01
with torch.no_grad():
    for param in model.parameters():
        param -= learning_rate * param.grad

# 清空梯度，为下一轮训练做准备
model.zero_grad()

```


