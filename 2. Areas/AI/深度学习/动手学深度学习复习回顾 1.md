---
date_created: 2024-07-19 19:35
---

[[python深度学习第二版 1]]

1. 前向传播（Forward Pass）： 输入数据通过模型进行前向传播，得到模型的预测输出。
2. 计算损失（Compute Loss）： 使用损失函数计算模型输出与实际标签之间的差距。
3. 反向传播（Backward Pass）： 使用自动微分（autograd）系统，计算损失函数关于模型参数的梯度。这就是 `l.backward()` 所完成的任务。
4. 参数更新（Update Parameters）： 利用计算得到的梯度，使用优化算法来更新模型的参数，以减小损失函数。

```python
import torch

# 假设有一个模型和损失函数
model = ...  # 定义模型
criterion = torch.nn.CrossEntropyLoss()  # 选择损失函数

# 前向传播
outputs = model(inputs)
# 计算损失
loss = criterion(outputs, labels)

# 反向传播
loss.backward()

# 参数更新，例如使用梯度下降
learning_rate = 0.01
with torch.no_grad():
    for param in model.parameters():
        param -= learning_rate * param.grad

# 清空梯度，为下一轮训练做准备
model.zero_grad()

```

1. 前向传播（Forward Pass）：输入数据通过模型进行前向传播，得到模型的预测输出。
   重點：前向传播是指将输入数据通过模型进行计算，逐层传递参数，直到得到模型的输出结果。
2. 计算损失（Compute Loss）：使用损失函数计算模型输出与实际标签之间的差距。
   重點：损失函数用于评估模型的预测输出和实际标签之间的差距，数值越小表示模型的预测越准确。
3. 反向传播（Backward Pass）：使用自动微分（autograd）系统，计算损失函数关于模型参数的梯度。这就是 `l.backward()` 所完成的任务。
   重點：反向传播是指通过损失函数的梯度，从输出层反向计算每一层的梯度，以更新模型的参数。
4. 参数更新（Update Parameters）：利用计算得到的梯度，使用优化算法来更新模型的参数，以减小损失函数。
   重點：通过反向传播计算出模型参数的梯度，然后使用优化算法（如SGD、Adam等）来更新模型参数，从而最小化损失函数。
