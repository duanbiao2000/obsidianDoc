 
# 机器学习入门指南：十大算法速成记

## 什么是机器学习？

机器学习，就是让计算机通过数据学习、自我进化，从而实现特定功能的技术。它就像是我们给电脑装上了一个“大脑”，让它能够自主地理解和处理信息。


### 1. 线性回归（Linear Regression）

**用途**：预测连续值。

**公式**：y = wx + b

**特点**：简单易懂，适用于线性关系较强的数据。

### 2. 逻辑回归（Logistic Regression）

**用途**：分类问题，输出概率。

**公式**：y = 1 / (1 + e^(-wx+b))

**特点**：可以处理非线性数据。

### 3. 决策树（Decision Tree）

**用途**：分类与回归。

**特点**：直观易懂，易于解释。

### 4. 支持向量机（SVM）

**用途**：二分类。

**特点**：在高维空间表现优秀。

### 5. 朴素贝叶斯（Naive Bayes）

**用途**：分类。

**特点**：简单快速，适用于文本分类。

### 6. K-近邻（K-Nearest Neighbors）

**用途**：分类与回归。

**特点**：容易实现，适用于小数据集。

### 7. K-均值聚类（K-Means Clustering）

**用途**：无监督学习，聚类。

**特点**：简单高效，适用于数据可视化。

### 8. 随机森林（Random Forest）

**用途**：分类与回归。

**特点**：提高泛化能力，减少过拟合。

### 9. AdaBoost

**用途**：集成学习，提高分类器性能。

**特点**：多轮迭代，逐渐提升性能。

### 10. 神经网络（Neural Network）

**用途**：复杂的函数逼近。

**特点**：模拟生物神经网络，处理非线性问题。

## 机器学习实战，从数据开始！

1. 数据准备：收集、清洗、整合数据。
2. 特征工程：处理数据，提取有效特征。
3. [[#选择算法]]：根据问题选择合适的算法。
4. 模型训练：用训练集训练模型。
5. 模型评估：用测试集评估模型性能。
6. 模型优化：根据评估结果调整模型参数。

 








## 机器学习，让未来不再遥远！

掌握这些基础知识，你就可以开始探索机器学习的无穷魅力了。记住，多实践、多思考，你将会在机器学习的道路上越走越远！


# <白话机器学习算法_黄莉婷苏川集>
数据->算法->参数调优->创建模型->比较->选择最好的一个

行:数据点
列:变量,属性,特征,维度
变量类型:二值,分类,整型,连续(用来表示小数)

变量选择(一开始借助简单的图来研究变量相关性)

## 特征工程:
合并变量: 降维  - > 提取最有用的信息,获得更精简的变量集
 特征工程是机器学习中非常重要的一个步骤,其目标是从原始数据中派生出描述样本的特征,以便机器学习算法可以效果地作用。特征工程主要包含以下几个方面:

1. 特征选取(Feature Selection)

从原始特征中选择对模型最有用的特征子集。例如从图像数据中选取颜色、纹理、形状等有区分能力的特征。特征选取可以减少训练时间,防止过拟合,提高模型性能。

2. 特征提取(Feature Extraction) 

将原始特征转换或组合成更有代表性的新特征。例如通过主成分分析(PCA)算法对图像特征进行降维。特征提取可以减少特征冗余,凝聚特征信息。

3. 特征变换(Feature Transformation)

对特征应用数学函数变换以增强特征表达能力。例如对文本特征取对数可以使高频和低频特征更加区分。特征变换可以提高模型训练效果。

4. 特征缩放(Feature Scaling)

调整特征值范围到相似的尺度,防止某些特征远大于其他特征。例如进行最小-最大规范化(Min-Max Normalization),将特征缩放到0-1区间。特征缩放可以提高优化效率和模型稳定性。

5. 特征交互(Feature Interaction)

探索特征之间的交互作用和相关性。例如结合年龄和工作年限作为新特征。特征交互可以创建更复杂和富表达能力的特征。

总体来说,良好的特征工程可以大大提升机器学习的效果和泛化能力,需要对数据和问题有深刻理解,才能设计出区分样本的优秀特征。

## 缺失数据
近似, 计算, 移除(万不得已)

## 选择算法
![](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130105438937-100223111.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130105638274-940642552.png)

## 强化学习

## 参数调优

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130105911578-646473485.png)


控制模型整体复杂度的一种方法是，通过**正则化引入惩罚参数**。这个新参数会通过人为增大预测误差，对模型复杂度的增加进行惩罚，从而使算法同时考虑复杂度和准确度。使模型保持简单有助于提高模型的泛化能力。

## 评价模型
- 预测准确率
- 混淆矩阵
- 均方根误差

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130110518392-1752275958.png)
 假正类(False Positive)和假负类(False Negative)是评估分类模型性能常用的两个指标。

假正类指被模型误判为正类的负类样本。比如病人实际上不患某种疾病,但分类模型判断其患病。

假负类则反之,指被模型误判为负类的正类样本。如病人实际上患某种疾病,但分类模型判断其不患病。

混淆矩阵(Confusion Matrix)给出了这四类情况的汇总:

- 真正类(True Positive):正类预测为正类
- 假正类(False Positive):负类预测为正类 
- 假负类(False Negative):正类预测为负类
- 真负类(True Negative):负类预测为负类

混淆矩阵直观显示了分类模型的预测力,常用来评估二分类和多分类模型的性能。


针对假正类和假负类的比例,还可以计算精确率(Precision)和召回率(Recall)等指标,全面评价模型的预测偏差。

## 交叉验证

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130110842350-1841274031.png)

# 第二章 K类聚值

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130111320844-194089584.png)

# 组群数量
首先，群组数量要足够大，以便提取有意义的模式，用作商业决策参考；其次，还要足够小，
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130111546642-788642676.png)

## 迭代过程
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130111647766-1805929724.png)

步骤 1：首先猜测每个群组的中心点。因为暂时不能确定通过猜测
得到的中心点是否正确，所以称它们为伪中心点。
步骤 2：把每个数据点分配给最近的伪中心点。这样一来，就得到
了两个群组，即红色群组和蓝色群组，如图 2-3 所示。
步骤 3：根据群组成员的分布，调整伪中心点的位置。
步骤 4：重复步骤 2 和步骤 3，直至群组成员不再发生变化。


# 第三章:主成分分析

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130112005370-656077612.png)
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130112048416-1016792515.png)

# 第四章:关联规则
识别关联规则的常用指标有 3 个：支持度、置信度和提升度。

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113207115-1432206262.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113238633-1302113767.png)

## 先验原则
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113400667-2078455446.png)

# 网络关系分析

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113528905-242739715.png)

## 世界关系格局图
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113823717-593552692.png)

## Louvain 方法

## PageRange算法
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130114432671-1614758435.png)

# 第六章:回归关系

## 梯度下降法
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115048761-1859405819.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115137883-519370418.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115217438-523734751.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115331735-902504085.png)

更高阶技术: 套索回归或岭回归  ->消除多重共线性

# 第七章:K最近邻算法和异常检测
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115943564-2091905606.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130120012994-1539787184.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130120654184-1944746160.png)

# 第8章 支持向量机 (可跳过)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130121243182-1281550192.png)

支持向量机具备在高维空间操纵数据的能力，这使得它在分析有多个变量的数据集时大受欢迎。支持向量机的常见应用场景包括遗传信息破译以及文本情感分析。

# 第9章决策树
生成决策树: 递归拆分
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130121715998-1955202492.png)
## 决策树的多样化方法: (黑盒,难以可视化)
 - 随机森林: 随机选择二元选择题,多棵决策树,综合预测结果
 - 梯度提升: 有策略选择二元选择题,逐步提高预测准确度,然后将所有预测结果加权平均. 

# 第10章: 随机森林
这基于以下事实：虽然错误的预测结果可能有很多，但是正确的只有一个。通过组合具有不同优缺点的模型，往往能强化正确的预测结果，同时使错误相互抵消。**这种通过组合不同模型来提高预测准确度的方法被称为集成方法**。
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122341398-1829036894.png)
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122411264-118277919.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122509131-832385367.png)

不可解释性可能会带来伦理问题,尤其适用于那些预测准确度比可解释性更重要的场合。
虽然**随机森林的预测结果不具有可解释性**，但是仍然可以根据对预测结果的贡献度大小对各个预测变量进行排序。

# 第11章: 神经网络

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122932202-655597639.png)

 损失层：虽然图 11-7 并未显示损失层，但是在神经网络的训练过程中，损失层是存在的。该层通常位于最后，并提供有关输入是否识别正确的反馈；如果不正确，则给出误差量。
在训练神经网络的过程中，损失层至关重要。若预测正确，来自于损失层的反馈会强化产生该预测结果的激活路径；**若预测错误，则错误会沿着路径逆向返回，这条路径上的神经元的激活条件就会被重新调整，以减少错误。这个过程称为反向传播。**
通过不断重复这个训练过程，神经网络会学习输入信号和正确输出标签之间的联系，并且把这些联系作为激活规则编入每个神经元。因此，为了提高神经网络的预测准确度，需要调整管
理激活规则的部件。

# 第12章: A/B测试和多臂老虎机
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123540126-1481827384.png)

### epsilon递减策略
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123726978-1934748458.png)


# 附录
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123904159-90282734.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123916407-771161137.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123933080-427978839.png)

## 选择算法
选择算法是一个非常广泛的话题，因为“合适的算法”取决于许多因素，包括：

* **问题的性质:**  这是一个排序问题、搜索问题、图问题、机器学习问题，还是其他类型的问题？  问题的具体要求是什么？例如，需要找到最优解还是近似解？需要处理的数据量有多大？数据是否有序？

* **算法的特性:**  算法的时间复杂度是多少？空间复杂度是多少？算法是否易于实现和理解？算法的鲁棒性如何？算法是否需要特定的硬件或软件？

* **资源限制:**  可用的计算资源（CPU、内存、存储空间）有多少？时间限制是多少？

为了更好地回答“选择算法”这个问题，我们需要知道具体的**问题描述**。  但是，我可以提供一些常见问题类型及其常用算法的例子：

**常见问题类型和常用算法:**

* **排序:**
    * **冒泡排序 (Bubble Sort):** 简单易懂，但效率低，时间复杂度 O(n²)。适合小规模数据或教学目的。
    * **插入排序 (Insertion Sort):**  简单易懂，效率中等，时间复杂度 O(n²)。适合小规模数据或几乎有序的数据。
    * **选择排序 (Selection Sort):** 简单易懂，效率中等，时间复杂度 O(n²)。
    * **归并排序 (Merge Sort):**  效率高，时间复杂度 O(n log n)，稳定排序。适合大规模数据。
    * **快速排序 (Quick Sort):**  效率高，平均时间复杂度 O(n log n)，但最坏情况 O(n²)。适合大规模数据，但需要谨慎选择枢轴。
    * **堆排序 (Heap Sort):**  效率高，时间复杂度 O(n log n)，不稳定排序。适合大规模数据。

* **搜索:**
    * **线性搜索 (Linear Search):**  简单，时间复杂度 O(n)。适合小规模数据或无序数据。
    * **二分搜索 (Binary Search):**  效率高，时间复杂度 O(log n)。需要有序数据。
    * **深度优先搜索 (DFS):**  用于图的遍历。
    * **广度优先搜索 (BFS):**  用于图的遍历。

* **Graph Algorithms:**  This is a major category deserving its own section. Include:
    * **Shortest Path Algorithms:** Dijkstra's algorithm (weighted graphs), Bellman-Ford algorithm (weighted graphs with negative edges), Floyd-Warshall algorithm (all pairs shortest paths).
    * **Minimum Spanning Tree Algorithms:** Prim's algorithm, Kruskal's algorithm.
    * **Topological Sort:** For directed acyclic graphs (DAGs).
    * **Strongly Connected Components:** Kosaraju's algorithm, Tarjan's algorithm.

* **Dynamic Programming:** A powerful technique for solving optimization problems by breaking them down into smaller overlapping subproblems, solving each subproblem only once, and storing their solutions to avoid redundant computations.  This significantly improves efficiency compared to brute-force approaches. Examples include:
    * **Fibonacci Sequence:** Calculating the nth Fibonacci number efficiently by storing previously computed values.  A recursive solution without memoization (dynamic programming) has exponential time complexity, while a dynamic programming solution has linear time complexity.
    * **Knapsack Problem:**  Various variations exist (0/1 knapsack, fractional knapsack, unbounded knapsack).  Dynamic programming provides optimal solutions for the 0/1 knapsack problem, where you have a knapsack with a weight limit and a set of items, each with a weight and value.  The goal is to maximize the total value of items within the weight constraint.  Fractional knapsack allows taking fractions of items, making it solvable with a greedy approach, while the unbounded knapsack allows taking multiple instances of the same item.
    * **Longest Common Subsequence (LCS):** Finding the longest subsequence common to all sequences in a set of sequences (often just two).  Dynamic programming builds a table to store the lengths of LCS for subproblems.
    * **Shortest Path Problems (e.g., Bellman-Ford, Floyd-Warshall):**  These algorithms, while sometimes categorized separately, leverage dynamic programming principles to find the shortest paths in a graph.  Bellman-Ford handles negative edge weights, while Floyd-Warshall finds shortest paths between all pairs of vertices.
    * **Edit Distance:** Determining the minimum number of edits (insertions, deletions, substitutions) needed to transform one string into another.  Dynamic programming efficiently computes this distance.
    * **Sequence Alignment:**  Used in bioinformatics to align DNA or protein sequences, finding optimal alignments based on scoring matrices.
    * **Matrix Chain Multiplication:** Determining the optimal parenthesization of a matrix chain multiplication to minimize the number of scalar multiplications.

Dynamic programming is characterized by:

* **Overlapping Subproblems:** The problem can be broken down into smaller subproblems that are reused multiple times.
* **Optimal Substructure:** An optimal solution to the problem can be constructed from optimal solutions to its subproblems.