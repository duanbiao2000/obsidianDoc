 机器学习中常见的十大算法包括:

1.线性回归(Linear Regression) - 用于预测连续值的简单线性回归模型。

2.逻辑回归(Logistic Regression) - 用于分类问题的logistic回归模型。

3.决策树(Decision Tree) - 一种流行的树形分类与回归方法。

4.支持向量机(SVM) - 一种二分类模型, Finds an optimal hyperplane in an N-dimensional space that distinctly classifies the data points.

5.朴素贝叶斯(Naive Bayes) - 一种基于贝叶斯定理与特征条件独立假设的分类算法。

6.K-近邻(K-Nearest Neighbors) - 一种基于实例的学习,通过测量不同特征的距离进行分类。

7.K-均值聚类(K-Means Clustering) - 一种简单的无监督学习聚类算法。 

8.随机森林(Random Forest) - 包含多个决策树的集成学习方法,对过拟合有稳定的抗性。

9.提升树(Boosted Trees) - 一种把多个弱学习器组合成一个强学习器的集成算法。

10.神经网络(Neural Network) - 模拟生物神经网络的人工神经网络,可用于复杂的函数逼近。

 好的,我用简明的语言并给出关键公式和图例简要介绍这十大机器学习算法:

1. 线性回归(Linear Regression)
预测连续值的线性模型。公式:y = wx + b

2. 逻辑回归(Logistic Regression)
分类模型,基于Sigmoid函数将线性回归的输出转换为概率。公式: y = 1 / (1 + e^(-wx+b))

3. 决策树(Decision Tree)
基于特征的条件划分,树形模型。

4. 支持向量机(SVM)
找到最大间隔超平面进行分类。

5. 朴素贝叶斯(Naive Bayes)  
基于贝叶斯定理与特征条件独立假设的分类器。

6. K近邻(K-NN)
基于距离度量确定K个最近邻居分类。

7. K均值聚类(K-Means)
基于距离度量将样本划分为K个簇。

8. 随机森林(Random Forest)
多个决策树集成,提高泛化能力。

9. AdaBoost
多轮迭代提升弱分类器性能。

10. 神经网络(NN)
模拟生物神经网络,可表示复杂函数。


 



## 选择算法
![](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130105438937-100223111.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130105638274-940642552.png)


## 参数调优

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130105911578-646473485.png)


控制模型整体复杂度的一种方法是，通过**正则化引入惩罚参数**。这个新参数会通过人为增大预测误差，对模型复杂度的增加进行惩罚，从而使算法同时考虑复杂度和准确度。使模型保持简单有助于提高模型的泛化能力。

## 评价模型
- 预测准确率
- 混淆矩阵
- 均方根误差

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130110518392-1752275958.png)
 假正类(False Positive)和假负类(False Negative)是评估分类模型性能常用的两个指标。

假正类指被模型误判为正类的负类样本。比如病人实际上不患某种疾病,但分类模型判断其患病。

假负类则反之,指被模型误判为负类的正类样本。如病人实际上患某种疾病,但分类模型判断其不患病。

混淆矩阵(Confusion Matrix)给出了这四类情况的汇总:

- 真正类(True Positive):正类预测为正类
- 假正类(False Positive):负类预测为正类 
- 假负类(False Negative):正类预测为负类
- 真负类(True Negative):负类预测为负类

混淆矩阵直观显示了分类模型的预测力,常用来评估二分类和多分类模型的性能。


针对假正类和假负类的比例,还可以计算精确率(Precision)和召回率(Recall)等指标,全面评价模型的预测偏差。


# 第二章 K类聚值

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130111320844-194089584.png)

# 组群数量
首先，群组数量要足够大，以便提取有意义的模式，用作商业决策参考；其次，还要足够小，
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130111546642-788642676.png)

## 迭代过程
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130111647766-1805929724.png)

步骤 1：首先猜测每个群组的中心点。因为暂时不能确定通过猜测
得到的中心点是否正确，所以称它们为伪中心点。
步骤 2：把每个数据点分配给最近的伪中心点。这样一来，就得到
了两个群组，即红色群组和蓝色群组，如图 2-3 所示。
步骤 3：根据群组成员的分布，调整伪中心点的位置。
步骤 4：重复步骤 2 和步骤 3，直至群组成员不再发生变化。


# 第三章:主成分分析

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130112005370-656077612.png)
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130112048416-1016792515.png)

# 第四章:关联规则
识别关联规则的常用指标有 3 个：支持度、置信度和提升度。

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113207115-1432206262.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113238633-1302113767.png)

## 先验原则
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113400667-2078455446.png)

# 网络关系分析

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113528905-242739715.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130113823717-593552692.png)

## Louvain 方法

## PageRange算法
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130114432671-1614758435.png)

# 第六章:回归关系

## 梯度下降法
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115048761-1859405819.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115137883-519370418.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115217438-523734751.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115331735-902504085.png)

更高阶技术: 套索回归或岭回归  ->消除多重共线性

# 第七章:K最近邻算法和异常检测
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130115943564-2091905606.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130120012994-1539787184.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130120654184-1944746160.png)

# 第8章 支持向量机 (可跳过)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130121243182-1281550192.png)

支持向量机具备在高维空间操纵数据的能力，这使得它在分析有多个变量的数据集时大受欢迎。支持向量机的常见应用场景包括遗传信息破译以及文本情感分析。

# 第9章决策树
生成决策树: 递归拆分
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130121715998-1955202492.png)
## 决策树的多样化方法: (黑盒,难以可视化)
 - 随机森林: 随机选择二元选择题,多棵决策树,综合预测结果
 - 梯度提升: 有策略选择二元选择题,逐步提高预测准确度,然后将所有预测结果加权平均. 

# 第10章: 随机森林
这基于以下事实：虽然错误的预测结果可能有很多，但是正确的只有一个。通过组合具有不同优缺点的模型，往往能强化正确的预测结果，同时使错误相互抵消。**这种通过组合不同模型来提高预测准确度的方法被称为集成方法**。
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122341398-1829036894.png)
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122411264-118277919.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122509131-832385367.png)

不可解释性可能会带来伦理问题,尤其适用于那些预测准确度比可解释性更重要的场合。
虽然随机森林的预测结果不具有可解释性，但是仍然可以根据对预测结果的贡献度大小对各个预测变量进行排序。

# 第11章: 神经网络

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130122932202-655597639.png)

 损失层：虽然图 11-7 并未显示损失层，但是在神经网络的训练过程中，损失层是存在的。该层通常位于最后，并提供有关输入是否识别正确的反馈；如果不正确，则给出误差量。
在训练神经网络的过程中，损失层至关重要。若预测正确，来自于损失层的反馈会强化产生该预测结果的激活路径；若预测错误，则错误会沿着路径逆向返回，这条路径上的神经元的激活条件就会被重新调整，以减少错误。这个过程称为反向传播。
通过不断重复这个训练过程，神经网络会学习输入信号和正确输出标签之间的联系，并且把这些联系作为激活规则编入每个神经元。因此，为了提高神经网络的预测准确度，需要调整管
理激活规则的部件。

# 第12章: A/B测试和多臂老虎机
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123540126-1481827384.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123726978-1934748458.png)


# 附录
![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123904159-90282734.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123916407-771161137.png)

![image](https://img2024.cnblogs.com/blog/488941/202401/488941-20240130123933080-427978839.png)


苏川集（Kenneth Soo）毕业于美国斯坦福大学，并取得统计学硕士学位。此前，他在英国华威大学学习 MORSE，不仅名列前茅，还是运筹学和管理科学组的助理研究员，从事网络随机故障下应用程序的双目标稳健优化研究。
MORSE 的全称是 Mathematics, Operational Research, Statistics and Economics，即数
学、运筹学、统计学和经济学。——编者注


# Python机器学习入门与实战
3.3 数据处理

3.3.1 判断缺失数据

3.3.2 删除缺失数据

3.3.3 填充缺失数据

3.3.4 移除重复数据

3.3.5 替换数据

3.3.6 排列和随机采样

## 第Ⅲ篇 专业技能提升篇
第5章 机器学习算法之算法综述
第6章 机器学习算法之决策树
第7章 机器学习算法之朴素贝叶斯
第8章 机器学习算法之逻辑回归
第9章 机器学习算法之支持向量机
第10章 机器学习算法之AdaBoost
第11章 机器学习算法之线性回归
第12章 机器学习算法之k-means
第13章 机器学习算法之PCA

## 第Ⅳ篇 深度学习延伸篇
第14章 深度学习延伸之卷积神经网络

