

**核心区别概览**

| 特性            | `function_calling`         | `json_mode`                  | `raw`                   | `auto`                                   |
| :------------ | :------------------------- | :--------------------------- | :---------------------- | :--------------------------------------- |
| **主要目的**      | 让 LLM 请求执行预定义的**函数/工具**    | 强制 LLM 的**整个输出**为有效的 JSON 格式 | 直接进行**文本**输入和输出，无特殊结构约束 | **自动选择** 最合适的方式（通常优先 `function_calling`） |
| **交互机制**      | LLM 输出特定结构（如 JSON）指定函数名和参数 | LLM 被约束，只能生成符合 JSON 语法的文本    | 纯文本提示 -> 纯文本响应          | 库/框架根据 LLM 能力和配置决定                       |
| **LLM 输出**    | 可能包含普通文本 + 函数调用请求结构        | **必须**是完整的、有效的 JSON 对象/数组    | 自由格式的文本                 | 取决于自动选择的结果                               |
| **灵活性**       | 高（可混合文本和工具调用）              | 中（仅限于 JSON 结构）               | 最高（完全自由，但也最不可靠）         | 依赖库的实现                                   |
| **可靠性 (结构化)** | 高（针对函数调用）                  | 非常高（针对 JSON 输出）              | 低（依赖提示工程和输出解析）          | 通常较高（倾向于使用原生支持的功能）                       |
| **LLM 支持**    | 需要 LLM 明确支持函数调用特性          | 需要 LLM 明确支持 JSON 输出模式        | 几乎所有 LLM 都支持            | 取决于具体 LLM 是否支持前两种模式                      |

**如何选择？**

*   如果你需要 LLM **调用外部 API 或执行代码**，并且使用的 LLM 支持此功能，**`function_calling`** 是最佳选择。
*   如果你只需要 LLM **稳定地输出结构化的 JSON 数据**，并且不涉及多轮的工具调用-结果反馈循环，**`json_mode`** 是一个很好的选择（如果 LLM 支持）。
*   如果你在与不支持特殊模式的 LLM 交互，或者进行非常基础的文本生成任务，或者想要完全手动控制输出解析，使用 **`raw`**。
*   如果你不确定，或者希望库能帮你做决定，或者希望代码更通用，**`auto`** 是一个方便的起点，但要注意理解它在你的具体环境下的行为。

