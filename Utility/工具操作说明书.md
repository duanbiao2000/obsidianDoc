
## 摘要

本文旨在介绍一系列自动化脚本和工具，用于处理和分析科技爱好者周刊的文档。这些工具涵盖了从创建Markdown文件目录、处理和生成周刊文档、进行热词统计，到批量聚合热词信息等多个方面。通过这些脚本，可以高效地管理和分析大量的文本数据，为研究和学习提供便利。  

## 知识库Markdown文件目录生成脚本 

**文件名：**summary.py

**目的：** 自动创建知识库中所有Markdown文件的总目录。

**方法：** 通过Python脚本遍历指定目录下的所有子目录和Markdown文件，生成一个包含所有文件链接的SUMMARY.md文件。

**实现：** 脚本首先设置Markdown文件所在的目录和输出文件的路径。然后，创建并打开输出文件，写入基础内容。接着，遍历目录中的每个子目录和文件，将目录名和文件名（去除.md扩展名）以Markdown链接的形式写入文件。

```python
import os  
  
# 设置你的Markdown文件所在的目录  
directory = "./"  
# 设置输出文件的路径  
output_file = "../SUMMARY.md"  
  
# 创建Markdown文件并写入基础内容  
with open(output_file, 'w',  encoding='utf-8') as md_file:  
    md_file.write("# Summary\n\n")  
    for chapter in os.listdir(directory):  
        chapter_path = os.path.join(directory, chapter)  
        if os.path.isdir(chapter_path):  
            md_file.write(f"* {chapter}\n")  
            for file in os.listdir(chapter_path):  
                if file.endswith(".md"):  
                    file_path = os.path.join(chapter, file)  
                    md_file.write(f"* [{file[:-3]}]({file_path})\n")  
  
# 打印完成信息  
print(f"{output_file} has been created.")
```
![image.png](https://cdn.jsdelivr.net/gh/duanbiao2000/BlogGallery/picture/20240602204141.png)

[SUMMARY](../SUMMARY.md)

## 1.科技爱好者周刊处理脚本

**文件：**爱好者周刊处理脚本.sh

**目的：** 克隆最新的周刊文档，剔除非文字内容，生成周刊文档，用于词频统计。

**方法：** 使用Bash脚本自动化处理过程，包括克隆仓库、合并文件、删除空行和特定内容，以及清理链接。

**实现：** 脚本首先克隆指定的GitHub仓库，并进入文档目录。然后，合并所有相关的Markdown文件为一个文件，并进行一系列文本清理操作，包括删除空行、图片链接、特定标题段落和iframe标签。最后，输出结果并提示用户。

```sh
#!/bin/bash

# 克隆仓库并进入文档目录
#git clone https://github.com/ruanyf/weekly.git
cd weekly/docs

# 合并所有以 issue-2 开头的 md 文件为一个 weekly.md 文件，并放在上上一级目录
#cat issue-2??.md > ../../weekly_$(date +'%Y%m%d_%H%M%S').md
cat issue-*.md > ../../weekly_$(date +'%Y%m%d_%H%M%S').md
# 去除空行和开头空格
sed -i '/^$/d' ../../weekly_*.md
sed -i 's/^[[:space:]]*//' ../../weekly_*.md

# 删除带有图片的行
sed -i '/^!\[\]/d' ../../weekly_*.md

# 删除回顾到完的内容
sed -i '/## 回顾/,/（完）/d' ../../weekly_*.md

# 删除包含 iframe 的内容
sed -i '/<iframe/,/<\/iframe>/d' ../../weekly_*.md

# 删除链接
#sed -i 's/(https[^)]*)//g' ../../weekly_*.md
sed -i 's/(\(https\?\)[^)]*)//g' ../../weekly_*.md
sed -i 's/[][]//g' ../../weekly_*.md

# 提示用户结果已保存
echo "Weekly.md file generated."

# 添加暂停，以便用户查看输出
read -p "Press [Enter] key to exit..."

```
**输出:** weekly_$(date +'%Y%m%d_%H%M%S').md top100词频文件.

## 2. 热词统计脚本

**文件：** 热词统计.py

**目的：** 对指定目录中的Markdown文档进行热词统计，生成Top 100热词列表。

**方法：** 利用Python脚本结合jieba库进行中文分词，并使用Counter类统计词频。

**实现：** 脚本定义了文件路径和输出文件名，读取屏蔽词列表，然后遍历指定目录中的所有文件，进行分词和词频统计。统计结果经过过滤后，按词频排序并取前100个热词，最后将结果写入输出文件。

```python
#!/usr/bin/env python3

import os
from collections import Counter
import jieba
import string

# 定义文件路径和输出文件名
docs_dir = "wordFreq"
output_file = "热词榜top100.md"
stopwords_file = "stopwords.txt"

# 提示用户词频统计和文件生成开始
print("Generating word frequency list from docs directory...")
print(f"Generating {output_file} file...")

# 提取文本内容，并统计词频
word_counts = Counter()

# 读取屏蔽词列表文件
stopwords = set()
with open(stopwords_file, 'r', encoding='utf-8') as f:
    for line in f:
        stopwords.add(line.strip())

for root, dirs, files in os.walk(docs_dir):
    for file in files:
        with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
            text = f.read()
            # 使用 jieba 进行中文分词
            words = jieba.cut(text)
            word_counts.update(words)

# 输出标题和分隔线到文件
with open(output_file, 'w', encoding='utf-8') as f:
    f.write("Top 100 Hot Words\n")
    f.write("==================\n\n")

# 生成过滤后的词频列表
filtered_word_counts = [
    (word, count)
    for word, count in word_counts.items()
    if len(word) > 1  # 排除单个字
    and not word.isdigit()  # 排除纯数字
    and word not in stopwords
    and word not in string.whitespace
    and word not in string.punctuation
]

# 按词频从高到低排序
filtered_word_counts.sort(key=lambda x: x[1], reverse=True)

# 取前 100 个词频
top_100_word_counts = filtered_word_counts[:100]

# 输出词频到文件
with open(output_file, 'a', encoding='utf-8') as f:
    for word, count in top_100_word_counts:
        f.write(f"{count}\t{word}\n")


# 提示用户文件生成完毕
print(f"Generation of {output_file} complete.")

# 提示用户结果已保存
print(f"Results saved to {output_file}")

```

**相关文件:** 

1.停用词文本obsidianDoc\weekly\stopwords.txt

2.统计文件路径: \obsidianDoc\weekly\wordFreq\
> [!note] 通常只要放入上一步生成的weekly_$(date +'%Y%m%d_%H%M%S').md文件即可


[weekly_20240602_191324](../weekly/wordFreq/weekly_20240602_191324.md)
![image.png](https://cdn.jsdelivr.net/gh/duanbiao2000/BlogGallery/picture/20240602204054.png)

## 3. 热词信息批量聚合脚本.py

**目的：** 根据热词统计结果，在指定目录中生成批量热词笔记。 

**方法：** 通过Python脚本读取热词榜文件，并使用grep命令搜索每个热词在文档中的出现位置和文本。  

**实现：** 脚本首先创建结果保存目录，然后逐行读取热词榜文件，对于每个热词，执行grep搜索并保存结果到指定文件。最后，提示用户搜索结果已保存。

> [!note] 借鉴的shell:
> ```sh
> # grep -nri '{keyword}' {search_dir} > {output_file}
grep -nri css docs | nl > css.md
> ```



```python
#!/usr/bin/env python3

import os

# 读取热词榜文件
hot_words_file = "热词榜top100.md"

# 定义搜索目录和结果保存目录
search_dir = "docs"
output_dir = "hWordComb"

# 创建结果保存目录
os.makedirs(output_dir, exist_ok=True)

# 提示用户搜索和文件生成开始
print("Generating search results from hot words list...")

# 逐行读取热词榜文件，并生成搜索结果文件
with open(hot_words_file, 'r', encoding='utf-8') as f:
    for line in f:
        # 分割词频和关键字
        parts = line.strip().split('\t')
        if len(parts) != 2:
            print(f"Ignoring line: {line.strip()} - Incorrect format")
            continue
        count, keyword = parts
        # 生成搜索结果文件路径
        output_file = os.path.join(output_dir, f"{keyword}.md")
        # 执行 grep 搜索并保存结果到文件
        os.system(f"grep -nri '{keyword}' {search_dir} > {output_file}")
        # 提示文件生成完毕
        print(f"Generation of {output_file} complete.")

# 提示用户结果已保存
print("Search results saved.")

# 添加暂停，以便用户查看输出
input("Press [Enter] key to exit...")

```

![image.png](https://cdn.jsdelivr.net/gh/duanbiao2000/BlogGallery/picture/20240602205930.png)


## 4. 封装待续
#zk-todo 

## KeepChatGPT by xcanwin

**用法：** 作为油猴插件，改善与ChatGPT的交互体验，实现自动继续对话的功能。

![image.png](https://cdn.jsdelivr.net/gh/duanbiao2000/BlogGallery/picture/20240602210116.png)

## 结论

本文介绍的自动化脚本和工具为科技爱好者周刊的文档处理和分析提供了一套完整的解决方案。通过这些工具，用户可以高效地管理和分析大量的文本数据，从而更好地理解和利用科技领域的信息。这些脚本的开发和应用展示了自动化技术在文本处理和知识管理中的潜力和价值。