好的，我将根据深度学习中注意力机制的思想，尝试从 [[基于大脑机制的几点省思]] 这篇笔记中“关注”并提取出那些最核心、最重要的部分，旨在呈现约 20% 的关键信息。

这里是提取的关键概念和它们的精炼解释：

## Keras名词术语 - 核心提炼 (约20%)

本部分聚焦于神经网络构建中的基本“零件”及其核心功能。

### 激活函数 (Activation Functions)

引入非线性，处理复杂模式。

#### ReLU (Rectified Linear Unit)
*   **核心：** 简单的开关，输入>0时传递，否则为0。
*   **关键点：** 简单有效，非线性激活。

#### Softmax
*   **核心：** 输出层投票系统，将得分转为概率分布，总和为1。
*   **关键点：** 用于多分类，非线性。

##### 非线性激活的关键点
*   ReLU, Sigmoid, Softmax 等都是非线性函数。
*   它们是处理复杂模式、超越线性关系的关键。

### 卷积运算 (Convolutional Operations)

提取局部特征。

#### Conv (Convolution)
*   **核心：** 用“放大镜”（卷积核）扫描图像，提取特定特征（如边缘）。
*   **关键点：** 提取图像等数据的局部特征。

##### 卷积运算的关键点
*   卷积用于提取局部特征。

### 池化操作 (Pooling Operations)

减小尺寸，保留重要信息，提高鲁棒性。

#### Max_pool (Max Pooling)
*   **核心：** 在小窗口内只保留最大值。
*   **关键点：** 减小特征图尺寸，保留重要信息，提高对平移的鲁棒性。

##### 池化操作的关键点
*   减小特征图尺寸，降低计算量，提高模型对平移的鲁棒性。

### 正常化操作 (Normalization Operations)

加速训练，提高稳定性。

#### Batch_normalization (Batch Normalization)
*   **核心：** 对批次数据进行均值和方差的统一调整。
*   **关键点：** 加速训练，提高稳定性，增强泛化能力。

##### 规范化操作的关键点
*   用于加速训练，提高稳定性，并增强泛化能力。

### 损失函数 (Loss Functions)

衡量预测与真实的差距。

#### Binary_crossentropy / Categorical_crossentropy
*   **核心：** 衡量模型预测（概率值或概率分布）与真实标签之间的差距。
*   **关键点：** 用于衡量模型预测和真实结果之间的差距，训练目标是最小化损失。

### 注意力机制 (Attention Mechanisms)

让模型关注最重要的部分。

#### Dot_product_attention
*   **核心：** 计算输入部分之间的相关性，根据相关性加权组合信息。
*   **关键点：** 让模型能够关注输入中最重要的部分。

##### 注意力机制的关键点
*   让模型能够关注输入中最重要的部分。

### 数据编码操作 (Data Encoding Operations)

将类别数据转为数值形式。

#### One_hot
*   **核心：** 将每个类别转为一个向量，只有一个位置是1，其余为0。
*   **关键点：** 用于将类别数据转换成数值形式，方便模型处理 (单标签)。

##### 数据编码操作的关键点
*   用于将类别数据转换成数值形式，方便模型处理。

这些是笔记中涵盖的最核心的概念、它们的简要定义或核心功能，以及各部分的“关键点”总结， representando the most "attended-to" parts based on common deep learning understanding and the note's structure.