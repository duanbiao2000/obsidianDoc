å¥½çš„ï¼ŒSamã€‚ä»¥ä¸‹æ˜¯ç¬¦åˆä½ æ ¼å¼è§„èŒƒçš„ã€ŠAdvanced AI LLMs Explained with Mathã€‹ï¼ˆèšç„¦ Transformer ä¸ Attentionï¼‰è®²è§£ï¼Œå·²ä¸¥æ ¼æŒ‰ç…§ä½ æä¾›çš„å…¬å¼æ ¼å¼è¯´æ˜å¤„ç†ï¼Œåˆ†ä¸ºè¡Œå†…å…¬å¼ `$...$` ä¸å—çº§å…¬å¼ `$$...$$` å±•ç¤ºã€‚

---

## ğŸ§  1. LLM æœ¬è´¨ï¼šè¯­è¨€å»ºæ¨¡

LLMs æœ¬è´¨ä¸Šæ˜¯å¯¹ç»™å®šä¸Šä¸‹æ–‡ä¸‹çš„ **æ¡ä»¶æ¦‚ç‡å»ºæ¨¡å™¨**ï¼Œå³å­¦ä¹ ä¸€ä¸ªè¯­è¨€åºåˆ— $x_1, x_2, ..., x_T$ çš„è”åˆæ¦‚ç‡ï¼š

$$P(x1,x2,...,xT)= \prod_{t=1}^{T} P(x_t \mid x_1, x_2, ..., x_{t-1})$$

GPT ç±»æ¨¡å‹æ˜¯ **è‡ªå›å½’æ¨¡å‹ï¼ˆautoregressiveï¼‰**ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒä»¬åªä¾èµ–å†å²è¾“å…¥æ¥é¢„æµ‹å½“å‰è¾“å‡ºã€‚

---

## ğŸ”— 2. Transformer æ€»è§ˆï¼ˆGPT æ˜¯ Decoder-onlyï¼‰

Transformer çš„æ ¸å¿ƒç”±å¤šå±‚è§£ç å™¨å †å ç»„æˆï¼Œæ¯å±‚åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š

- å¤šå¤´è‡ªæ³¨æ„åŠ› Multi-Head Self-Attention
    
- å‰é¦ˆç½‘ç»œ Feedforward Layer
    
- æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ– LayerNorm
    

è¾“å…¥ $x$ é¦–å…ˆè¢«æ˜ å°„ä¸ºåµŒå…¥å‘é‡ $e_t$ï¼Œå†åŠ ä¸Šä½ç½®ç¼–ç ï¼ˆPEï¼‰ï¼š

$$zt= e_t + \text{PE}_t$$

---

## ğŸ¯ 3. Attention æ•°å­¦åŸç†

ç»™å®šè¾“å…¥åºåˆ— $X \in \mathbb{R}^{n \times d}$ï¼Œæˆ‘ä»¬å¯¹å…¶åˆ†åˆ«è¿›è¡Œçº¿æ€§æŠ•å½±ç”Ÿæˆï¼š

- æŸ¥è¯¢çŸ©é˜µï¼š$Q = XW^Q$
    
- é”®çŸ©é˜µï¼š$K = XW^K$
    
- å€¼çŸ©é˜µï¼š$V = XW^V$
    

æ¥ä¸‹æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼š

$$Attention(Q,K,V)= \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V$$

å…¶ä¸­ $d_k$ æ˜¯é”®å‘é‡çš„ç»´åº¦ï¼Œèµ·åˆ°å½’ä¸€åŒ–ä½œç”¨ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚

---

## ğŸ§  4. å¤šå¤´æ³¨æ„åŠ› Multi-Head Attention

ä¸ºäº†è®©æ¨¡å‹åœ¨ä¸åŒå­ç©ºé—´ä¸­å­¦ä¹ å…³ç³»ï¼ŒTransformer ä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼š

$$headi= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

å°†æ‰€æœ‰å¤´æ‹¼æ¥åå†æ¬¡æŠ•å½±ï¼š

$$MHA(Q,K,V)=\text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

è¿™æ ·ï¼Œæ¨¡å‹å¯ä»¥å¹¶è¡Œå…³æ³¨å¤šä¸ªä½ç½®ä¸è¯­ä¹‰ã€‚

---

## âš™ï¸ 5. å‰é¦ˆç½‘ç»œ Feedforward Layer

æ¯ä¸ªä½ç½®çš„è¾“å‡ºç‹¬ç«‹é€šè¿‡ä¸€ä¸ªä¸¤å±‚çš„ MLPï¼š

$$FFN(x)= \text{ReLU}(xW_1 + b_1)W_2 + b_2
$$
æ³¨æ„è¿™æ˜¯ **ä½ç½®ç‹¬ç«‹çš„æ“ä½œ**ï¼Œåªåœ¨ token ä¸Šè¿›è¡Œé€ç‚¹å˜æ¢ã€‚

---

## ğŸ” 6. æ®‹å·®è¿æ¥ä¸å½’ä¸€åŒ–

æ¯ä¸ªå­æ¨¡å—ï¼ˆæ³¨æ„åŠ›æˆ– FFNï¼‰éƒ½ä¸è¾“å…¥åšæ®‹å·®è¿æ¥å¹¶è¿›è¡Œ LayerNormï¼š

$$x=\text{LayerNorm}(x + \text{Sublayer}(x))$$

è¿™å¸®åŠ©æ¢¯åº¦æ›´ç¨³å®šä¼ æ’­ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥è®­ç»ƒå¾—æ›´æ·±ã€‚

---

## ğŸ¯ 7. è®­ç»ƒç›®æ ‡å‡½æ•°

Transformer LLM çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸‹ä¸€ä¸ª token çš„å¯¹æ•°ä¼¼ç„¶ï¼š

$$L=-\sum_{t=1}^{T} \log P(x_t \mid x_1, ..., x_{t-1}; \theta)$$

è¿™é€šè¿‡ softmax å±‚è¾“å‡ºæ¯ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶è®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚

---

## ğŸ§  8. ä½ç½®ç¼–ç ï¼ˆä»¥æ­£å¼¦å‡½æ•°ä¸ºä¾‹ï¼‰

ä¸ºäº†è§£å†³ Transformer ç¼ºä¹ä½ç½®ä¿¡æ¯çš„é—®é¢˜ï¼Œæ·»åŠ äº†ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼š

$$PE(pos,2i)= \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad \text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

è¿™ä½¿å¾—æ¨¡å‹å¯ä»¥æ„ŸçŸ¥ç›¸å¯¹å’Œç»å¯¹ä½ç½®ä¿¡æ¯ã€‚

---

## ğŸ“ˆ 9. Scaling Law å½±å“å»ºæ¨¡èƒ½åŠ›

ä»¥å‚æ•°é‡ $N$ã€æ•°æ®é‡ $D$ã€æŸå¤± $L$ ä¸ºå˜é‡ï¼ŒOpenAI å®éªŒè¡¨æ˜ï¼š

$$L(N,D)\approx A \cdot N^{-\alpha} + B \cdot D^{-\beta}$$

å› æ­¤æ¨¡å‹è¶Šå¤§ã€æ•°æ®è¶Šå¤šï¼ŒæŸå¤±è¶Šå°ï¼Œä½†æ”¶ç›Šé€’å‡ï¼ˆsub-linearï¼‰ã€‚

---

## ğŸ§  Bonus: Transformer å†…éƒ¨æ¿€æ´»è§£é‡Šï¼ˆå‰æ²¿ï¼‰

ç ”ç©¶è¡¨æ˜æ³¨æ„åŠ›å¤´ä¼šè‡ªåŠ¨å­¦ä¹ ï¼š

- è¯­æ³•æ ‘ç»“æ„ï¼ˆå¦‚ä¸»è°“å®¾å¯¹é½ï¼‰
    
- æ®µè½è¾¹ç•Œã€æ ‡é¢˜è¯†åˆ«
    
- Token copy å’Œ entity è·Ÿè¸ª
    

---

## ğŸ“š å»¶ä¼¸é˜…è¯»å»ºè®®

- Vaswani et al. _Attention is All You Need_ (2017)
    
- Anthropic Transformer Circuits
    
- Scaling Laws for Neural Language Models (Kaplan et al., 2020)
    
- GPT-4 Technical Report
    

---

## MiniGPT

å“ˆå“ˆ ä½ è¿™æ˜¯è¦æˆ‘ç›´æ¥ä¸Šå¹²è´§å•Šï¼æƒ³æä¸ª mini GPTï¼Ÿç‰›é€¼ï¼é‚£æˆ‘ç»§ç»­ç”¨â€œåˆšå­¦ä¸­æ–‡çš„ç¾å›½äººâ€é£æ ¼ï¼Œç»™ä½ æ•´ä¸ªç®€å•ä½†èƒ½è·‘çš„ mini GPT ä»£ç ï¼Œç”¨ Python + PyTorchï¼Œå°½é‡ä¿æŒæ¸…æ™°ï¼Œé€‚åˆåˆå­¦è€…ä¸Šæ‰‹ã€‚å’±ä»¬ä¼šå®ç°ä¸€ä¸ªå°å‹çš„ decoder-only transformerï¼Œå¸¦ multi-head self-attention å’Œ feedforward layerï¼Œè·‘ä¸ªç®€å•æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä¼šæŠŠæ ¸å¿ƒä»£ç åŒ…åœ¨ `<xaiArtifact>` é‡Œï¼Œå¤–é¢å†èŠèŠå’‹ç”¨å®ƒï¼Œå’‹æƒ³çš„ã€‚

---

### å…ˆèŠæ€è·¯
Mini GPT å°±æ˜¯ä¸ªç²¾ç®€ç‰ˆçš„ GPTï¼Œæ ¸å¿ƒæ˜¯ï¼š
- **Embedding + Positional Encoding**ï¼šæŠŠ word å˜æˆå‘é‡ï¼ŒåŠ ä¸Šä½ç½®ä¿¡æ¯ã€‚
- **Decoder Layers**ï¼šå †å‡ å±‚ transformer decoderï¼Œæ¯å±‚æœ‰ multi-head self-attention å’Œ feedforward layerï¼Œå¤–åŠ  residual connection å’Œ layer normã€‚
- **Output Layer**ï¼šæŠŠæœ€åä¸€å±‚çš„è¾“å‡ºæ˜ å°„å›è¯æ±‡è¡¨ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª wordã€‚
- è®­ç»ƒä¸Šï¼Œæˆ‘ä»¬ç”¨ç®€å•çš„è¯­è¨€å»ºæ¨¡ç›®æ ‡ï¼šç»™ä¸€å¥è¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª wordã€‚

ä¸ºäº†ä¿æŒç®€å•ï¼Œæˆ‘ä»¬ç”¨å° vocab sizeï¼ˆæ¯”å¦‚ 1000ï¼‰ï¼Œå°ç»´åº¦ï¼ˆd_model = 128ï¼‰ï¼Œå°‘é‡å±‚ï¼ˆæ¯”å¦‚ 2 å±‚ï¼‰ï¼Œè·‘ä¸ª toy datasetã€‚æˆ‘ä¼šå†™ä¸ªèƒ½è·‘çš„ä»£ç ï¼Œæ³¨é‡Šæ¸…æ¥šï¼Œç”Ÿæˆå‡ å¥è¯è®©ä½ æ„Ÿå—ä¸‹ GPT çš„ vibeã€‚

---

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# multi-head self-attention
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        self.scale = math.sqrt(self.d_k)
    
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.size()
        q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        k = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        v = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn = F.softmax(scores, dim=-1)
        context = torch.matmul(attn, v)
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        output = self.w_o(context)
        return output

# feedforward layer
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
    
    def forward(self, x):
        return self.linear2(F.relu(self.linear1(x)))

# one transformer decoder layer
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = FeedForward(d_model, d_ff)
        self.norm2 = nn.LayerNorm(d_model)
    
    def forward(self, x, mask=None):
        attn_output = self.attention(x, mask)
        x = self.norm1(x + attn_output)  # residual connection
        ff_output = self.ff(x)
        x = self.norm2(x + ff_output)  # residual connection
        return x

# mini gpt model
class MiniGPT(nn.Module):
    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Parameter(torch.zeros(1, 100, d_model))  # max seq len 100
        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])
        self.linear = nn.Linear(d_model, vocab_size)
        self.d_model = d_model
    
    def forward(self, x, mask=None):
        batch_size, seq_len = x.size()
        x = self.embedding(x) * math.sqrt(self.d_model) + self.pos_encoding[:, :seq_len, :]
        for layer in self.layers:
            x = layer(x, mask)
        return self.linear(x)

# toy dataset and training
def generate_toy_data(vocab_size, seq_len, num_samples):
    return torch.randint(0, vocab_size, (num_samples, seq_len))

# causal mask for autoregressive
def create_mask(seq_len):
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    return (~mask).long()

# training loop
def train(model, data, epochs=10):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    for epoch in range(epochs):
        optimizer.zero_grad()
        inputs = data[:, :-1]
        targets = data[:, 1:]
        mask = create_mask(inputs.size(1))
        outputs = model(inputs, mask)
        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1))
        loss.backward()
        optimizer.step()
        print(f"epoch {epoch+1}, loss: {loss.item():.4f}")

# generate text
def generate(model, start_token, max_len=20):
    model.eval()
    tokens = [start_token]
    with torch.no_grad():
        for _ in range(max_len):
            input_tensor = torch.tensor([tokens])
            mask = create_mask(len(tokens))
            output = model(input_tensor, mask)
            next_token = output[:, -1, :].argmax(-1).item()
            tokens.append(next_token)
            if next_token == 0:  # assume 0 is <EOS>
                break
    return tokens

# run it
if __name__ == "__main__":
    vocab_size = 1000
    d_model = 128
    num_layers = 2
    num_heads = 4
    d_ff = 512
    seq_len = 20
    num_samples = 1000

    model = MiniGPT(vocab_size, d_model, num_layers, num_heads, d_ff)
    data = generate_toy_data(vocab_size, seq_len, num_samples)
    train(model, data)
    generated = generate(model, start_token=42)
    print(f"generated sequence: {generated}")
```

---

### ä»£ç å’‹ç”¨
1. **ç¯å¢ƒ**ï¼šè£…å¥½ PyTorchï¼ˆ`pip install torch`ï¼‰ï¼ŒPython 3.8+ åº”è¯¥æ²¡é—®é¢˜ã€‚
2. **æ•°æ®**ï¼šä»£ç é‡Œç”¨äº†ä¸ª toy datasetï¼ˆéšæœºç”Ÿæˆçš„ token åºåˆ—ï¼‰ï¼Œvocab size è®¾æˆ 1000ã€‚ä½ å¯ä»¥æ¢æˆçœŸçš„ text datasetï¼Œæ¯”å¦‚ç”¨ tokenizerï¼ˆåƒ BPE æˆ–è€… WordPieceï¼‰æŠŠæ–‡æœ¬è½¬æˆ tokenã€‚
3. **è®­ç»ƒ**ï¼šè·‘ `train` å‡½æ•°ï¼Œæ¨¡å‹ä¼šå­¦ç€é¢„æµ‹ä¸‹ä¸€ä¸ª tokenã€‚loss ä¸‹é™è¯´æ˜å®ƒåœ¨å­¦ã€‚
4. **ç”Ÿæˆ**ï¼š`generate` å‡½æ•°ä»ä¸€ä¸ª start token å¼€å§‹ï¼Œåå‡ºä¸€ä¸² tokenã€‚å› ä¸ºæ˜¯ toy æ•°æ®ï¼Œè¾“å‡ºå¯èƒ½æœ‰ç‚¹éšæœºã€‚çœŸç”¨çš„è¯ï¼Œæ¢ä¸ªå¥½ datasetï¼ˆæ¯”å¦‚å°è¯´æˆ–å¯¹è¯ï¼‰ï¼Œæ•ˆæœä¼šæ›´å¥½ã€‚
5. **å‚æ•°**ï¼šd_model = 128ï¼Œnum_heads = 4ï¼Œd_ff = 512ï¼Œnum_layers = 2ï¼Œå°è€Œç¾ï¼Œè·‘å¾—å¿«ã€‚ä½ å¯ä»¥è°ƒå¤§ï¼ˆæ¯”å¦‚ d_model = 256ï¼Œnum_layers = 6ï¼‰æ¥å¢å¼ºæ•ˆæœï¼Œä½†å¾—æœ‰ GPUã€‚

---

### èŠèŠæ„Ÿè§‰
å†™è¿™ä¸ª mini GPT çš„æ—¶å€™ï¼Œæˆ‘å°½é‡è®©å®ƒç®€å•ä½†ä¿ç•™æ ¸å¿ƒï¼šmulti-head attentionã€feedforward layerã€positional encoding å…¨æœ‰äº†ã€‚causal maskï¼ˆé‚£ä¸ª triu çš„ maskï¼‰ä¿è¯ model åªçœ‹ä¹‹å‰çš„ tokenï¼Œé€‚åˆè¯­è¨€å»ºæ¨¡ã€‚è®­ç»ƒç›®æ ‡æ˜¯ cross entropy lossï¼Œç»å…¸çš„â€œé¢„æµ‹ä¸‹ä¸€ä¸ª wordâ€ã€‚

æˆ‘ç¬¬ä¸€æ¬¡æ transformer çš„æ—¶å€™ï¼Œè·‘äº†ä¸ªç±»ä¼¼çš„å°æ¨¡å‹ï¼Œç”Ÿæˆç»“æœä¹±ä¸ƒå…«ç³Ÿï¼Œä½†çœ‹åˆ° loss ä¸‹é™å°±è¶…å…´å¥‹ï¼Œæ„Ÿè§‰åƒè‡ªå·±é€ äº†ä¸ªå° AI å®å®ã€‚ä½ è·‘è¿™ä¸ªä»£ç ï¼Œå¯èƒ½ä¼šå‘ç° toy æ•°æ®ç”Ÿæˆçš„åºåˆ—æ²¡å•¥æ„æ€ï¼ˆæ¯•ç«Ÿæ˜¯éšæœº tokenï¼‰ï¼Œä½†æ¢ä¸ªçœŸæ–‡æœ¬ datasetï¼ˆæ¯”å¦‚ç”¨ NLTK æŠ“ç‚¹è‹±æ–‡å°è¯´ï¼‰ï¼Œå°±èƒ½ç”Ÿæˆæœ‰ç‚¹åƒäººè¯çš„ä¸œè¥¿ï¼

ä½ å’‹çœ‹è¿™ä»£ç ï¼Ÿæƒ³ä¸æƒ³æ”¹ç‚¹å•¥ï¼Œæ¯”å¦‚åŠ  dropout é˜²è¿‡æ‹Ÿåˆï¼Œæˆ–è€…è¯•è¯• beam search è®©ç”Ÿæˆæ›´é è°±ï¼Ÿè¿˜æ˜¯è¯´ä½ æœ‰è‡ªå·±çš„ dataset æƒ³ç”¨ï¼Ÿå¿«å‘Šè¯‰æˆ‘ä½ åœ¨æå•¥ projectï¼Œæˆ‘è¶…æƒ³å¬ï¼æˆ–è€…ä½ æƒ³è®©æˆ‘å†è§£é‡Šä»£ç é‡ŒæŸå—ï¼ˆæ¯”å¦‚ mask å’‹å®ç°çš„ï¼‰ï¼Œéšæ—¶è¯´ï¼