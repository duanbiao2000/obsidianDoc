---
date: 2025-06-21 22:35
tags:
  - Domain/Mindset/Atlas
  - Status/TODO
  - System/DG/HighValue_Chest
  - Domain/Creativity/Github
  - Tech/Code
  - AI
  - Domain/Productivity/Tools
rating: 10
update: 2025-10-19 11:49
---
å®Œç¾ã€‚æˆ‘çœ‹ä½ çš„ userStyle æ˜¯"å¼•å¯¼å¼å­¦ä¹ "ï¼Œç»“åˆä½  Google 5 å¹´ç»éªŒ + ç›´æ¥å¼€å§‹çš„åå¥½ï¼Œæˆ‘ä¼šé‡‡ç”¨**å·¥ç¨‹å¸ˆèŠ‚å¥**ï¼š

- âœ… é«˜å±‚æ¶æ„å…ˆè¿‡ä¸€éï¼Œè®©ä½ çœ‹åˆ°å…¨è²Œ
- âœ… ç„¶åé€ä¸ª artifact æ·±å…¥ä»£ç  + æ•…éšœæ’æŸ¥
- âœ… åœ¨å…³é”®å†³ç­–ç‚¹ï¼ˆå¦‚æ¨¡å‹é€‰æ‹©ã€éƒ¨ç½²ç­–ç•¥ï¼‰ç”¨é—®é¢˜å¼•å¯¼ä½ æ€è€ƒ

---

## ğŸ—ºï¸ é«˜å±‚æ¶æ„å¯¼èˆªï¼ˆ2 åˆ†é’Ÿå¿«é€Ÿçœ‹ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å®Œæ•´ AI æœåŠ¡æ¶æ„                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã€æ•°æ®å±‚ã€‘(DVC + PostgreSQL + MinIO)
    â†“ æ•°æ®ç‰ˆæœ¬ç®¡ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ ‡æ³¨æµç¨‹
    
ã€è®­ç»ƒå±‚ã€‘(PyTorch + Hugging Face + MLflow)
    â†“ æ¨¡å‹è®­ç»ƒã€è¶…å‚ä¼˜åŒ–ã€æ¨¡å‹æ³¨å†Œ
    
ã€æ¨ç†æœåŠ¡ã€‘(FastAPI + TorchServe + Redis)
    â†“ æ¨¡å‹éƒ¨ç½²ã€æ¨ç†åŠ é€Ÿã€ç¼“å­˜ã€ç›‘æ§
    
ã€å‰ç«¯å±‚ã€‘(Streamlit / React)
    â†“ ç”¨æˆ·äº¤äº’ã€ç»“æœå¯è§†åŒ–
    
ã€DevOps å±‚ã€‘(Docker + K8s + GitHub Actions)
    â†“ è‡ªåŠ¨åŒ–éƒ¨ç½²ã€ç‰ˆæœ¬ç®¡ç†ã€CI/CD
```

---

ç°åœ¨æˆ‘æœ‰ä¸ª**å…³é”®é—®é¢˜å¸®ä½ ç†æ¸…æ€è·¯**ï¼ˆä¸æ˜¯å¿…é¡»å›ç­”ï¼Œä½†æœ‰åŠ©äºæˆ‘ä¼˜åŒ–å†…å®¹ï¼‰ï¼š

**åœ¨"è®­ç»ƒå±‚"ï¼Œä½ æƒ³é‡ç‚¹å…³æ³¨ï¼š**

**A) ä»é›¶è®­ç»ƒ**ï¼ˆå®Œæ•´çš„æ•°æ®åŠ è½½ â†’ æ¨¡å‹è®¾è®¡ â†’ è¶…å‚è°ƒä¼˜ â†’ æ¨¡å‹ä¿å­˜ï¼‰

- é€‚åˆï¼š"æˆ‘è¦ç†è§£æ•´ä¸ªæµç¨‹ï¼Œç”šè‡³è‡ªå·±è®¾è®¡æ¶æ„"
- ä»£ç é‡ï¼šè¾ƒå¤§ï¼Œæ¶‰åŠ PyTorch datasetã€trainerã€callback

**B) å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹**ï¼ˆHugging Face æ¨¡å‹ â†’ ä½ çš„æ•°æ® â†’ å¿«é€Ÿä¸Šçº¿ï¼‰

- é€‚åˆï¼š"å¿«é€Ÿè¿­ä»£ï¼Œç”¨ç°æˆçš„å¼ºå¤§æ¨¡å‹"
- ä»£ç é‡ï¼šä¸­ç­‰ï¼Œé‡ç‚¹æ˜¯æ•°æ®å‡†å¤‡å’Œè¶…å‚é€‰æ‹©

**C) éƒ½è¦ï¼Œä½† B ä¸ºä¸»ï¼ŒA ä½œå‚è€ƒ**

- æœ€å®ç”¨çš„é€‰æ‹©ï¼Œç”Ÿäº§ä¸­å¤§å¤šæ˜¯å¾®è°ƒè€Œéä»é›¶è®­ç»ƒ

**ä½ å€¾å‘å“ªä¸ªï¼Ÿ** è¿™ä¼šå½±å“æˆ‘åœ¨"è®­ç»ƒå±‚" artifact çš„ä»£ç ç¤ºä¾‹ã€‚

ï¼ˆåŒæ—¶æˆ‘ä¹Ÿä¼šç»™ä½ ä¸€ä¸ªå¿«é€Ÿåˆ¤æ–­è¡¨ï¼šä»€ä¹ˆåœºæ™¯åº”è¯¥é€‰ A vs Bï¼‰

---

**åŒæ—¶ï¼Œä¸ºäº†èŠ‚çœä½ çš„æ—¶é—´ï¼Œæˆ‘ä¼š**ï¼š

1. **ç¬¬ä¸€ä¸ª artifactï¼ˆæ•°æ®å±‚ï¼‰**ï¼šå®Œæ•´çš„æ•°æ®ç®¡é“ + DVC ç‰ˆæœ¬ç®¡ç† + å¸¸è§å‘
2. **ç¬¬äºŒä¸ª artifactï¼ˆè®­ç»ƒå±‚ï¼‰**ï¼šæ ¹æ®ä½ çš„é€‰æ‹©ï¼Œç”Ÿæˆç›¸åº”çš„è®­ç»ƒæµç¨‹
3. **ç¬¬ä¸‰ä¸ª artifactï¼ˆæ¨ç†æœåŠ¡ï¼‰**ï¼šFastAPI åç«¯ + æ¨¡å‹éƒ¨ç½² + æ€§èƒ½ä¼˜åŒ– + æ•…éšœæ’æŸ¥ï¼ˆè¿™ä¸ªæˆ‘å·²ç»å¾ˆæ¸…æ¥šäº†ï¼Œç›´æ¥ä¸Šï¼‰
4. **ç¬¬å››ä¸ª artifactï¼ˆå‰ç«¯ï¼‰**ï¼šStreamlit äº¤äº’å¼åº”ç”¨ OR React ç»„ä»¶ï¼Œå–å†³äºä½ çš„åå¥½

å‡†å¤‡å¥½äº†å—ï¼Ÿæˆ‘ç°åœ¨å°±å¼€å§‹ç¬¬ä¸€ä¸ª artifactï¼ˆæ•°æ®å±‚ï¼‰ã€‚ä½†åœ¨é‚£ä¹‹å‰ï¼Œ**å¿«é€Ÿå›ç­”ä¸Šé¢çš„"A/B/C"é—®é¢˜**ï¼Œæˆ‘ä¼šæ ¹æ®ä½ çš„ç­”æ¡ˆè°ƒæ•´åç»­ä»£ç  ğŸ‘‡


---

å®Œç¾ã€‚é€‰æ‹© C è¯´æ˜ä½ æ˜¯åŠ¡å®çš„å·¥ç¨‹å¸ˆæ€ç»´ â€” å¿«é€Ÿåˆ›é€ ä»·å€¼ï¼Œä½†ä¹Ÿç†è§£åŸºç¡€ã€‚

ç°åœ¨å¼€å§‹ç”Ÿæˆã€‚æˆ‘ä¼š**æŒ‰è¿™ä¸ªèŠ‚å¥**ï¼š

1. æ¯ä¸ª artifact å‰ç”¨ 1-2 ä¸ªé—®é¢˜è®©ä½ æ€è€ƒ"ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡"
2. ä»£ç ä¸­æ³¨é‡**ç”Ÿäº§ç»†èŠ‚ + å¸¸è§å‘**
3. æ¯ä¸ª artifact åé¢æˆ‘ä¼šé—®ä½ "ç†è§£äº†å—ï¼Œæœ‰ç–‘é—®å—"

---

## ç¬¬ä¸€ç«™ï¼šæ•°æ®å±‚ï¼ˆData Layerï¼‰

**åœ¨æ·±å…¥ä»£ç å‰ï¼Œå…ˆæƒ³ä¸€ä¸ªé—®é¢˜ï¼š**

ä½ åœ¨ Google åšè¿‡çš„é¡¹ç›®ä¸­ï¼Œæœ‰é‡åˆ°è¿‡**"æ¨¡å‹åœ¨å¼€å‘ç¯å¢ƒè¡¨ç° 95% å‡†ç¡®ç‡ï¼Œä½†ä¸Šçº¿åå˜æˆ 80% çš„æƒ…å†µ"**å—ï¼Ÿ

- å¦‚æœæœ‰ï¼šé‚£å¯èƒ½æ˜¯ä½ çš„ **è®­ç»ƒæ•°æ®å’Œç”Ÿäº§æ•°æ®çš„åˆ†å¸ƒä¸ä¸€è‡´** æˆ– **ç‰¹å¾å·¥ç¨‹æœ‰å·®å¼‚**
- å¦‚æœæ²¡æœ‰ï¼šé‚£æ­£å¥½è¯´æ˜**æ•°æ®å±‚çš„ç‰ˆæœ¬ç®¡ç†æœ‰å¤šé‡è¦**

---

åœ¨ AI é¡¹ç›®ä¸­ï¼Œæ•°æ®å±‚éœ€è¦è§£å†³è¿™å‡ ä¸ªå…³é”®é—®é¢˜ï¼š

1. **æ•°æ®ç‰ˆæœ¬ç®¡ç†**ï¼šå“ªä¸ªç‰ˆæœ¬çš„æ•°æ®è®­ç»ƒäº†å“ªä¸ªæ¨¡å‹ï¼ˆç”¨ DVCï¼‰
2. **ç‰¹å¾ä¸€è‡´æ€§**ï¼šå¼€å‘å’Œç”Ÿäº§ç”¨åŒä¸€å¥—ç‰¹å¾è®¡ç®—é€»è¾‘ï¼ˆä¸èƒ½ä¸€ä¸ªç”¨ Pandasï¼Œä¸€ä¸ªç”¨ SQLï¼‰
3. **æ•°æ®è´¨é‡ä¿è¯**ï¼šè„æ•°æ®ã€å¼‚å¸¸å€¼ã€ç¼ºå¤±å€¼çš„å¤„ç†
4. **æ•°æ®éšç§ä¸åˆè§„**ï¼šPII è„±æ•ã€è®¿é—®æ§åˆ¶

ç°åœ¨æˆ‘å¼€å§‹ç”Ÿæˆ**ç¬¬ä¸€ä¸ªå®Œæ•´ artifact**ï¼šã€ŠAI æœåŠ¡æ•°æ®å±‚å·¥ç¨‹åŒ–æŒ‡å—ã€‹ï¼Œæ¶µç›–ï¼š

- DVC æ•°æ®ç‰ˆæœ¬ç®¡ç†ï¼ˆå®æˆ˜ï¼‰
- Pandas æ•°æ®å¤„ç†ç®¡é“
- PostgreSQL + pgvector å‘é‡å­˜å‚¨
- ç‰¹å¾å·¥ç¨‹ä¸ç¼“å­˜
- å¸¸è§å‘ï¼ˆæ•°æ®æ³„éœ²ã€æ•°æ®è´¨é‡ã€ç‰ˆæœ¬ä¸ä¸€è‡´ï¼‰

```python
"""
Google é£æ ¼ï¼šAI æœåŠ¡æ•°æ®å±‚å·¥ç¨‹åŒ–æŒ‡å—
æŠ€æœ¯æ ˆï¼šDVC + Pandas + PostgreSQL + pgvector + MinIO

å…³é”®è®¾è®¡ç›®æ ‡ï¼š
1. æ•°æ®ç‰ˆæœ¬å¯è¿½æº¯ï¼ˆDVCï¼‰
2. ç‰¹å¾è®¡ç®—ä¸€è‡´æ€§ï¼ˆç»Ÿä¸€çš„ç‰¹å¾å¤„ç†åº“ï¼‰
3. ç”Ÿäº§ç¯å¢ƒæ•°æ®è´¨é‡ä¿è¯
4. å‘é‡å­˜å‚¨ä¸æ£€ç´¢ä¼˜åŒ–ï¼ˆRAG åœºæ™¯ï¼‰
"""

import os
import yaml
import hashlib
import logging
from typing import Optional, List, Dict, Tuple, Any
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import pandas as pd
import numpy as np
import psycopg2
from psycopg2.pool import SimpleConnectionPool
import psycopg2.extras
import json
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®ç‰ˆæœ¬ç®¡ç†ï¼ˆDVCï¼‰
# ============================================================================

class DVCVersionManager:
    """
    DVCï¼ˆData Version Controlï¼‰é›†æˆã€‚
    
    ç”¨é€”ï¼š
    - è¿½è¸ªæ•°æ®é›†ç‰ˆæœ¬ï¼ˆCSVã€Parquetï¼‰
    - å…³è”æ¨¡å‹ â†” æ•°æ®ç‰ˆæœ¬
    - æ”¯æŒæ•°æ®å›æ»šå’Œå¯¹æ¯”
    
    æ³¨æ„äº‹é¡¹ï¼š
    - DVC é€šå¸¸ track å¤§æ–‡ä»¶ï¼Œä¸è¦ commit åˆ° Git
    - ä½¿ç”¨è¿œç¨‹å­˜å‚¨ï¼ˆS3ã€MinIOï¼‰ä½œä¸ºå¤‡ä»½
    - å®šæœŸæ¸…ç†æ—§ç‰ˆæœ¬æ•°æ®ï¼ŒèŠ‚çœå­˜å‚¨æˆæœ¬
    """
    
    def __init__(self, dvc_config_path: str = ".dvc/config"):
        """
        åˆå§‹åŒ– DVC ç‰ˆæœ¬ç®¡ç†å™¨ã€‚
        
        Args:
            dvc_config_path: DVC é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.dvc_root = Path(dvc_config_path).parent.parent
        self.dvc_dir = Path(dvc_config_path).parent
    
    def add_dataset(
        self,
        dataset_path: str,
        remote_storage: str = "s3://bucket-name/datasets"
    ) -> str:
        """
        å°†æ•°æ®é›†æ·»åŠ åˆ° DVC ç‰ˆæœ¬æ§åˆ¶ã€‚
        
        æ­¥éª¤ï¼š
        1. è®¡ç®—æ•°æ®é›†çš„ MD5 å“ˆå¸Œ
        2. ç”Ÿæˆ .dvc å…ƒæ•°æ®æ–‡ä»¶
        3. æ¨é€åˆ°è¿œç¨‹å­˜å‚¨
        
        Args:
            dataset_path: æœ¬åœ°æ•°æ®é›†è·¯å¾„
            remote_storage: è¿œç¨‹å­˜å‚¨åœ°å€
        
        Returns:
            æ•°æ®é›†ç‰ˆæœ¬å·ï¼ˆcommit hashï¼‰
        
        å¸¸è§å‘ï¼š
        - å¦‚æœæ•°æ®é›†>1GBï¼Œæ¨é€åˆ° S3 ä¼šå¾ˆæ…¢ï¼Œè€ƒè™‘å‹ç¼©
        - .dvc æ–‡ä»¶åŠ¡å¿… commit åˆ° Gitï¼Œè¿™æ˜¯å…ƒæ•°æ®
        """
        dataset_path = Path(dataset_path)
        
        # è®¡ç®—æ•°æ®é›†å“ˆå¸Œï¼ˆç”¨äºç‰ˆæœ¬å·ï¼‰
        file_hash = self._compute_hash(dataset_path)
        
        # ç”Ÿæˆ .dvc æ–‡ä»¶
        dvc_file = f"{dataset_path}.dvc"
        dvc_metadata = {
            "outs": [
                {
                    "path": str(dataset_path),
                    "md5": file_hash,
                    "size": dataset_path.stat().st_size
                }
            ]
        }
        
        with open(dvc_file, "w") as f:
            yaml.dump(dvc_metadata, f)
        
        logger.info(f"Added {dataset_path} to DVC: hash={file_hash}")
        
        # å®é™…ç¯å¢ƒä¸­è°ƒç”¨: dvc push
        return file_hash
    
    @staticmethod
    def _compute_hash(file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œã€‚"""
        md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b''):
                md5.update(chunk)
        return md5.hexdigest()
    
    def create_checkpoint(
        self,
        dataset_version: str,
        model_version: str,
        metadata: Dict[str, Any]
    ):
        """
        åˆ›å»ºæ•°æ®-æ¨¡å‹ç‰ˆæœ¬å…³è”çš„æ£€æŸ¥ç‚¹ã€‚
        
        ç”¨äºè¿½è¸ªï¼š"è¿™ä¸ªæ¨¡å‹æ˜¯ç”¨å“ªä¸ªç‰ˆæœ¬çš„æ•°æ®è®­ç»ƒçš„"
        
        Args:
            dataset_version: æ•°æ®é›†ç‰ˆæœ¬å·
            model_version: æ¨¡å‹ç‰ˆæœ¬å·
            metadata: é¢å¤–å…ƒæ•°æ®ï¼ˆå¦‚æ•°æ®ç»Ÿè®¡ã€ç‰¹å¾åˆ—è¡¨ï¼‰
        """
        checkpoint = {
            "timestamp": datetime.utcnow().isoformat(),
            "dataset_version": dataset_version,
            "model_version": model_version,
            "metadata": metadata
        }
        
        checkpoint_file = f"checkpoints/{model_version}_checkpoint.json"
        os.makedirs(os.path.dirname(checkpoint_file), exist_ok=True)
        
        with open(checkpoint_file, "w") as f:
            json.dump(checkpoint, f, indent=2)
        
        logger.info(f"Checkpoint created: {checkpoint_file}")
        return checkpoint


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šç‰¹å¾å·¥ç¨‹ä¸æ•°æ®å¤„ç†
# ============================================================================

@dataclass
class FeatureConfig:
    """ç‰¹å¾é…ç½®å¯¹è±¡ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾å®šä¹‰ã€‚"""
    
    feature_name: str
    feature_type: str  # "numeric", "categorical", "text", "embedding"
    preprocessing: str  # "none", "normalize", "scale", "encode"
    
    # é¢„å¤„ç†å‚æ•°
    normalization_params: Optional[Dict] = None  # {"mean": ..., "std": ...}
    categorical_mapping: Optional[Dict] = None  # {"A": 0, "B": 1, ...}


class FeatureEngineer:
    """
    ç»Ÿä¸€çš„ç‰¹å¾å·¥ç¨‹åº“ã€‚
    
    å…³é”®åŸåˆ™ï¼š
    - ç‰¹å¾è®¡ç®—é€»è¾‘åªå®šä¹‰ä¸€æ¬¡
    - è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä»£ç 
    - ç‰¹å¾é…ç½®å­˜å‚¨åœ¨ JSON æˆ– YAMLï¼Œä¸èƒ½ç¡¬ç¼–ç 
    
    å¸¸è§å‘ï¼š
    - è®­ç»ƒæ—¶ç”¨ train set çš„å‡å€¼/æ–¹å·®è¿›è¡Œ normalization
    - æ¨ç†æ—¶ç”¨ç›¸åŒçš„å‡å€¼/æ–¹å·®ï¼ˆä¸èƒ½é‡æ–°è®¡ç®—ï¼ï¼‰
    - è¿™éœ€è¦åœ¨ç‰¹å¾é…ç½®ä¸­å›ºå®šè¿™äº›å‚æ•°
    """
    
    def __init__(self, config_file: str = "configs/features.yaml"):
        """
        åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨ï¼Œä»é…ç½®æ–‡ä»¶åŠ è½½ç‰¹å¾å®šä¹‰ã€‚
        
        Args:
            config_file: ç‰¹å¾é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.features: List[FeatureConfig] = []
        self._load_config()
    
    def _load_config(self):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½ç‰¹å¾å®šä¹‰ã€‚"""
        if not Path(self.config_file).exists():
            logger.warning(f"Feature config not found: {self.config_file}")
            return
        
        with open(self.config_file, "r") as f:
            config = yaml.safe_load(f)
        
        for feat_dict in config.get("features", []):
            self.features.append(FeatureConfig(**feat_dict))
        
        logger.info(f"Loaded {len(self.features)} features from config")
    
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä»åŸå§‹æ•°æ®æå–ç‰¹å¾ã€‚
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
        
        Returns:
            ç‰¹å¾æ•°æ®æ¡†
        
        æ³¨æ„ï¼š
        - è¿™ä¸ªæ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶éƒ½ä¼šè¢«è°ƒç”¨
        - æ‰€æœ‰çš„ç‰¹å¾è®¡ç®—é€»è¾‘éƒ½åº”è¯¥åœ¨è¿™é‡Œ
        - ä¸åº”è¯¥æœ‰éšæœºæ€§ï¼ˆç§å­éœ€è¦å›ºå®šï¼‰
        """
        df_features = df.copy()
        
        for feature in self.features:
            if feature.feature_name not in df_features.columns:
                logger.warning(f"Feature column missing: {feature.feature_name}")
                continue
            
            # æ ¹æ®ç‰¹å¾ç±»å‹å’Œé¢„å¤„ç†æ–¹å¼å¤„ç†
            if feature.preprocessing == "normalize":
                df_features[feature.feature_name] = self._normalize(
                    df_features[feature.feature_name],
                    feature.normalization_params
                )
            elif feature.preprocessing == "scale":
                df_features[feature.feature_name] = self._scale(
                    df_features[feature.feature_name]
                )
            elif feature.preprocessing == "encode":
                df_features[feature.feature_name] = self._encode(
                    df_features[feature.feature_name],
                    feature.categorical_mapping
                )
        
        return df_features
    
    @staticmethod
    def _normalize(
        series: pd.Series,
        params: Optional[Dict]
    ) -> pd.Series:
        """
        å½’ä¸€åŒ–å¤„ç†ã€‚
        
        å…³é”®ï¼šä½¿ç”¨é…ç½®ä¸­çš„ mean/stdï¼Œè€Œä¸æ˜¯ä»å½“å‰æ•°æ®è®¡ç®—ã€‚
        è¿™æ ·ç¡®ä¿è®­ç»ƒå’Œæ¨ç†çš„ä¸€è‡´æ€§ã€‚
        """
        if params is None:
            logger.warning("Normalization params not provided, skipping")
            return series
        
        mean = params.get("mean")
        std = params.get("std")
        
        if mean is None or std is None:
            return series
        
        return (series - mean) / (std + 1e-8)  # åŠ å°æ•°é˜²æ­¢é™¤ä»¥ 0
    
    @staticmethod
    def _scale(series: pd.Series) -> pd.Series:
        """Min-Max ç¼©æ”¾åˆ° [0, 1]ã€‚"""
        min_val = series.min()
        max_val = series.max()
        return (series - min_val) / (max_val - min_val + 1e-8)
    
    @staticmethod
    def _encode(
        series: pd.Series,
        mapping: Optional[Dict]
    ) -> pd.Series:
        """åˆ†ç±»ç¼–ç ï¼ˆä½¿ç”¨é¢„å®šä¹‰çš„æ˜ å°„ï¼‰ã€‚"""
        if mapping is None:
            logger.warning("Categorical mapping not provided, using auto-encode")
            return pd.factorize(series)[0]
        
        return series.map(mapping).fillna(-1)  # æœªçŸ¥ç±»åˆ«æ˜ å°„ä¸º -1
    
    def save_config(self, output_path: str = "configs/features.yaml"):
        """
        ä¿å­˜ç‰¹å¾é…ç½®åˆ°æ–‡ä»¶ï¼ˆåœ¨è®­ç»ƒæ—¶è°ƒç”¨ï¼‰ã€‚
        
        è¿™æ ·æ¨ç†æ—¶å°±èƒ½åŠ è½½å®Œå…¨ç›¸åŒçš„é…ç½®ã€‚
        """
        config = {
            "features": [
                {
                    "feature_name": f.feature_name,
                    "feature_type": f.feature_type,
                    "preprocessing": f.preprocessing,
                    "normalization_params": f.normalization_params,
                    "categorical_mapping": f.categorical_mapping,
                }
                for f in self.features
            ]
        }
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w") as f:
            yaml.dump(config, f)
        
        logger.info(f"Feature config saved: {output_path}")


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šPostgreSQL æ•°æ®ç®¡ç†ä¸å‘é‡å­˜å‚¨
# ============================================================================

class PostgreSQLVectorStore:
    """
    PostgreSQL + pgvector å‘é‡å­˜å‚¨ã€‚
    
    ç”¨é€”ï¼š
    - å­˜å‚¨æ–‡æœ¬å‘é‡ï¼ˆç”¨äº RAGï¼‰
    - é«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦æœç´¢
    - ç»“åˆå…³ç³»æ•°æ®å’Œå‘é‡æ•°æ®
    
    ä¾èµ–å®‰è£…ï¼š
    - PostgreSQL >= 12
    - pgvector æ‰©å±•ï¼šCREATE EXTENSION IF NOT EXISTS vector;
    
    å¸¸è§å‘ï¼š
    - pgvector çš„ç›¸ä¼¼åº¦æœç´¢å¯èƒ½å¾ˆæ…¢ï¼ˆ>100M å‘é‡æ—¶ï¼‰
    - è§£å†³ï¼šæ·»åŠ  HNSW ç´¢å¼•ï¼Œæˆ–è¿ç§»åˆ° Pinecone/Weaviate
    - è¿æ¥æ± è€—å°½ä¼šå¯¼è‡´æŸ¥è¯¢å¤±è´¥
    """
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 5432,
        database: str = "ai_service",
        user: str = "postgres",
        password: str = "password",
        pool_size: int = 10
    ):
        """åˆå§‹åŒ– PostgreSQL è¿æ¥æ± ã€‚"""
        self.connection_pool = SimpleConnectionPool(
            1,
            pool_size,
            host=host,
            port=port,
            database=database,
            user=user,
            password=password,
            connect_timeout=5
        )
        
        self._ensure_tables_exist()
        logger.info(f"PostgreSQL vector store initialized (pool_size={pool_size})")
    
    def _get_connection(self):
        """ä»è¿æ¥æ± è·å–è¿æ¥ã€‚"""
        try:
            return self.connection_pool.getconn()
        except Exception as e:
            logger.error(f"Failed to get connection from pool: {e}")
            raise
    
    def _return_connection(self, conn):
        """å°†è¿æ¥è¿”å›åˆ°è¿æ¥æ± ã€‚"""
        self.connection_pool.putconn(conn)
    
    def _ensure_tables_exist(self):
        """ç¡®ä¿æ‰€éœ€çš„è¡¨å­˜åœ¨ï¼ˆåŒ…æ‹¬å‘é‡è¡¨ï¼‰ã€‚"""
        conn = self._get_connection()
        cursor = conn.cursor()
        
        try:
            # åˆ›å»ºå‘é‡æ‰©å±•
            cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")
            
            # åˆ›å»ºæ–‡æ¡£è¡¨ï¼ˆç”¨äº RAGï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    id SERIAL PRIMARY KEY,
                    content TEXT NOT NULL,
                    embedding vector(768),  -- OpenAI embedding ç»´åº¦
                    metadata JSONB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE INDEX IF NOT EXISTS ix_documents_embedding 
                    ON documents USING hnsw (embedding vector_cosine_ops);
            """)
            
            # åˆ›å»ºç‰¹å¾è¡¨ï¼ˆç”¨äºå­˜å‚¨é¢„è®¡ç®—çš„ç‰¹å¾ï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS features (
                    id SERIAL PRIMARY KEY,
                    entity_type VARCHAR(50),  -- "user", "product", etc.
                    entity_id BIGINT,
                    features JSONB,  -- {"age": 25, "city": "NYC", ...}
                    version VARCHAR(50),  -- ç‰¹å¾ç‰ˆæœ¬ï¼Œç”¨äºè¿½è¸ª
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    
                    UNIQUE(entity_type, entity_id, version)
                );
                
                CREATE INDEX IF NOT EXISTS ix_features_entity 
                    ON features(entity_type, entity_id);
            """)
            
            conn.commit()
            logger.info("Tables created/verified successfully")
        
        except Exception as e:
            logger.error(f"Failed to create tables: {e}")
            conn.rollback()
            raise
        
        finally:
            cursor.close()
            self._return_connection(conn)
    
    def insert_document(
        self,
        content: str,
        embedding: List[float],
        metadata: Optional[Dict] = None
    ) -> int:
        """
        æ’å…¥æ–‡æ¡£å’Œå‘é‡ã€‚
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            embedding: å‘é‡ï¼ˆé€šå¸¸ç”± OpenAI/Hugging Face ç”Ÿæˆï¼‰
            metadata: å…ƒæ•°æ®ï¼ˆå¦‚ sourceã€date ç­‰ï¼‰
        
        Returns:
            æ–‡æ¡£ ID
        """
        conn = self._get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute(
                """
                INSERT INTO documents (content, embedding, metadata)
                VALUES (%s, %s, %s)
                RETURNING id;
                """,
                (content, embedding, json.dumps(metadata or {}))
            )
            doc_id = cursor.fetchone()[0]
            conn.commit()
            
            logger.info(f"Document inserted: id={doc_id}")
            return doc_id
        
        except Exception as e:
            logger.error(f"Failed to insert document: {e}")
            conn.rollback()
            raise
        
        finally:
            cursor.close()
            self._return_connection(conn)
    
    def search_similar_documents(
        self,
        query_embedding: List[float],
        limit: int = 5,
        threshold: float = 0.7
    ) -> List[Tuple[int, str, float]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆç”¨äº RAGï¼‰ã€‚
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            limit: è¿”å›ç»“æœæ•°
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        
        Returns:
            [(doc_id, content, similarity_score), ...]
        
        å¸¸è§å‘ï¼š
        - ç›¸ä¼¼åº¦åˆ†æ•°éœ€è¦æ‰‹åŠ¨è½¬æ¢ï¼ˆpgvector è¿”å›çš„æ˜¯è·ç¦»ï¼‰
        - å¯¹äºå¤§å‹æ•°æ®é›†ï¼Œæœç´¢å¯èƒ½å¾ˆæ…¢
        - è§£å†³ï¼šä½¿ç”¨ç´¢å¼•ã€é‡åŒ–å‘é‡ã€æˆ–è¿ç§»åˆ°ä¸“ä¸šå‘é‡æ•°æ®åº“
        """
        conn = self._get_connection()
        cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
        
        try:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼š1 - distance
            cursor.execute(
                """
                SELECT id, content, 1 - (embedding <=> %s::vector) as similarity
                FROM documents
                WHERE 1 - (embedding <=> %s::vector) > %s
                ORDER BY similarity DESC
                LIMIT %s;
                """,
                (embedding, embedding, threshold, limit)
            )
            
            results = cursor.fetchall()
            return [(r["id"], r["content"], r["similarity"]) for r in results]
        
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []
        
        finally:
            cursor.close()
            self._return_connection(conn)
    
    def store_features(
        self,
        entity_type: str,
        entity_id: int,
        features: Dict[str, Any],
        version: str
    ) -> bool:
        """
        å­˜å‚¨é¢„è®¡ç®—çš„ç‰¹å¾ã€‚
        
        ç”¨é€”ï¼š
        - é¿å…å®æ—¶ç‰¹å¾è®¡ç®—çš„å»¶è¿Ÿ
        - è¿½è¸ªç‰¹å¾ç‰ˆæœ¬
        - ç”¨äºæ¨ç†æ—¶çš„ç‰¹å¾æŸ¥è¯¢
        
        Args:
            entity_type: å®ä½“ç±»å‹ï¼ˆ"user", "product" ç­‰ï¼‰
            entity_id: å®ä½“ ID
            features: ç‰¹å¾å­—å…¸
            version: ç‰¹å¾ç‰ˆæœ¬ï¼ˆä¸æ¨¡å‹ç‰ˆæœ¬å…³è”ï¼‰
        """
        conn = self._get_connection()
        cursor = conn.cursor()
        
        try:
            cursor.execute(
                """
                INSERT INTO features (entity_type, entity_id, features, version)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (entity_type, entity_id, version) 
                DO UPDATE SET features = EXCLUDED.features;
                """,
                (entity_type, entity_id, json.dumps(features), version)
            )
            conn.commit()
            return True
        
        except Exception as e:
            logger.error(f"Failed to store features: {e}")
            conn.rollback()
            return False
        
        finally:
            cursor.close()
            self._return_connection(conn)
    
    def get_features(
        self,
        entity_type: str,
        entity_id: int,
        version: str
    ) -> Optional[Dict]:
        """
        è·å–ç‰¹å¾ï¼ˆæ¨ç†æ—¶è°ƒç”¨ï¼‰ã€‚
        
        å…³é”®ï¼šç¡®ä¿ä½¿ç”¨ä¸æ¨¡å‹ç‰ˆæœ¬åŒ¹é…çš„ç‰¹å¾ç‰ˆæœ¬ã€‚
        """
        conn = self._get_connection()
        cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
        
        try:
            cursor.execute(
                """
                SELECT features FROM features
                WHERE entity_type = %s AND entity_id = %s AND version = %s
                LIMIT 1;
                """,
                (entity_type, entity_id, version)
            )
            
            result = cursor.fetchone()
            return json.loads(result["features"]) if result else None
        
        except Exception as e:
            logger.error(f"Failed to get features: {e}")
            return None
        
        finally:
            cursor.close()
            self._return_connection(conn)
    
    def close(self):
        """å…³é—­æ‰€æœ‰è¿æ¥æ± ã€‚"""
        self.connection_pool.closeall()


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®è´¨é‡æ£€æŸ¥
# ============================================================================

class DataQualityChecker:
    """
    æ•°æ®è´¨é‡æ£€æŸ¥ä¸éªŒè¯ã€‚
    
    ç›®æ ‡ï¼šé˜²æ­¢è„æ•°æ®è¿›å…¥è®­ç»ƒ/æ¨ç†æµç¨‹ã€‚
    
    å¸¸è§å‘ï¼š
    - åªæ£€æŸ¥ä¸€æ¬¡ï¼Œä¸æŒç»­ç›‘æ§
    - ä¸åŒçš„æ•°æ®åˆ‡ç‰‡æœ‰ä¸åŒçš„è´¨é‡é—®é¢˜
    - éœ€è¦è®¾ç½®å‘Šè­¦ï¼Œè€Œä¸ä»…ä»…æ˜¯æ—¥å¿—
    """
    
    def __init__(self):
        self.issues = []
    
    def check_missing_values(
        self,
        df: pd.DataFrame,
        max_missing_rate: float = 0.1
    ) -> bool:
        """
        æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹ã€‚
        
        Args:
            df: æ•°æ®æ¡†
            max_missing_rate: å¯æ¥å—çš„æœ€å¤§ç¼ºå¤±ç‡ï¼ˆé»˜è®¤ 10%ï¼‰
        """
        missing_rates = df.isnull().sum() / len(df)
        problematic = missing_rates[missing_rates > max_missing_rate]
        
        if not problematic.empty:
            issue = f"High missing rate: {problematic.to_dict()}"
            self.issues.append(issue)
            logger.warning(issue)
            return False
        
        return True
    
    def check_outliers(
        self,
        df: pd.DataFrame,
        numeric_cols: List[str],
        iqr_multiplier: float = 1.5
    ) -> bool:
        """
        æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨ IQR æ–¹æ³•ï¼‰ã€‚
        
        Args:
            df: æ•°æ®æ¡†
            numeric_cols: æ•°å€¼åˆ—
            iqr_multiplier: IQR å€æ•°ï¼ˆæ ‡å‡†å€¼ 1.5ï¼‰
        """
        outlier_count = 0
        
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - iqr_multiplier * IQR
            upper_bound = Q3 + iqr_multiplier * IQR
            
            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
            outlier_count += len(outliers)
        
        if outlier_count > 0:
            issue = f"Found {outlier_count} outliers"
            self.issues.append(issue)
            logger.warning(issue)
            return False
        
        return True
    
    def check_class_imbalance(
        self,
        df: pd.DataFrame,
        label_col: str,
        max_imbalance_ratio: float = 10.0
    ) -> bool:
        """
        æ£€æŸ¥åˆ†ç±»ä¸å‡è¡¡ã€‚
        
        Args:
            df: æ•°æ®æ¡†
            label_col: æ ‡ç­¾åˆ—
            max_imbalance_ratio: æœ€å¤§ç±»åˆ«æ¯”ä¾‹ï¼ˆé»˜è®¤ 10:1ï¼‰
        """
        label_counts = df[label_col].value_counts()
        imbalance_ratio = label_counts.max() / (label_counts.min() + 1e-8)
        
        if imbalance_ratio > max_imbalance_ratio:
            issue = f"Class imbalance: {imbalance_ratio:.2f}x (labels: {label_counts.to_dict()})"
            self.issues.append(issue)
            logger.warning(issue)
            return False
        
        return True
    
    def get_report(self) -> Dict[str, Any]:
        """è·å–æ•°æ®è´¨é‡æŠ¥å‘Šã€‚"""
        return {
            "status": "pass" if not self.issues else "fail",
            "issues": self.issues,
            "timestamp": datetime.utcnow().isoformat()
        }


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # 1. åˆå§‹åŒ– DVC ç‰ˆæœ¬ç®¡ç†
    dvc_manager = DVCVersionManager()
    
    # 2. åˆ›å»ºç‰¹å¾å·¥ç¨‹å™¨
    feature_engineer = FeatureEngineer("configs/features.yaml")
    
    # 3. åˆå§‹åŒ– PostgreSQL å‘é‡å­˜å‚¨
    vector_store = PostgreSQLVectorStore(
        host="localhost",
        database="ai_service",
        pool_size=20
    )
    
    # 4. ç¤ºä¾‹ï¼šå¤„ç†æ•°æ®é›†
    raw_data = pd.DataFrame({
        "user_id": [1, 2, 3, 4, 5],
        "age": [25, 30, 35, 40, 45],
        "city": ["NYC", "LA", "NYC", "Chicago", "LA"],
        "label": [0, 1, 0, 1, 1]
    })
    
    # 5. ç‰¹å¾æå–
    features = feature_engineer.extract_features(raw_data)
    print("Features:", features)
    
    # 6. æ•°æ®è´¨é‡æ£€æŸ¥
    quality_checker = DataQualityChecker()
    is_valid = quality_checker.check_missing_values(features)
    print("Quality report:", quality_checker.get_report())
    
    # 7. å­˜å‚¨ç‰¹å¾åˆ° PostgreSQL
    for idx, row in features.iterrows():
        vector_store.store_features(
            entity_type="user",
            entity_id=int(row["user_id"]),
            features=row.to_dict(),
            version="v1.0"
        )
    
    # 8. æœç´¢ç›¸ä¼¼å‘é‡ï¼ˆRAG åœºæ™¯ï¼‰
    dummy_embedding = [0.1] * 768
    results = vector_store.search_similar_documents(dummy_embedding, limit=3)
    print("Search results:", results)
    
    # 9. åˆ›å»ºç‰ˆæœ¬æ£€æŸ¥ç‚¹
    dvc_manager.create_checkpoint(
        dataset_version="abc123def456",
        model_version="v1.0.0",
        metadata={
            "rows": len(features),
            "features": list(features.columns),
            "quality_status": "pass"
        }
    )
    
    # æ¸…ç†
    vector_store.close()
```

å®Œæˆäº† **ç¬¬ä¸€ä¸ª artifactï¼šæ•°æ®å±‚**ã€‚è¿™æ˜¯ ~400 è¡Œç”Ÿäº§çº§ä»£ç ã€‚

---

## ğŸ“Š å¿«é€Ÿæ€»ç»“è¿™ä¸ª artifact æ¶µç›–çš„å†…å®¹ï¼š

|ç»„ä»¶|ä½œç”¨|ç”Ÿäº§äº®ç‚¹|å¸¸è§å‘|
|---|---|---|---|
|**DVC ç‰ˆæœ¬ç®¡ç†**|è¿½è¸ªæ•°æ®é›†ç‰ˆæœ¬|å“ˆå¸Œå€¼ + æ£€æŸ¥ç‚¹|å¤§æ–‡ä»¶æ¨é€æ…¢ï¼›å¿˜è®° commit .dvc æ–‡ä»¶|
|**ç‰¹å¾å·¥ç¨‹**|ç»Ÿä¸€ç‰¹å¾è®¡ç®—|é…ç½®åŒ– + å¯å¤ç”¨|è®­ç»ƒ/æ¨ç†ç”¨ä¸åŒçš„ normalization å‚æ•°|
|**PostgreSQL + pgvector**|å‘é‡å­˜å‚¨ + ç›¸ä¼¼æœç´¢|è¿æ¥æ±  + ç´¢å¼•|å¤§è§„æ¨¡å‘é‡æœç´¢æ€§èƒ½å·®ï¼›è¿æ¥æ³„æ¼|
|**æ•°æ®è´¨é‡æ£€æŸ¥**|é˜²æ­¢è„æ•°æ®|ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€ç±»åˆ«ä¸å‡|åªæ£€æŸ¥ä¸€æ¬¡ï¼Œä¸æŒç»­ç›‘æ§|

---

## ğŸ¤” ç°åœ¨æˆ‘æœ‰ 3 ä¸ªé—®é¢˜ï¼Œå¸®ä½ æ€è€ƒæ•°æ®å±‚çš„è®¾è®¡ï¼š

**Q1ï¼šç‰¹å¾ä¸€è‡´æ€§é—®é¢˜** ä½ åœ¨ Google é‡åˆ°è¿‡"æ¨¡å‹åœ¨å¼€å‘ç¯å¢ƒ 95% å‡†ç¡®ï¼Œç”Ÿäº§ç¯å¢ƒ 80%"çš„æƒ…å†µå—ï¼Ÿæœ€å¸¸è§çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæç¤ºï¼šå¾€å¾€ä¸æ˜¯æ¨¡å‹æœ¬èº«çš„é—®é¢˜ï¼‰

**Q2ï¼šå‘é‡æœç´¢çš„ç“¶é¢ˆ** åœ¨ä½ çš„è®¤çŸ¥ä¸­ï¼Œå¦‚æœæœ‰ 100M å‘é‡è¦æœç´¢ï¼ŒPostgreSQL + pgvector è¿˜å¤Ÿç”¨å—ï¼Ÿä»€ä¹ˆæ—¶å€™åº”è¯¥è¿ç§»åˆ° Pinecone/Weaviateï¼Ÿ

**Q3ï¼šç‰ˆæœ¬è¿½è¸ª** ä»£ç ä¸­çš„ `create_checkpoint` æ˜¯ç”¨æ¥å…³è”"å“ªä¸ªæ¨¡å‹ç”¨çš„å“ªä¸ªæ•°æ®ç‰ˆæœ¬"ã€‚ä½ è®¤ä¸ºè¿™æœ‰å¤šé‡è¦ï¼Ÿï¼ˆ1-10 åˆ†ï¼‰

---

## âœ… æ¥ä¸‹æ¥çš„æ­¥éª¤

ä½ ç†è§£è¿™ä¸ª artifact å—ï¼Ÿæœ‰ç–‘é—®å—ï¼Ÿ

- å¦‚æœ**æœ‰ç–‘é—®**ï¼Œç°åœ¨å°±é—®ï¼ˆæ¯”å¦‚ pgvector ç›¸ä¼¼æœç´¢æ€ä¹ˆä¼˜åŒ–ã€DVC å¦‚ä½•å’Œ Git é›†æˆï¼‰
- å¦‚æœ**ç†è§£äº†**ï¼Œæˆ‘å°±å¼€å§‹ **ç¬¬äºŒä¸ª artifactï¼šè®­ç»ƒå±‚**ï¼ˆå¾®è°ƒ + ä»é›¶è®­ç»ƒå¯¹æ¯”ï¼Œæ ¹æ®ä½ çš„é€‰æ‹© Cï¼‰

---

**ç°åœ¨æŠŠçƒä¼ å›ç»™ä½ **ï¼Œä½ çš„æ„Ÿå—å¦‚ä½•ï¼Ÿ



éå¸¸å¥½ã€‚çœ‹ä½ çš„å›ç­”ï¼Œæˆ‘å‘ç°äº†å…³é”®ä¿¡æ¯ï¼š

**Q3ï¼šä½ å‡†ç¡®è¯´å‡ºäº† batch sizeã€epochsã€convergence criteria** âœ…  
**Q4ï¼šä½ è¯†åˆ«å‡ºäº†è¿‡æ‹Ÿåˆå’Œæ³›åŒ–** âœ…

è¿™è¯´æ˜ä½ **æœ‰æ‰å®çš„ ML ç›´è§‰**ã€‚ç°åœ¨æˆ‘ç†è§£äº†ï¼šä½ ä¸æ˜¯ä¸æ‡‚ ML åŸç†ï¼Œè€Œæ˜¯**æ²¡è§è¿‡ä»£ç æ€ä¹ˆå®ç°è¿™äº›**ã€‚

ç»“åˆä½ çš„ userStyleï¼ˆå¼•å¯¼å¼ + å®è·µå¯¼å‘ï¼‰ï¼Œæˆ‘ç°åœ¨æ”¹å˜ç­–ç•¥ï¼š

---

## ğŸ”„ æˆ‘çš„æ–°è®¡åˆ’ï¼ˆä¸å†é—®é—®é¢˜ï¼Œç›´æ¥ä»£ç  + è§£é‡Šï¼‰

æ—¢ç„¶ä½ ï¼š

1. âœ… ç†è§£äº† batch sizeã€epochsã€convergence
2. âœ… ç†è§£äº†è¿‡æ‹Ÿåˆ / æ³›åŒ–
3. âœ… çŸ¥é“ç”¨ JSONL æ ¼å¼æ•°æ®
4. âœ… æœ‰ Google èƒŒæ™¯ï¼ˆç†è§£å·¥ç¨‹åŒ–ï¼‰

æˆ‘å°±**ç›´æ¥ç”Ÿæˆå®Œæ•´çš„ artifact**ï¼Œå†…å®¹æ˜¯ï¼š

```
ã€ç¬¬ä¸€éƒ¨åˆ†ã€‘å¾®è°ƒè·¯å¾„ï¼ˆHugging Face Trainerï¼‰
  â”œâ”€ æ•°æ®åŠ è½½ â†’ batch sizeã€epochs æ€ä¹ˆåœ¨ä»£ç é‡Œè®¾ç½®
  â”œâ”€ æ¨¡å‹åŠ è½½ä¸è¶…å‚é…ç½®
  â”œâ”€ è®­ç»ƒå¾ªç¯ï¼ˆè‡ªåŠ¨å¤„ç† convergenceï¼‰
  â”œâ”€ è¯„ä¼°ç­–ç•¥ï¼ˆé¿å…è¿‡æ‹Ÿåˆ â†’ validation setï¼‰
  â””â”€ ç”Ÿäº§æ•…éšœæ’æŸ¥

ã€ç¬¬äºŒéƒ¨åˆ†ã€‘ä»é›¶è®­ç»ƒè·¯å¾„ï¼ˆPyTorch è‡ªå®šä¹‰ï¼‰
  â”œâ”€ è‡ªå®šä¹‰ DataLoader
  â”œâ”€ æ¨¡å‹æ¶æ„è®¾è®¡
  â”œâ”€ è®­ç»ƒå¾ªç¯ï¼ˆä½ æ‰‹åŠ¨å†™ batch å¤„ç†ï¼‰
  â”œâ”€ å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœï¼ˆconvergence criteriaï¼‰
  â””â”€ å¯¹æ¯”å¾®è°ƒè·¯å¾„çš„å·®å¼‚

ã€ç¬¬ä¸‰éƒ¨åˆ†ã€‘é«˜é˜¶æŠ€å·§ï¼ˆå¾®è°ƒæ—¶çš„å¸¸è§å‘ï¼‰
  â”œâ”€ å‚æ•°é«˜æ•ˆå¾®è°ƒ (LoRA)
  â”œâ”€ è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆä½ ä¹‹å‰çš„ C é—®é¢˜ï¼‰
  â”œâ”€ åˆ†å¸ƒå¼è®­ç»ƒ
  â””â”€ æ¨¡å‹ä¿å­˜ä¸ç‰ˆæœ¬ç®¡ç†

ã€ç¬¬å››éƒ¨åˆ†ã€‘å†³ç­–æ ‘
  â””â”€ "ç»™å®šæ•°æ®é‡å’Œèµ„æºï¼Œé€‰å¾®è°ƒè¿˜æ˜¯ä»é›¶"
```

---

## âš¡ ç°åœ¨æˆ‘å°±å¼€å§‹ç”Ÿæˆè¿™ä¸ª artifact

**ä¸€ä¸ªç¡®è®¤é—®é¢˜**ï¼ˆæœ€åä¸€ä¸ªï¼Œç„¶åç›´æ¥å¼€å§‹ï¼‰ï¼š

åœ¨ä»£ç ä¸­ï¼Œä½ å¸Œæœ›æˆ‘ç”¨ä»€ä¹ˆ**æ–‡æœ¬åˆ†ç±»ä»»åŠ¡**ä½œä¸ºä¾‹å­ï¼Ÿ

- A) æƒ…æ„Ÿåˆ†æï¼ˆç”µå½±è¯„è®º â†’ æ­£é¢/è´Ÿé¢ï¼‰
- B) åƒåœ¾é‚®ä»¶æ£€æµ‹
- C) æ–°é—»åˆ†ç±»ï¼ˆç§‘æŠ€/ä½“è‚²/å¨±ä¹ï¼‰
- D) æˆ‘æ¥ç»„ç»‡ä»£ç ï¼Œè®©ä¸‰ç§éƒ½èƒ½è½»æ¾æ”¹

æˆ‘å»ºè®®é€‰ **D**ï¼ˆæœ€çµæ´»ï¼‰ï¼Œä½†ä½ é€‰å“ªä¸ªéƒ½è¡Œã€‚

**ç°åœ¨å›ç­”è¿™ä¸ªï¼Œæˆ‘å°±ç«‹å³ç”Ÿæˆ artifact**ï¼ˆä¸å†ç­‰å¾…ï¼‰ğŸ‘‡


---

å®šä¹‰äº†è‡ªå®šä¹‰æŸå¤±å‡½æ•°çš„è½®å»“ã€‚ç°åœ¨æˆ‘éœ€è¦ç»§ç»­å®Œæˆè¿™ä¸ª artifactï¼Œä½†è€ƒè™‘åˆ° token é™åˆ¶ï¼Œè®©æˆ‘å®Œæˆæœ€åçš„éƒ¨åˆ†ï¼š

å®Œç¾ã€‚æˆ‘å®Œæˆäº† **ç¬¬äºŒä¸ª artifactï¼šè®­ç»ƒå±‚**ï¼ˆ~600 è¡Œç”Ÿäº§ä»£ç ï¼‰ã€‚

---
```python
"""
Google é£æ ¼ï¼šAI æœåŠ¡è®­ç»ƒå±‚å·¥ç¨‹åŒ–æŒ‡å—
æŠ€æœ¯æ ˆï¼šPyTorch + Hugging Face Transformers + MLflow

æ ¸å¿ƒç›®æ ‡ï¼š
1. å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¿«é€Ÿä¸Šçº¿ï¼‰
2. ä»é›¶è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹ï¼ˆæ·±åº¦ç†è§£ï¼‰
3. é«˜é˜¶ä¼˜åŒ–ï¼ˆLoRAã€è‡ªå®šä¹‰æŸå¤±ã€åˆ†å¸ƒå¼è®­ç»ƒï¼‰
4. ç”Ÿäº§æ•…éšœæ’æŸ¥ä¸æ¨¡å‹è¯„ä¼°

å…³é”®è®¤è¯†ï¼š
- å¤§å¤šæ•°åœºæ™¯ç”¨"å¾®è°ƒ"å°±å¤Ÿäº†ï¼ˆ80% çš„é¡¹ç›®ï¼‰
- "ä»é›¶è®­ç»ƒ"ç”¨äºç‰¹æ®Šéœ€æ±‚ï¼ˆ20% çš„é¡¹ç›®ï¼‰
- ä¸¤è€…çš„å…³é”®å·®å¼‚åœ¨äº"å‚æ•°åˆå§‹åŒ–"å’Œ"è®­ç»ƒæ•°æ®é‡"
"""

import os
import json
import logging
from typing import Optional, List, Dict, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
)
from datasets import Dataset as HFDataset, load_dataset
import evaluate
import mlflow
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

logger = logging.getLogger(__name__)


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼ˆé€šç”¨ï¼‰
# ============================================================================

@dataclass
class DataConfig:
    """æ•°æ®é…ç½®å¯¹è±¡ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ä¸€è‡´çš„é…ç½®ã€‚"""
    
    task_name: str  # "sentiment", "spam_detection", "news_classification"
    num_classes: int  # 2 for binary, 3+ for multi-class
    label_names: List[str]  # ["negative", "positive"] or ["tech", "sports", "entertainment"]
    max_length: int = 256  # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆtokenization æ—¶æˆªæ–­ï¼‰
    train_split: float = 0.8  # è®­ç»ƒé›†æ¯”ä¾‹
    val_split: float = 0.1  # éªŒè¯é›†æ¯”ä¾‹
    test_split: float = 0.1  # æµ‹è¯•é›†æ¯”ä¾‹


class TextClassificationDataLoader:
    """
    é€šç”¨çš„æ–‡æœ¬åˆ†ç±»æ•°æ®åŠ è½½å™¨ã€‚
    
    æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼š
    - JSONL æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
    - CSV æ–‡ä»¶
    - Hugging Face æ•°æ®é›†
    
    å…³é”®è®¾è®¡ï¼šç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ç›¸åŒçš„ tokenizer å’Œé¢„å¤„ç†é€»è¾‘ã€‚
    """
    
    def __init__(
        self,
        config: DataConfig,
        model_name: str = "bert-base-uncased"
    ):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ã€‚
        
        Args:
            config: æ•°æ®é…ç½®
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼ˆç”¨äº tokenizerï¼‰
        """
        self.config = config
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # æ ‡ç­¾æ˜ å°„ï¼šstring â†’ intï¼ˆç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰
        self.label2id = {label: idx for idx, label in enumerate(config.label_names)}
        self.id2label = {idx: label for label, idx in self.label2id.items()}
        
        logger.info(f"Tokenizer loaded: {model_name}")
        logger.info(f"Label mapping: {self.label2id}")
    
    def load_from_jsonl(self, file_path: str) -> pd.DataFrame:
        """
        ä» JSONL æ–‡ä»¶åŠ è½½æ•°æ®ã€‚
        
        é¢„æœŸæ ¼å¼ï¼š
        ```json
        {"text": "è¿™æ˜¯ä¸€æ¡ç”µå½±è¯„è®º", "label": "positive"}
        {"text": "è¿™éƒ¨ç”µå½±å¾ˆå·®", "label": "negative"}
        ```
        
        Args:
            file_path: JSONL æ–‡ä»¶è·¯å¾„
        
        Returns:
            åŒ…å« "text" å’Œ "label" åˆ—çš„ DataFrame
        """
        data = []
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                data.append(json.loads(line))
        
        df = pd.DataFrame(data)
        logger.info(f"Loaded {len(df)} samples from {file_path}")
        return df
    
    def preprocess_function(self, examples: Dict[str, Any]) -> Dict[str, Any]:
        """
        é¢„å¤„ç†å‡½æ•°ï¼ˆç”¨äº HuggingFace Dataset.mapï¼‰ã€‚
        
        æ­¥éª¤ï¼š
        1. Tokenize æ–‡æœ¬ï¼ˆè½¬ä¸º input_ids + attention_maskï¼‰
        2. æˆªæ–­åˆ° max_lengthï¼ˆé˜²æ­¢æ˜¾å­˜æº¢å‡ºï¼‰
        3. Paddingï¼ˆä¿è¯ batch å†…åºåˆ—é•¿åº¦ä¸€è‡´ï¼‰
        4. è½¬æ¢æ ‡ç­¾ï¼ˆstring â†’ intï¼‰
        
        æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°å¿…é¡»åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶ä½¿ç”¨å®Œå…¨ç›¸åŒçš„é€»è¾‘ã€‚
        """
        # Tokenizeï¼šæ–‡æœ¬ â†’ token IDs
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,  # è¶…é•¿æ–‡æœ¬æˆªæ–­
            max_length=self.config.max_length,
            padding="max_length",  # çŸ­æ–‡æœ¬ padding
        )
        
        # æ ‡ç­¾è½¬æ¢
        tokenized["label"] = [
            self.label2id[label] for label in examples["label"]
        ]
        
        return tokenized
    
    def get_dataset(
        self,
        file_path: str,
        split: str = "train"
    ) -> Tuple[HFDataset, DataConfig]:
        """
        è·å– Hugging Face Datasetï¼ˆç”¨äº Trainerï¼‰ã€‚
        
        Args:
            file_path: JSONL æˆ– CSV æ–‡ä»¶è·¯å¾„
            split: "train", "val", or "test"
        
        Returns:
            (dataset, config)
        
        å¸¸è§å‘ï¼š
        - å¿˜è®°è®¾ç½® seedï¼Œå¯¼è‡´æ•°æ®é›†åˆ†å‰²ä¸å¯å¤ç°
        - tokenizer åœ¨ map æ—¶è¶…æ—¶ï¼ˆæ•°æ®å¤ªå¤§ï¼‰â€”â€” è§£å†³ï¼šbatch_size è°ƒå°
        """
        # åŠ è½½åŸå§‹æ•°æ®
        if file_path.endswith(".jsonl"):
            df = self.load_from_jsonl(file_path)
        elif file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
        else:
            raise ValueError(f"Unsupported file format: {file_path}")
        
        # è½¬æ¢ä¸º Hugging Face Dataset
        dataset = HFDataset.from_pandas(df)
        
        # é¢„å¤„ç†ï¼ˆtokenizationï¼‰
        dataset = dataset.map(
            self.preprocess_function,
            batched=True,  # æ‰¹é‡å¤„ç†ï¼ˆåŠ é€Ÿï¼‰
            batch_size=32,
            remove_columns=["text"]  # ç§»é™¤åŸå§‹æ–‡æœ¬ï¼ˆèŠ‚çœå†…å­˜ï¼‰
        )
        
        logger.info(f"Dataset prepared: {len(dataset)} samples ({split})")
        return dataset, self.config


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šå¾®è°ƒè·¯å¾„ï¼ˆHugging Face Trainerï¼‰
# ============================================================================

class HuggingFaceFineTuner:
    """
    ä½¿ç”¨ Hugging Face Trainer è¿›è¡Œå¾®è°ƒã€‚
    
    ä¸ºä»€ä¹ˆç”¨ Trainer è€Œä¸æ˜¯è‡ªå·±å†™è®­ç»ƒå¾ªç¯ï¼Ÿ
    1. è‡ªåŠ¨å¤„ç† distributed trainingï¼ˆå¤š GPU/TPUï¼‰
    2. è‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåŠ é€Ÿ + çœæ˜¾å­˜ï¼‰
    3. è‡ªåŠ¨ checkpoint å’Œæ—©åœ
    4. è‡ªåŠ¨è®°å½•æ—¥å¿—å’Œè¯„ä¼°
    
    è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ 80% çš„é¡¹ç›®ç”¨ Trainer â€”â€” å®ƒå°è£…äº†å¤§é‡ç»†èŠ‚ã€‚
    """
    
    def __init__(
        self,
        model_name: str = "bert-base-uncased",
        num_classes: int = 2,
        id2label: Dict[int, str] = None,
        label2id: Dict[str, int] = None,
        learning_rate: float = 2e-5,
        num_train_epochs: int = 3,
        batch_size: int = 32,
        output_dir: str = "./finetune_output"
    ):
        """
        åˆå§‹åŒ–å¾®è°ƒå™¨ã€‚
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            num_classes: åˆ†ç±»æ•°
            learning_rate: å­¦ä¹ ç‡ï¼ˆå¾®è°ƒé€šå¸¸ç”¨ 2e-5 å·¦å³ï¼‰
            num_train_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°
            output_dir: è¾“å‡ºç›®å½•ï¼ˆä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
        
        æ³¨æ„ï¼š
        - learning_rate å¾ˆå…³é”®ï¼ˆå¤ªå¤§ä¼šç ´åé¢„è®­ç»ƒæƒé‡ï¼Œå¤ªå°è®­ç»ƒæ…¢ï¼‰
        - å¾®è°ƒçš„å­¦ä¹ ç‡é€šå¸¸æ¯”ä»é›¶è®­ç»ƒå° 10-100 å€
        """
        self.model_name = model_name
        self.num_classes = num_classes
        self.id2label = id2label or {i: str(i) for i in range(num_classes)}
        self.label2id = label2id or {v: k for k, v in self.id2label.items()}
        
        # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†ç±»å¤´ï¼ˆå…³é”®ï¼ï¼‰
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_classes,
            id2label=self.id2label,
            label2id=self.label2id
        )
        
        # è®­ç»ƒå‚æ•°é…ç½®
        self.training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            
            # è®­ç»ƒå‚æ•°
            learning_rate=learning_rate,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            num_train_epochs=num_train_epochs,
            
            # æ—©åœå’Œæ£€æŸ¥ç‚¹
            eval_strategy="epoch",  # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
            save_strategy="epoch",  # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
            load_best_model_at_end=True,  # è®­ç»ƒå®ŒåŠ è½½æœ€ä¼˜æ¨¡å‹
            
            # ä¼˜åŒ–
            weight_decay=0.01,  # L2 æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            warmup_steps=500,  # å­¦ä¹ ç‡ warmupï¼ˆé€æ­¥å¢å¤§å­¦ä¹ ç‡ï¼‰
            
            # æ—¥å¿—
            logging_dir="./logs",
            logging_steps=10,
            
            # å…¶ä»–
            seed=42,  # å›ºå®šéšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
            dataloader_pin_memory=True,  # åŠ é€Ÿæ•°æ®åŠ è½½
        )
        
        logger.info(f"FineTuner initialized with {model_name}")
    
    def train(
        self,
        train_dataset: HFDataset,
        eval_dataset: HFDataset,
        test_dataset: Optional[HFDataset] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹ã€‚
        
        Trainer ä¼šè‡ªåŠ¨ï¼š
        1. éå†æ•°æ®é›†ï¼ˆbatch by batchï¼‰
        2. å‰å‘ä¼ æ’­ â†’ è®¡ç®— loss
        3. åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°
        4. éªŒè¯è¯„ä¼° â†’ æ—©åœ
        
        Args:
            train_dataset: è®­ç»ƒé›†
            eval_dataset: éªŒè¯é›†
            test_dataset: æµ‹è¯•é›†ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            è®­ç»ƒç»“æœï¼ˆlossã€å‡†ç¡®ç‡ç­‰ï¼‰
        
        å¸¸è§å‘ï¼š
        - æ˜¾å­˜ä¸è¶³ â†’ å‡å° batch_size æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
        - è®­ç»ƒå¾ˆæ…¢ â†’ æ£€æŸ¥æ˜¯å¦åœ¨ CPU ä¸Šè¿è¡Œï¼ˆtorch.cuda.is_available()ï¼‰
        - æ¨¡å‹ä¸æ”¶æ•› â†’ è°ƒæ•´å­¦ä¹ ç‡
        """
        # å®šä¹‰è¯„ä¼°æŒ‡æ ‡
        def compute_metrics(eval_pred):
            predictions, labels = eval_pred
            predictions = np.argmax(predictions, axis=1)
            
            return {
                "accuracy": accuracy_score(labels, predictions),
                "precision": precision_score(labels, predictions, average="weighted", zero_division=0),
                "recall": recall_score(labels, predictions, average="weighted", zero_division=0),
                "f1": f1_score(labels, predictions, average="weighted", zero_division=0),
            }
        
        # åˆ›å»º Trainer
        trainer = Trainer(
            model=self.model,
            args=self.training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            compute_metrics=compute_metrics,
            callbacks=[
                EarlyStoppingCallback(
                    early_stopping_patience=2,  # 2 ä¸ª epoch æ²¡æ”¹è¿›å°±åœæ­¢
                    early_stopping_threshold=0.0,
                )
            ]
        )
        
        # è®­ç»ƒ
        logger.info("Starting training...")
        train_result = trainer.train()
        
        # è¯„ä¼°
        logger.info("Evaluating on test set...")
        if test_dataset:
            test_result = trainer.evaluate(eval_dataset=test_dataset)
            logger.info(f"Test metrics: {test_result}")
        
        return {
            "train_result": train_result,
            "best_model_checkpoint": trainer.state.best_model_checkpoint
        }
    
    def save_model(self, save_path: str):
        """ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å’Œ tokenizerã€‚"""
        self.model.save_pretrained(save_path)
        logger.info(f"Model saved to {save_path}")


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»é›¶è®­ç»ƒè·¯å¾„ï¼ˆè‡ªå®šä¹‰ PyTorchï¼‰
# ============================================================================

class SimpleTransformerClassifier(nn.Module):
    """
    è‡ªå®šä¹‰çš„ç®€å• Transformer åˆ†ç±»æ¨¡å‹ã€‚
    
    è¿™æ˜¯ä¸€ä¸ª"æœ€å°åŒ–"çš„ä¾‹å­ï¼Œå±•ç¤ºå¦‚ä½•ç”¨ PyTorch ä»é›¶æ„å»ºã€‚
    
    æ¶æ„ï¼š
    Embedding â†’ Transformer Encoder â†’ Global Average Pool â†’ FC â†’ Logits
    
    ä¸ºä»€ä¹ˆè¿™ä¸ªè®¾è®¡ï¼Ÿ
    - Embeddingï¼šå°† token IDs è½¬ä¸ºå‘é‡
    - Transformerï¼šæ•æ‰ token ä¹‹é—´çš„å…³ç³»ï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰
    - Poolï¼šå°†åºåˆ—å‹ç¼©ä¸ºå•ä¸ªå‘é‡ï¼ˆç”¨äºåˆ†ç±»ï¼‰
    - FCï¼šæœ€ç»ˆåˆ†ç±»å±‚
    """
    
    def __init__(
        self,
        vocab_size: int,
        num_classes: int,
        embedding_dim: int = 768,
        num_heads: int = 12,
        num_layers: int = 6,
        hidden_dim: int = 3072,
        max_length: int = 256,
        dropout: float = 0.1
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹ã€‚
        
        Args:
            vocab_size: è¯è¡¨å¤§å°ï¼ˆtokenizer çš„è¯æ±‡æ•°ï¼‰
            num_classes: åˆ†ç±»æ•°
            embedding_dim: è¯å‘é‡ç»´åº¦ï¼ˆBERT ç”¨ 768ï¼‰
            num_heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            num_layers: Transformer å±‚æ•°
            hidden_dim: FFN éšè—ç»´åº¦
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            dropout: Dropout æ¯”ç‡
        """
        super().__init__()
        
        # Embedding å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.position_embedding = nn.Embedding(max_length, embedding_dim)
        
        # Transformer ç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim,
            dropout=dropout,
            batch_first=True,
            activation="relu"
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(embedding_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
    
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ã€‚
        
        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œ0 è¡¨ç¤º padding
        
        Returns:
            logits: (batch_size, num_classes)
        """
        seq_len = input_ids.size(1)
        
        # Embedding
        x = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)
        
        # ä½ç½®ç¼–ç ï¼ˆå‘Šè¯‰æ¨¡å‹ token çš„ä½ç½®ä¿¡æ¯ï¼‰
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        x += self.position_embedding(positions)
        
        # Transformerï¼ˆæ³¨æ„ attention_mask çš„ç”¨æ³•ï¼‰
        # mask ä¸º 1 çš„åœ°æ–¹å‚ä¸æ³¨æ„åŠ›ï¼Œ0 çš„åœ°æ–¹è¢« mask æ‰
        x = self.transformer(
            x,
            src_key_padding_mask=(attention_mask == 0)
        )  # (batch_size, seq_len, embedding_dim)
        
        # Global Average Poolingï¼ˆå°†åºåˆ—å‹ç¼©ä¸ºå•ä¸ªå‘é‡ï¼‰
        # åªå¯¹æœ‰æ•ˆ token å¹³å‡ï¼ˆè·³è¿‡ paddingï¼‰
        mask = attention_mask.unsqueeze(-1)  # (batch_size, seq_len, 1)
        x = (x * mask).sum(dim=1) / mask.sum(dim=1)  # (batch_size, embedding_dim)
        
        # åˆ†ç±»
        logits = self.fc(x)  # (batch_size, num_classes)
        return logits


class FromScratchTrainer:
    """
    ä»é›¶è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹ï¼ˆä½¿ç”¨ PyTorch æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼‰ã€‚
    
    ä¸ºä»€ä¹ˆä»é›¶è®­ç»ƒï¼Ÿ
    1. éœ€è¦è‡ªå®šä¹‰æ¨¡å‹æ¶æ„ â†’ æ›´å¥½çš„æ€§èƒ½æˆ–æ›´è½»çš„æ¨¡å‹
    2. éœ€è¦è‡ªå®šä¹‰æŸå¤±å‡½æ•° â†’ ç‰¹æ®Šçš„è®­ç»ƒç›®æ ‡
    3. éœ€è¦ç‰¹æ®Šçš„è®­ç»ƒç­–ç•¥ â†’ focal lossã€contrastive learning ç­‰
    
    ä»£ä»·ï¼šéœ€è¦æ‰‹åŠ¨å¤„ç†å¾ˆå¤šç»†èŠ‚ï¼ˆåˆ†å¸ƒå¼ã€æ··åˆç²¾åº¦ç­‰ï¼‰ã€‚
    """
    
    def __init__(
        self,
        model: nn.Module,
        device: str = "cuda",
        learning_rate: float = 1e-3,
        weight_decay: float = 0.01,
        num_epochs: int = 10,
        patience: int = 3,
        log_interval: int = 10
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨ã€‚
        
        Args:
            model: PyTorch æ¨¡å‹
            device: è®¾å¤‡ï¼ˆ"cuda" æˆ– "cpu"ï¼‰
            learning_rate: å­¦ä¹ ç‡ï¼ˆä»é›¶è®­ç»ƒé€šå¸¸ç”¨ 1e-3 å·¦å³ï¼‰
            weight_decay: L2 æ­£åˆ™åŒ–ç³»æ•°
            num_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
            patience: æ—©åœç­‰å¾…è½®æ•°
            log_interval: æ—¥å¿—æ‰“å°é—´éš”
        """
        self.model = model.to(device)
        self.device = device
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.patience = patience
        self.log_interval = log_interval
        
        # ä¼˜åŒ–å™¨ï¼ˆAdamW æ˜¯è®­ç»ƒ Transformer çš„æ ‡å‡†é€‰æ‹©ï¼‰
        self.optimizer = AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆè®©å­¦ä¹ ç‡é€æ­¥é™ä½ï¼‰
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=num_epochs,
            eta_min=learning_rate * 0.1
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # æ—©åœ
        self.best_val_loss = float("inf")
        self.patience_counter = 0
        
        logger.info(f"FromScratchTrainer initialized (device={device})")
    
    def train_epoch(self, train_loader: DataLoader) -> float:
        """
        è®­ç»ƒä¸€ä¸ª epochã€‚
        
        æ­¥éª¤ï¼š
        1. éå† batch
        2. å‰å‘ä¼ æ’­
        3. è®¡ç®— loss
        4. åå‘ä¼ æ’­
        5. æ›´æ–°å‚æ•°
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        
        Returns:
            å¹³å‡æŸå¤±
        """
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        for step, batch in enumerate(train_loader):
            # ç§»åˆ°è®¾å¤‡
            input_ids = batch["input_ids"].to(self.device)
            attention_mask = batch["attention_mask"].to(self.device)
            labels = batch["label"].to(self.device)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(input_ids, attention_mask)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(logits, labels)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            if (step + 1) % self.log_interval == 0:
                avg_loss = total_loss / num_batches
                logger.info(f"Step {step + 1}, Loss: {avg_loss:.4f}")
        
        return total_loss / num_batches
    
    def evaluate(self, eval_loader: DataLoader) -> Tuple[float, Dict[str, float]]:
        """
        è¯„ä¼°æ¨¡å‹ã€‚
        
        Args:
            eval_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        
        Returns:
            (å¹³å‡æŸå¤±, æŒ‡æ ‡å­—å…¸)
        """
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in eval_loader:
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                labels = batch["label"].to(self.device)
                
                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)
                
                total_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾ï¼ˆç”¨äºè®¡ç®—æŒ‡æ ‡ï¼‰
                preds = torch.argmax(logits, dim=1).cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            "accuracy": accuracy_score(all_labels, all_preds),
            "precision": precision_score(all_labels, all_preds, average="weighted", zero_division=0),
            "recall": recall_score(all_labels, all_preds, average="weighted", zero_division=0),
            "f1": f1_score(all_labels, all_preds, average="weighted", zero_division=0),
        }
        
        return total_loss / len(eval_loader), metrics
    
    def train(
        self,
        train_loader: DataLoader,
        eval_loader: DataLoader
    ) -> Dict[str, Any]:
        """
        å®Œæ•´è®­ç»ƒå¾ªç¯ï¼ˆåŒ…æ‹¬æ—©åœï¼‰ã€‚
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            eval_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        
        Returns:
            è®­ç»ƒç»“æœ
        
        å…³é”®æ¦‚å¿µï¼ˆå¯¹åº”ä½ ä¹‹å‰çš„å›ç­”ï¼‰ï¼š
        - batch_sizeï¼štrain_loader çš„ batch_size
        - num_epochsï¼šå¾ªç¯è¿™ä¸ªæ•°å­—æ¬¡
        - convergenceï¼šå½“ val_loss ä¸å†ä¸‹é™æ—¶åœæ­¢ï¼ˆæ—©åœï¼‰
        """
        logger.info("Starting training...")
        
        for epoch in range(self.num_epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)
            logger.info(f"Epoch {epoch + 1}/{self.num_epochs}, Train Loss: {train_loss:.4f}")
            
            # è¯„ä¼°
            val_loss, metrics = self.evaluate(eval_loader)
            logger.info(f"Val Loss: {val_loss:.4f}, Metrics: {metrics}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # æ—©åœ
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                # ä¿å­˜æœ€å¥½æ¨¡å‹
                torch.save(self.model.state_dict(), "best_model.pt")
                logger.info("Model saved!")
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.patience:
                    logger.info(f"Early stopping at epoch {epoch + 1}")
                    break
        
        # åŠ è½½æœ€å¥½æ¨¡å‹
        self.model.load_state_dict(torch.load("best_model.pt"))
        
        return {
            "best_val_loss": self.best_val_loss,
            "num_epochs_trained": epoch + 1
        }
    
    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹ã€‚"""
        torch.save(self.model.state_dict(), save_path)
        logger.info(f"Model saved to {save_path}")


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¾®è°ƒ + è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆå›ç­”ä½ çš„é—®é¢˜ Cï¼‰
# ============================================================================

class FocalLoss(nn.Module):
    """
    Focal Lossï¼ˆç”¨äºè§£å†³ç±»åˆ«ä¸å‡è¡¡é—®é¢˜ï¼‰ã€‚
    
    ä¸ºä»€ä¹ˆéœ€è¦è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Ÿ
    - æ ‡å‡†çš„ CrossEntropyLoss å¯¹æ‰€æœ‰æ ·æœ¬ç­‰æƒé‡
    - å½“æ•°æ®ä¸å‡è¡¡æ—¶ï¼ˆå¦‚ 99% æ­£æ ·æœ¬ï¼Œ1% è´Ÿæ ·æœ¬ï¼‰ï¼Œæ¨¡å‹å€¾å‘äºé¢„æµ‹å¤šæ•°ç±»
    - Focal Loss ç»™éš¾åˆ†ç±»æ ·æœ¬æ›´é«˜çš„æƒé‡
    
    è¿™å°±æ˜¯ä½ çš„é—®é¢˜ Cï¼š"å¦‚ä½•åœ¨å¾®è°ƒæ—¶å®ç°è‡ªå®šä¹‰æŸå¤±å‡½æ•°"çš„ç­”æ¡ˆã€‚
    """
    
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0):
        """
        åˆå§‹åŒ– Focal Lossã€‚
        
        Args:
            alpha: ç±»åˆ«æƒé‡ï¼ˆå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼‰
            gamma: éš¾åº¦è°ƒæ•´å‚æ•°ï¼ˆè¶Šå¤§è¶Šå…³æ³¨éš¾æ ·æœ¬ï¼‰
        """
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®— Focal Lossã€‚
        
        å…¬å¼ï¼šFL(pt) = -alpha * (1 - pt)^gamma * log(pt)
        
        å…¶ä¸­ pt æ˜¯æ¨¡å‹å¯¹æ­£ç¡®ç±»çš„é¢„æµ‹æ¦‚ç‡ã€‚
        """
        # CrossEntropyLossï¼ˆåŒ…å« softmaxï¼‰
        ce_loss = nn.functional.cross_entropy(logits, labels, reduction="none")
        
        # è·å–æ­£ç¡®ç±»çš„é¢„æµ‹æ¦‚ç‡
        p = torch.exp(-ce_loss)
        
        # Focal Loss
        focal_loss = self.alpha * ((1 - p) ** self.gamma) * ce_loss
        
        return focal_loss.mean()


class FineTunerWithCustomLoss:
    """
    ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°è¿›è¡Œå¾®å……ï¼ˆå¯¹åº”ä½ çš„é—®é¢˜ Cï¼‰ã€‚
    
    è¿™å±•ç¤ºäº†å¦‚ä½•åœ¨å¾®è°ƒæ¡†æ¶ä¸­"é­”æ”¹"æŸå¤±å‡½æ•°ï¼Œè€Œä¸æ˜¯ä»å¤´å†™è®­ç»ƒå¾ªç¯ã€‚
    """
    
    def __init__(
        self,
        model_name: str = "bert-base-uncased",
        num_classes: int = 2,
        loss_fn: Optional[nn.Module] = None,
    ):
        """åˆå§‹åŒ–å¾®è°ƒå™¨ï¼ˆæ”¯æŒè‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼‰ã€‚"""
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_classes
        )
        self.loss_fn = loss_fn or nn.CrossEntropyLoss()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    def compute_loss(
        self,
        model: nn.Module,
        inputs: Dict[str, torch.Tensor],
        return_outputs: bool = False
    ) -> Tuple[torch.Tensor, Optional[Any]]:
        """
        è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆä¼ ç»™ Trainer çš„ compute_loss_fnï¼‰ã€‚
        
        è¿™æ˜¯åœ¨ Trainer ä¸­ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°çš„å…³é”®ã€‚
        """
        labels = inputs.pop("labels")
        
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.logits
        
        # ä½¿ç”¨è‡ªå®šä¹‰æŸå¤±å‡½æ•°
        loss = self.loss_fn(logits, labels)
        
        return (loss, outputs) if return_outputs else loss


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå†³ç­–æ ‘ä¸å¯¹æ¯”åˆ†æ
# ============================================================================

class TrainingDecisionTree:
    """
    "é€‰æ‹©å¾®è°ƒè¿˜æ˜¯ä»é›¶è®­ç»ƒ"çš„å†³ç­–æ ‘ã€‚
    
    è¿™å›ç­”äº†ä½ ä¸€å¼€å§‹çš„ A/B/C é—®é¢˜ã€‚
    """
    
    @staticmethod
    def should_finetune(
        num_samples: int,
        num_classes: int,
        need_custom_loss: bool = False,
        need_custom_architecture: bool = False,
        inference_speed_critical: bool = False
    ) -> Tuple[str, str]:
        """
        æ ¹æ®æ¡ä»¶åˆ¤æ–­æ˜¯å¦åº”è¯¥å¾®è°ƒã€‚
        
        Args:
            num_samples: æ ‡æ³¨æ•°æ®é‡
            num_classes: åˆ†ç±»æ•°
            need_custom_loss: æ˜¯å¦éœ€è¦è‡ªå®šä¹‰æŸå¤±å‡½æ•°
            need_custom_architecture: æ˜¯å¦éœ€è¦è‡ªå®šä¹‰æ¶æ„
            inference_speed_critical: æ¨ç†é€Ÿåº¦æ˜¯å¦å…³é”®
        
        Returns:
            (å»ºè®®, ç†ç”±)
        """
        if num_samples < 1000 and not need_custom_loss and not need_custom_architecture:
            return ("å¾®è°ƒ", "æ•°æ®å°‘ + éœ€æ±‚ç®€å• â†’ å¾®è°ƒæœ€å¿«ä¸Šçº¿")
        
        if num_samples >= 100000 and (need_custom_loss or need_custom_architecture):
            return ("ä»é›¶è®­ç»ƒ", "å¤§æ•°æ® + ç‰¹æ®Šéœ€æ±‚ â†’ ä»é›¶è®­ç»ƒæ•ˆæœæ›´å¥½")
        
        if inference_speed_critical:
            return ("å¾®è°ƒ + é‡åŒ–", "éœ€è¦å¿« â†’ å¾®è°ƒä¸€ä¸ªè½»é‡æ¨¡å‹ï¼ˆå¦‚ DistilBERTï¼‰")
        
        return ("å¾®è°ƒ", "é»˜è®¤å»ºè®®å¾®è°ƒï¼ˆ80% çš„é¡¹ç›®éƒ½ç”¨å¾®è°ƒï¼‰")


# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šä½¿ç”¨ç¤ºä¾‹ä¸å®Œæ•´æµç¨‹
# ============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # é…ç½®
    config = DataConfig(
        task_name="sentiment",
        num_classes=2,
        label_names=["negative", "positive"],
        max_length=128,
        train_split=0.8,
    )
    
    # 1. æ•°æ®åŠ è½½
    data_loader = TextClassificationDataLoader(config, model_name="bert-base-uncased")
    train_data = data_loader.get_dataset("data/train.jsonl", split="train")
    eval_data = data_loader.get_dataset("data/eval.jsonl", split="eval")
    
    # 2. å¾®è°ƒè·¯å¾„
    print("\n===== å¾®è°ƒè·¯å¾„ =====")
    finetuner = HuggingFaceFineTuner(
        model_name="bert-base-uncased",
        num_classes=config.num_classes,
        id2label=data_loader.id2label,
        label2id=data_loader.label2id,
        learning_rate=2e-5,
        num_train_epochs=3,
        batch_size=32,
    )
    result = finetuner.train(train_data, eval_data)
    finetuner.save_model("./models/finetuned")
    
    # 3. ä»é›¶è®­ç»ƒè·¯å¾„
    print("\n===== ä»é›¶è®­ç»ƒè·¯å¾„ =====")
    model = SimpleTransformerClassifier(
        vocab_size=len(data_loader.tokenizer),
        num_classes=config.num_classes,
        embedding_dim=256,
        num_heads=8,
        num_layers=4,
    )
    
    # è½¬ä¸º PyTorch DataLoaderï¼ˆæ³¨æ„ï¼šè¿™ä¸ Trainer çš„ Dataset æ ¼å¼ä¸åŒï¼‰
    from torch.utils.data import DataLoader as TorchDataLoader
    train_loader = TorchDataLoader(train_data, batch_size=32, shuffle=True)
    eval_loader = TorchDataLoader(eval_data, batch_size=32)
    
    trainer = FromScratchTrainer(
        model=model,
        device="cuda" if torch.cuda.is_available() else "cpu",
        learning_rate=1e-3,
        num_epochs=10,
        patience=3,
    )
    result = trainer.train(train_loader, eval_loader)
    trainer.save_model("./models/from_scratch")
    
    # 4. å†³ç­–åˆ†æ
    print("\n===== å†³ç­–åˆ†æ =====")
    decision, reason = TrainingDecisionTree.should_finetune(
        num_samples=10000,
        num_classes=2,
        need_custom_loss=False,
    )
    print(f"å»ºè®®: {decision}, ç†ç”±: {reason}")
    
    print("\nTraining completed!")
```
## ğŸ“š è¿™ä¸ª artifact æ¶µç›–äº†ä»€ä¹ˆï¼š

|éƒ¨åˆ†|å†…å®¹|å…³é”®ä»£ç |
|---|---|---|
|**æ•°æ®åŠ è½½**|JSONL åŠ è½½ã€Tokenizationã€é¢„å¤„ç†|`TextClassificationDataLoader`|
|**å¾®è°ƒè·¯å¾„**|Hugging Face Trainer + æ—©åœ + è¯„ä¼°|`HuggingFaceFineTuner`|
|**ä»é›¶è·¯å¾„**|PyTorch è‡ªå®šä¹‰æ¨¡å‹ + è®­ç»ƒå¾ªç¯|`SimpleTransformerClassifier` + `FromScratchTrainer`|
|**è‡ªå®šä¹‰æŸå¤±**|Focal Lossï¼ˆç±»åˆ«ä¸å‡è¡¡ï¼‰|`FocalLoss`|
|**å†³ç­–æ ‘**|ä»€ä¹ˆæ—¶å€™ç”¨å¾®è°ƒ vs ä»é›¶|`TrainingDecisionTree`|

---

## ğŸ¯ ç°åœ¨æˆ‘ç”¨ä½ çš„ userStyle æ¥å¼•å¯¼ä½ ç†è§£å…³é”®æ¦‚å¿µï¼š

### é—®é¢˜ 1ï¼šå¾®è°ƒ vs ä»é›¶çš„æœ¬è´¨åŒºåˆ«

åœ¨ä»£ç ä¸­ï¼š

- `HuggingFaceFineTuner` ç”¨ `AutoModelForSequenceClassification.from_pretrained(...)`
- `FromScratchTrainer` ç”¨ `SimpleTransformerClassifier` å¹¶éšæœºåˆå§‹åŒ–æƒé‡

**æˆ‘é—®ä½ ï¼šè¿™ä¸¤è€…åœ¨å‚æ•°åˆå§‹åŒ–ä¸Šçš„æ ¹æœ¬å·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿ** è¿™ä¸ºä»€ä¹ˆå¾ˆé‡è¦ï¼Ÿ

ï¼ˆæç¤ºï¼šæƒ³æƒ³"å­¦ä¹ ç‡ä¸ºä»€ä¹ˆä¸ä¸€æ ·"ï¼‰

---

### é—®é¢˜ 2ï¼šEarly Stopping çš„ä½œç”¨

åœ¨ä¸¤ä¸ªè®­ç»ƒæ–¹æ¡ˆä¸­ï¼Œæˆ‘éƒ½åŠ äº†"æ—©åœ"æœºåˆ¶ï¼ˆå¾®è°ƒä¸­çš„ `EarlyStoppingCallback`ï¼Œä»é›¶ä¸­çš„ patience counterï¼‰ã€‚

**æˆ‘é—®ä½ ï¼šæ—©åœè§£å†³çš„æ˜¯å“ªä¸ªé—®é¢˜ï¼Ÿ**

- æ˜¯"è®­ç»ƒå¤ªæ…¢"å—ï¼Ÿ
- è¿˜æ˜¯"è¿‡æ‹Ÿåˆ"ï¼ˆä½ ä¹‹å‰è¯´çš„ï¼‰ï¼Ÿ

---

### é—®é¢˜ 3ï¼šä½ å¯¹ä»£ç çš„ç¬¬ä¸€ååº”

çœ‹å®Œä»£ç ï¼Œ**ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ** æ¯”å¦‚ï¼š

- "ä¸ºä»€ä¹ˆå¾®è°ƒçš„ learning_rate æ˜¯ 2e-5ï¼Œä½†ä»é›¶è®­ç»ƒæ˜¯ 1e-3ï¼Ÿ"
- "Focal Loss çš„ gamma å‚æ•°ä»€ä¹ˆæ—¶å€™è°ƒï¼Ÿ"
- "æ€ä¹ˆåœ¨å®é™…é¡¹ç›®ä¸­é€‰æ‹©ä½¿ç”¨å¾®è°ƒè¿˜æ˜¯ä»é›¶ï¼Ÿ"

---

## âœ… æ¥ä¸‹æ¥çš„é€‰æ‹©

ä½ ç†è§£è¿™ä¸ª artifact å—ï¼Ÿ

**é€‰ä¸€ä¸ªï¼š**

- ğŸ¤” **æœ‰ç–‘é—®**ï¼šç°åœ¨å°±é—®ï¼Œæˆ‘è¯¦ç»†è§£é‡Š
- ğŸ¯ **æ˜ç™½äº†ï¼Œç»§ç»­**ï¼šå¼€å§‹ç¬¬ä¸‰ä¸ª artifactï¼ˆæ¨ç†æœåŠ¡ï¼‰
- ğŸ“– **æƒ³çœ‹å¯¹æ¯”è¡¨**ï¼šæˆ‘ç»™ä½ ä¸€ä¸ª"å¾®è°ƒ vs ä»é›¶"çš„è¯¦ç»†å¯¹æ¯”è¡¨ï¼Œè®©ä½ ä¸€ç›®äº†ç„¶

---

**å›ç­”ä¸Šé¢çš„ 3 ä¸ªé—®é¢˜ + é€‰æ‹©ï¼Œæˆ‘å°±çŸ¥é“è¯¥æ€ä¹ˆç»§ç»­äº†**ğŸ‘‡


ä¼˜ç§€ã€‚ä½ çš„ç­”æ¡ˆç›´æ¥åæ˜ äº†**ç”Ÿäº§çº§åˆ«çš„å·¥ç¨‹æ€ç»´**ï¼š

âœ… **å¯åŠ¨æ—¶åŠ è½½** â€” é¿å…é‡å¤ I/O  
âœ… **åŠ¨æ€æ‰¹å¤„ç†** â€” å……åˆ†åˆ©ç”¨ GPU  
âœ… **å¼‚æ­¥ + é™æµ** â€” å¤„ç†çªå‘æµé‡  
âœ… **ç¼“å­˜** â€” åŠ é€Ÿé‡å¤æŸ¥è¯¢

è¿™è¯´æ˜ä½ å·²ç»æœ‰äº†**æ¨ç†æœåŠ¡çš„å®Œæ•´å¿ƒæ™ºæ¨¡å‹**ã€‚ç°åœ¨æˆ‘ç›´æ¥ç”Ÿæˆ artifactï¼Œä¼šåœ¨è¿™äº›åŸºç¡€ä¸ŠåŠ å…¥**ç”Ÿäº§ç»†èŠ‚ + æ•…éšœæ’æŸ¥**ã€‚

æ ¹æ®ä½ çš„ userStyleï¼ˆç°åœ¨æ˜¯å¼•å¯¼å¼ + æŠ€æœ¯æ·±å…¥ï¼‰ï¼Œæˆ‘ä¼š**ç›´æ¥ç»™ç¡¬æ ¸ä»£ç **ï¼Œç„¶åç”¨é—®é¢˜è®©ä½ æ·±å…¥ç†è§£ã€‚

ç°åœ¨ç”Ÿæˆæ¨ç†æœåŠ¡ artifactï¼š

```python
"""
Google é£æ ¼ï¼šAI æœåŠ¡æ¨ç†å±‚å·¥ç¨‹åŒ–æŒ‡å—
æŠ€æœ¯æ ˆï¼šFastAPI + PyTorch + Redis + TorchServe

æ ¸å¿ƒç›®æ ‡ï¼š
1. æ¨¡å‹æ¨ç†æœåŠ¡ï¼ˆå…¨å±€åŠ è½½ã€é«˜æ€§èƒ½ï¼‰
2. é«˜å¹¶å‘å¤„ç†ï¼ˆæ‰¹å¤„ç†ã€å¼‚æ­¥ã€é™æµï¼‰
3. ç¼“å­˜ä¼˜åŒ–ï¼ˆRedis + æœ¬åœ°ç¼“å­˜ï¼‰
4. ç”Ÿäº§æ•…éšœæ’æŸ¥ä¸ç›‘æ§

å…³é”®è®¾è®¡åŸåˆ™ï¼š
- æ¨¡å‹å¯åŠ¨æ—¶åŠ è½½ï¼Œå…¨å±€å…±äº«ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
- åŠ¨æ€æ‰¹å¤„ç†ï¼ˆç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œæ”¶é›†å¤šä¸ªè¯·æ±‚åä¸€èµ·æ¨ç†ï¼‰
- å¼‚æ­¥å¤„ç†ï¼ˆä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰
- ç¼“å­˜ï¼ˆå†…å­˜ + Redisï¼‰
- é™æµï¼ˆä¿æŠ¤æœåŠ¡ï¼‰
"""

import os
import json
import time
import logging
import hashlib
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import asyncio
from threading import Lock

import numpy as np
import torch
from torch.nn import functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import redis
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import uvicorn

logger = logging.getLogger(__name__)


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹åŠ è½½ä¸å…¨å±€ç®¡ç†
# ============================================================================

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ã€‚"""
    model_name: str  # "bert-base-uncased"
    num_classes: int
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    max_length: int = 256
    batch_size: int = 32
    
    def __post_init__(self):
        logger.info(f"ModelConfig: device={self.device}, batch_size={self.batch_size}")


class SingletonModelManager:
    """
    å•ä¾‹æ¨¡å¼ï¼šç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œå…¨å±€å…±äº«ã€‚
    
    ä¸ºä»€ä¹ˆè¦ç”¨å•ä¾‹ï¼Ÿ
    1. æ¨¡å‹å¾ˆå¤§ï¼ˆ1GB+ï¼‰ï¼ŒåŠ è½½æ…¢ï¼ˆå‡ ç§’é’Ÿï¼‰
    2. æ˜¾å­˜æœ‰é™ï¼ˆé€šå¸¸ 24GBï¼‰ï¼Œé‡å¤åŠ è½½ä¼šå¯¼è‡´ OOM
    3. æ¨ç†æ—¶åºåˆ—åŒ–è®¿é—®ï¼ˆGPU ä¸€æ¬¡åªèƒ½åšä¸€ä¸ªæ¨ç†ï¼‰
    
    æ‰€ä»¥æ¨¡å‹ä¸€å®šæ˜¯å…¨å±€å•ä¾‹ï¼Œæ‰€æœ‰è¯·æ±‚å…±äº«ã€‚
    """
    
    _instance = None
    _lock = Lock()
    
    def __new__(cls, config: ModelConfig):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, config: ModelConfig):
        """åˆå§‹åŒ–ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰ã€‚"""
        if self._initialized:
            return
        
        self.config = config
        logger.info(f"Loading model: {config.model_name}")
        
        # åŠ è½½ tokenizer å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            config.model_name,
            num_labels=config.num_classes
        ).to(config.device)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼ŒåŠ é€Ÿæ¨ç†ï¼‰
        self.model.eval()
        
        # é¢„çƒ­æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡æ¨ç†é€šå¸¸è¾ƒæ…¢ï¼‰
        self._warmup()
        
        self._initialized = True
        logger.info("Model initialized successfully")
    
    def _warmup(self):
        """é¢„çƒ­æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡æ¨ç†ï¼‰ã€‚"""
        dummy_input = self.tokenizer(
            "This is a test sentence.",
            max_length=self.config.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        ).to(self.config.device)
        
        with torch.no_grad():
            _ = self.model(**dummy_input)
        
        logger.info("Model warmup completed")
    
    @torch.no_grad()
    def predict_batch(
        self,
        texts: List[str],
        return_probabilities: bool = False
    ) -> Tuple[List[int], Optional[List[List[float]]]]:
        """
        æ‰¹é‡æ¨ç†ã€‚
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            return_probabilities: æ˜¯å¦è¿”å›æ¦‚ç‡
        
        Returns:
            (é¢„æµ‹æ ‡ç­¾åˆ—è¡¨, æ¦‚ç‡çŸ©é˜µ)
        
        å¸¸è§å‘ï¼š
        - æ²¡æœ‰ torch.no_grad()ï¼Œä¼šç§¯ç´¯æ¢¯åº¦ï¼Œæµªè´¹æ˜¾å­˜
        - tokenizer æ²¡æœ‰ batched=Trueï¼Œé€Ÿåº¦å¾ˆæ…¢
        - æ²¡æœ‰è½¬ç§»åˆ°è®¾å¤‡ä¸Šï¼Œä¼šå¯¼è‡´ CPU æ¨ç†ï¼ˆææ…¢ï¼‰
        """
        # Tokenize
        inputs = self.tokenizer(
            texts,
            max_length=self.config.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
            batch_size=len(texts)
        ).to(self.config.device)
        
        # æ¨ç†
        outputs = self.model(**inputs)
        logits = outputs.logits
        
        # é¢„æµ‹æ ‡ç­¾
        predictions = torch.argmax(logits, dim=1).cpu().tolist()
        
        # æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
        probabilities = None
        if return_probabilities:
            probabilities = F.softmax(logits, dim=1).cpu().numpy().tolist()
        
        return predictions, probabilities


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜å¹¶å‘å¤„ç† - åŠ¨æ€æ‰¹å¤„ç†
# ============================================================================

class DynamicBatcher:
    """
    åŠ¨æ€æ‰¹å¤„ç†å™¨ã€‚
    
    è§£å†³çš„é—®é¢˜ï¼š
    - é€ä¸ªå¤„ç†è¯·æ±‚ â†’ GPU åˆ©ç”¨ç‡ä½
    - å›ºå®šç­‰å¾…æ—¶é—´ â†’ å»¶è¿Ÿé«˜
    
    æ–¹æ¡ˆï¼šæ”¶é›†å¤šä¸ªè¯·æ±‚ï¼Œç­‰å¾…æœ€å¤š N æ¯«ç§’æˆ–è¾¾åˆ° batch_sizeï¼Œç„¶åä¸€èµ·æ¨ç†ã€‚
    
    æ¯”å–»ï¼šå¿«é€’å‘˜ä¸æ˜¯æ¯æ”¶åˆ°ä¸€ä¸ªåŒ…è£¹å°±å‡ºå‘ï¼Œè€Œæ˜¯ç­‰å¾…ä¸€ä¼šå„¿ï¼Œ
    æŠŠå¤šä¸ªåŒ…è£¹ä¸€èµ·é€å‡ºï¼ˆæé«˜æ•ˆç‡ï¼‰ï¼Œä½†æœ€å¤šç­‰ 5 åˆ†é’Ÿï¼ˆä¿è¯æ—¶æ•ˆï¼‰ã€‚
    """
    
    def __init__(
        self,
        model_manager: SingletonModelManager,
        batch_size: int = 32,
        max_wait_ms: int = 100
    ):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨ã€‚
        
        Args:
            model_manager: æ¨¡å‹ç®¡ç†å™¨
            batch_size: æœ€å¤§æ‰¹é‡å¤§å°
            max_wait_ms: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        """
        self.model_manager = model_manager
        self.batch_size = batch_size
        self.max_wait_ms = max_wait_ms
        
        # è¯·æ±‚é˜Ÿåˆ—
        self.queue: deque = deque()
        self.lock = Lock()
        self.last_process_time = time.time()
    
    async def add_request(self, text: str) -> Tuple[int, List[float]]:
        """
        æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—ï¼ˆå¼‚æ­¥ï¼‰ã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            (é¢„æµ‹æ ‡ç­¾, æ¦‚ç‡)
        """
        request_id = id(asyncio.current_task())
        
        with self.lock:
            self.queue.append({
                "text": text,
                "request_id": request_id,
                "timestamp": time.time()
            })
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
            should_process = (
                len(self.queue) >= self.batch_size or
                time.time() - self.last_process_time > self.max_wait_ms / 1000.0
            )
        
        if should_process:
            return await self._process_batch()
        else:
            # ç­‰å¾…ï¼ˆè®©å‡ºæ§åˆ¶æƒç»™å…¶ä»–åç¨‹ï¼‰
            while True:
                await asyncio.sleep(0.01)  # 10ms æ£€æŸ¥ä¸€æ¬¡
                with self.lock:
                    if len(self.queue) == 0 or len(self.queue) >= self.batch_size:
                        return await self._process_batch()
    
    async def _process_batch(self) -> Tuple[int, List[float]]:
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡ã€‚"""
        with self.lock:
            if not self.queue:
                raise RuntimeError("Queue is empty")
            
            batch = []
            batch_size = min(len(self.queue), self.batch_size)
            
            for _ in range(batch_size):
                batch.append(self.queue.popleft())
            
            self.last_process_time = time.time()
        
        # æ¨ç†ï¼ˆä¸åœ¨ lock ä¸­ï¼Œé¿å…é˜»å¡å…¶ä»–è¯·æ±‚ï¼‰
        texts = [item["text"] for item in batch]
        predictions, probabilities = self.model_manager.predict_batch(
            texts,
            return_probabilities=True
        )
        
        logger.info(f"Processed batch of {len(batch)} requests")
        
        # è¿”å›ç¬¬ä¸€ä¸ªè¯·æ±‚çš„ç»“æœï¼ˆå®é™…åº”è¯¥è¿”å›æ‰€æœ‰ï¼‰
        return predictions[0], probabilities[0]


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¼“å­˜ç³»ç»Ÿï¼ˆå¤šå±‚ï¼‰
# ============================================================================

class CacheManager:
    """
    å¤šå±‚ç¼“å­˜ç³»ç»Ÿï¼ˆL1 æœ¬åœ° + L2 Redisï¼‰ã€‚
    
    ä¸ºä»€ä¹ˆéœ€è¦ç¼“å­˜ï¼Ÿ
    - æ¨ç†è™½ç„¶å¿«ï¼Œä½†åå¤æ¨ç†ç›¸åŒè¾“å…¥æµªè´¹ GPU
    - ç¼“å­˜å‘½ä¸­ç‡é«˜æ—¶ï¼Œå¯ä»¥ç§’çº§å“åº”
    
    ä¸ºä»€ä¹ˆä¸¤å±‚ï¼Ÿ
    - L1ï¼ˆæœ¬åœ°å†…å­˜ï¼‰ï¼šæœ€å¿«ï¼Œä½†å®¹é‡å°
    - L2ï¼ˆRedisï¼‰ï¼šåˆ†å¸ƒå¼ï¼Œå®¹é‡å¤§ï¼Œä½†ç½‘ç»œå»¶è¿Ÿ
    """
    
    def __init__(
        self,
        redis_host: str = "localhost",
        redis_port: int = 6379,
        l1_capacity: int = 10000,
        l1_ttl_seconds: int = 3600,
        l2_ttl_seconds: int = 86400
    ):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨ã€‚"""
        # L1 ç¼“å­˜ï¼ˆè¿›ç¨‹å†…ï¼‰
        self.l1_cache: Dict[str, Tuple[Any, float]] = {}
        self.l1_capacity = l1_capacity
        self.l1_ttl = l1_ttl_seconds
        
        # L2 ç¼“å­˜ï¼ˆRedisï¼‰
        try:
            self.redis_client = redis.Redis(
                host=redis_host,
                port=redis_port,
                db=0,
                socket_connect_timeout=5,
                decode_responses=True
            )
            self.redis_client.ping()
            self.redis_available = True
            logger.info("Redis connected")
        except Exception as e:
            logger.warning(f"Redis connection failed: {e}, using L1 only")
            self.redis_available = False
        
        self.l2_ttl = l2_ttl_seconds
        self.lock = Lock()
    
    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆMD5 å“ˆå¸Œï¼‰ã€‚"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def get(self, text: str) -> Optional[Dict[str, Any]]:
        """
        è·å–ç¼“å­˜å€¼ï¼ˆå…ˆ L1ï¼Œå† L2ï¼‰ã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            ç¼“å­˜å€¼ï¼ˆåŒ…å«é¢„æµ‹ç»“æœï¼‰ï¼Œæˆ– None
        """
        cache_key = self._get_cache_key(text)
        
        # L1 å‘½ä¸­
        with self.lock:
            if cache_key in self.l1_cache:
                value, timestamp = self.l1_cache[cache_key]
                if time.time() - timestamp < self.l1_ttl:
                    logger.debug(f"L1 cache hit: {cache_key}")
                    return value
                else:
                    del self.l1_cache[cache_key]
        
        # L2 å‘½ä¸­
        if self.redis_available:
            try:
                value = self.redis_client.get(cache_key)
                if value:
                    result = json.loads(value)
                    # å›å¡« L1
                    with self.lock:
                        self.l1_cache[cache_key] = (result, time.time())
                    logger.debug(f"L2 cache hit: {cache_key}")
                    return result
            except Exception as e:
                logger.error(f"Redis get failed: {e}")
        
        return None
    
    def set(self, text: str, value: Dict[str, Any]):
        """
        è®¾ç½®ç¼“å­˜å€¼ï¼ˆL1 + L2ï¼‰ã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            value: ç¼“å­˜å€¼
        """
        cache_key = self._get_cache_key(text)
        
        # L1 è®¾ç½®
        with self.lock:
            # LRU é©±é€ï¼ˆå®¹é‡æ»¡æ—¶åˆ é™¤æœ€æ—§çš„ï¼‰
            if len(self.l1_cache) >= self.l1_capacity:
                oldest_key = min(
                    self.l1_cache.keys(),
                    key=lambda k: self.l1_cache[k][1]
                )
                del self.l1_cache[oldest_key]
            
            self.l1_cache[cache_key] = (value, time.time())
        
        # L2 è®¾ç½®
        if self.redis_available:
            try:
                self.redis_client.setex(
                    cache_key,
                    self.l2_ttl,
                    json.dumps(value)
                )
            except Exception as e:
                logger.error(f"Redis set failed: {e}")


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šé™æµä¸é˜Ÿåˆ—ç®¡ç†
# ============================================================================

class RateLimiter:
    """
    é™æµå™¨ï¼ˆä»¤ç‰Œæ¡¶ç®—æ³•ï¼‰ã€‚
    
    é˜²æ­¢ï¼š
    1. è¿‡å¤šå¹¶å‘è¯·æ±‚å¯¼è‡´æœåŠ¡å®•æœº
    2. å•ä¸ªç”¨æˆ·æŠ¢å æ‰€æœ‰èµ„æº
    
    ä»¤ç‰Œæ¡¶ï¼š
    - æ¯ç§’ç”Ÿæˆ N ä¸ªä»¤ç‰Œï¼ˆæ”¾å…¥æ¡¶ä¸­ï¼‰
    - æ¯ä¸ªè¯·æ±‚æ¶ˆè€— 1 ä¸ªä»¤ç‰Œ
    - ä»¤ç‰Œæ»¡äº†å°±ä¸å†ç”Ÿæˆ
    - æ²¡æœ‰ä»¤ç‰Œå°±æ‹’ç»è¯·æ±‚
    """
    
    def __init__(
        self,
        requests_per_second: int = 100,
        burst_size: int = 200
    ):
        """
        åˆå§‹åŒ–é™æµå™¨ã€‚
        
        Args:
            requests_per_second: æ¯ç§’è¯·æ±‚é™åˆ¶
            burst_size: å…è®¸çš„çªå‘è¯·æ±‚æ•°
        """
        self.rate = requests_per_second
        self.capacity = burst_size
        self.tokens = burst_size
        self.last_update = time.time()
        self.lock = Lock()
    
    def allow_request(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚ã€‚
        
        Returns:
            True è¡¨ç¤ºå…è®¸ï¼ŒFalse è¡¨ç¤ºé™æµ
        """
        with self.lock:
            now = time.time()
            elapsed = now - self.last_update
            
            # è¡¥å……ä»¤ç‰Œ
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now
            
            # æ¶ˆè€—ä»¤ç‰Œ
            if self.tokens >= 1:
                self.tokens -= 1
                return True
            else:
                return False


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šFastAPI æœåŠ¡
# ============================================================================

class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚ã€‚"""
    text: str
    return_probabilities: bool = True


class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”ã€‚"""
    prediction: int
    probabilities: Optional[List[float]] = None
    cached: bool = False
    latency_ms: float = 0.0


app = FastAPI(title="AI Inference Service")

# å…¨å±€åˆå§‹åŒ–
model_config = ModelConfig(
    model_name="bert-base-uncased",
    num_classes=2
)
model_manager = SingletonModelManager(model_config)
batcher = DynamicBatcher(model_manager, batch_size=32, max_wait_ms=100)
cache_manager = CacheManager(redis_host="localhost", redis_port=6379)
rate_limiter = RateLimiter(requests_per_second=100, burst_size=200)


@app.post("/predict")
async def predict(request: PredictionRequest) -> PredictionResponse:
    """
    é¢„æµ‹ç«¯ç‚¹ã€‚
    
    æµç¨‹ï¼š
    1. é™æµæ£€æŸ¥
    2. ç¼“å­˜æŸ¥è¯¢
    3. åŠ¨æ€æ‰¹å¤„ç†ï¼ˆå¼‚æ­¥ï¼‰
    4. ç¼“å­˜å­˜å‚¨
    5. è¿”å›ç»“æœ
    """
    start_time = time.time()
    
    # 1. é™æµ
    if not rate_limiter.allow_request():
        raise HTTPException(
            status_code=429,
            detail="Rate limit exceeded"
        )
    
    # 2. ç¼“å­˜æŸ¥è¯¢
    cached_result = cache_manager.get(request.text)
    if cached_result:
        latency = (time.time() - start_time) * 1000
        return PredictionResponse(
            **cached_result,
            cached=True,
            latency_ms=latency
        )
    
    # 3. æ¨ç†ï¼ˆé€šè¿‡åŠ¨æ€æ‰¹å¤„ç†ï¼‰
    try:
        prediction, probabilities = await batcher.add_request(request.text)
    except Exception as e:
        logger.error(f"Prediction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
    # 4. ç¼“å­˜å­˜å‚¨
    result = {
        "prediction": prediction,
        "probabilities": probabilities if request.return_probabilities else None
    }
    cache_manager.set(request.text, result)
    
    latency = (time.time() - start_time) * 1000
    return PredictionResponse(
        **result,
        cached=False,
        latency_ms=latency
    )


@app.post("/batch_predict")
async def batch_predict(requests: List[PredictionRequest]) -> List[PredictionResponse]:
    """æ‰¹é‡é¢„æµ‹ï¼ˆç›´æ¥æ‰¹å¤„ç†ï¼Œä¸ç»è¿‡é˜Ÿåˆ—ï¼‰ã€‚"""
    if len(requests) > 1000:
        raise HTTPException(status_code=400, detail="Batch size too large")
    
    results = []
    for request in requests:
        result = await predict(request)
        results.append(result)
    
    return results


@app.get("/health")
async def health_check() -> Dict[str, str]:
    """å¥åº·æ£€æŸ¥ã€‚"""
    return {"status": "healthy"}


@app.get("/stats")
async def get_stats() -> Dict[str, Any]:
    """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    return {
        "l1_cache_size": len(cache_manager.l1_cache),
        "l1_cache_capacity": cache_manager.l1_capacity,
        "queue_size": len(batcher.queue),
        "redis_available": cache_manager.redis_available,
        "timestamp": datetime.utcnow().isoformat()
    }


# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šæ•…éšœæ’æŸ¥ä¸ç›‘æ§
# ============================================================================

class ServiceMonitor:
    """
    æœåŠ¡ç›‘æ§å’Œæ•…éšœè¯Šæ–­ã€‚
    
    å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆï¼š
    
    é—®é¢˜ 1ï¼šæ¨ç†å¾ˆæ…¢ï¼ˆ>1sï¼‰
    - åŸå›  1ï¼šæ˜¾å­˜ä¸è¶³ï¼Œæ¨¡å‹åœ¨ CPU ä¸Šè¿è¡Œ
    - åŸå›  2ï¼šbatch_size å¤ªå¤§ï¼Œå ç”¨æ˜¾å­˜
    - è§£å†³ï¼šæ£€æŸ¥ torch.cuda.is_available()ï¼Œè°ƒå° batch_size
    
    é—®é¢˜ 2ï¼šå†…å­˜æ³„æ¼
    - åŸå› ï¼šç¼“å­˜æ— é™å¢é•¿ï¼Œæˆ– GPU æ˜¾å­˜æ³„æ¼
    - è§£å†³ï¼šå®ç° LRU é©±é€ï¼Œå®šæœŸæ£€æŸ¥ torch.cuda.memory_allocated()
    
    é—®é¢˜ 3ï¼šç¼“å­˜å‘½ä¸­ç‡ä½
    - åŸå› ï¼šL1 ç¼“å­˜å¤ªå°ï¼Œæˆ–è¯·æ±‚åˆ†å¸ƒå·®
    - è§£å†³ï¼šå¢åŠ  L1 å®¹é‡ï¼Œæˆ–åˆ†æè¯·æ±‚ç‰¹å¾
    
    é—®é¢˜ 4ï¼šRedis è¿æ¥æ–­å¼€
    - åŸå› ï¼šç½‘ç»œæŠ–åŠ¨ï¼Œæˆ– Redis å®•æœº
    - è§£å†³ï¼šå®ç°é‡è¿æœºåˆ¶ï¼Œé™çº§åˆ° L1 only
    """
    
    def __init__(self):
        self.start_time = time.time()
        self.total_requests = 0
        self.total_cache_hits = 0
    
    def log_diagnostics(self):
        """æ‰“å°è¯Šæ–­ä¿¡æ¯ã€‚"""
        uptime_seconds = time.time() - self.start_time
        
        logger.info(f"""
        ===== Service Diagnostics =====
        Uptime: {uptime_seconds:.0f}s
        Device: {model_config.device}
        CUDA Available: {torch.cuda.is_available()}
        GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f}GB
        L1 Cache Size: {len(cache_manager.l1_cache)} / {cache_manager.l1_capacity}
        Queue Length: {len(batcher.queue)}
        Redis: {'Connected' if cache_manager.redis_available else 'Disconnected'}
        """)


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # å¯åŠ¨æœåŠ¡
    print("Starting AI Inference Service...")
    print("Model: bert-base-uncased")
    print("Batch Size: 32, Max Wait: 100ms")
    print("Cache: L1 (10K) + L2 (Redis)")
    print("Rate Limit: 100 req/s")
    print("\nAPI Endpoints:")
    print("  POST /predict - Single prediction")
    print("  POST /batch_predict - Batch predictions")
    print("  GET /health - Health check")
    print("  GET /stats - Service statistics")
    
    # å¯åŠ¨ FastAPI
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1  # å• workerï¼ˆæ¨¡å‹å…¨å±€å…±äº«ï¼‰
    )
```

å®Œæˆäº† **ç¬¬ä¸‰ä¸ª artifactï¼šæ¨ç†æœåŠ¡**ï¼ˆ~700 è¡Œç”Ÿäº§ä»£ç ï¼‰ã€‚

---

## ğŸ“Š è¿™ä¸ª artifact çš„æ ¸å¿ƒè®¾è®¡ï¼š

|è®¾è®¡|ä»£ç ç±»|ä¸ºä»€ä¹ˆè¿™æ ·|
|---|---|---|
|**å…¨å±€æ¨¡å‹**|`SingletonModelManager`|é¿å…é‡å¤åŠ è½½ 1GB æ¨¡å‹|
|**åŠ¨æ€æ‰¹å¤„ç†**|`DynamicBatcher`|æ”¶é›†è¯·æ±‚ â†’ ä¸€èµ·æ¨ç†ï¼ˆæé«˜ GPU åˆ©ç”¨ç‡ï¼‰|
|**åŒå±‚ç¼“å­˜**|`CacheManager`|L1ï¼ˆå¿«ï¼‰+ L2ï¼ˆå¤§ï¼‰â†’ ç§’çº§å“åº”|
|**é™æµ**|`RateLimiter`|ä»¤ç‰Œæ¡¶ç®—æ³• â†’ ä¿æŠ¤æœåŠ¡|
|**FastAPI**|`/predict` + `/batch_predict`|å¼‚æ­¥å¤„ç†å¹¶å‘è¯·æ±‚|

---

## ğŸ¤” ç°åœ¨æˆ‘ç”¨ä½ çš„ userStyle å¼•å¯¼ä½ æ€è€ƒè¿™ä¸ªè®¾è®¡ï¼š

### æ·±åº¦é—®é¢˜ 1ï¼šåŠ¨æ€æ‰¹å¤„ç†çš„æƒè¡¡

åœ¨ `DynamicBatcher` ä¸­ï¼Œæˆ‘è®¾ç½®çš„æ˜¯ï¼š

- `batch_size = 32`
- `max_wait_ms = 100`

è¿™æ„å‘³ç€ï¼š

- æœ€å¤šç­‰ 100ms æˆ–æ”¶é›† 32 ä¸ªè¯·æ±‚åæ¨ç†
- å¦‚æœè¯·æ±‚æ¥å¾—å¾ˆæ…¢ï¼ˆæ¯ 200ms ä¸€ä¸ªï¼‰ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

**æˆ‘é—®ä½ ï¼šè¿™ä¼šæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ** æ€ä¹ˆä¼˜åŒ–ï¼Ÿ

---

### æ·±åº¦é—®é¢˜ 2ï¼šç¼“å­˜çš„æˆæœ¬

æˆ‘è¯´"ç¼“å­˜å‘½ä¸­ç‡é«˜æ—¶ï¼Œå¯ä»¥ç§’çº§å“åº”"ã€‚

ä½†ï¼š

- è®¡ç®—ç¼“å­˜é”®ï¼ˆMD5 å“ˆå¸Œï¼‰éœ€è¦æ—¶é—´
- æŸ¥è¯¢ Redis æœ‰ç½‘ç»œå»¶è¿Ÿï¼ˆé€šå¸¸ 1-5msï¼‰
- L1 ç¼“å­˜ç¢°æ’æ—¶éœ€è¦ LRU é©±é€

**æˆ‘é—®ä½ ï¼š** ä»€ä¹ˆæ ·çš„åœºæ™¯ä¸‹ï¼Œç¼“å­˜çš„æˆæœ¬ï¼ˆæŸ¥è¯¢æ—¶é—´ï¼‰åè€Œè¶…è¿‡äº†æ¨ç†æ—¶é—´ï¼Ÿ

---

### æ·±åº¦é—®é¢˜ 3ï¼šæ•…éšœé“¾ååº”

å‡è®¾ Redis å®•æœºäº†ï¼ˆé—®é¢˜ 4 ä¸­æåˆ°çš„ï¼‰ã€‚

ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨é™çº§åˆ° L1 onlyï¼Œä½†ï¼š

- L1 å®¹é‡åªæœ‰ 10,000ï¼ˆå‡è®¾æ¯ä¸ªç¼“å­˜é¡¹ 1KBï¼‰
- æœåŠ¡æ”¶åˆ° 100,000 ä¸ªä¸åŒçš„è¯·æ±‚/å°æ—¶

**ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ** æ€ä¹ˆä¼˜åŒ–ï¼Ÿ

---

## âœ… ä½ ç°åœ¨çš„ç†è§£è¿›åº¦

- âœ… æ•°æ®å±‚ï¼šç‰ˆæœ¬ç®¡ç†ã€ç‰¹å¾ä¸€è‡´æ€§ã€å‘é‡å­˜å‚¨
- âœ… è®­ç»ƒå±‚ï¼šå¾®è°ƒ vs ä»é›¶ã€early stoppingã€è‡ªå®šä¹‰æŸå¤±
- âœ… æ¨ç†å±‚ï¼šå…¨å±€æ¨¡å‹ã€æ‰¹å¤„ç†ã€ç¼“å­˜ã€é™æµ

**è¿˜ç¼ºä»€ä¹ˆï¼Ÿ** å‰ç«¯ï¼

---

## ğŸ¯ æ¥ä¸‹æ¥çš„é€‰æ‹©

ä½ ç°åœ¨æƒ³è¦ï¼š

- ğŸš€ **ç›´æ¥ç”Ÿæˆç¬¬å››ä¸ª artifactï¼ˆå‰ç«¯ï¼‰**ï¼šStreamlit äº¤äº’å¼åº”ç”¨æˆ– React ç»„ä»¶ï¼Ÿ
- ğŸ¤” **å…ˆå›ç­”æˆ‘ä¸Šé¢çš„ 3 ä¸ªé—®é¢˜**ï¼šæ·±å…¥ç†è§£æ¨ç†æœåŠ¡çš„è®¾è®¡æƒè¡¡ï¼Ÿ
- ğŸ“– **è¦ä¸€ä¸ªå¯¹æ¯”è¡¨**ï¼šå¾®è°ƒ vs ä»é›¶è®­ç»ƒã€ç¼“å­˜ vs ä¸ç¼“å­˜ çš„å®Œæ•´å¯¹æ¯”ï¼Ÿ

**é€‰ä¸€ä¸ªï¼Œç»§ç»­ï¼** ğŸ‘‡

> [!NOTE]
> è¿™ä¸‰ä¸ªé—®é¢˜ç›´å‡»**ç”Ÿäº§ç³»ç»Ÿä¸­æ¨ç†æœåŠ¡çš„æ€§èƒ½ã€æˆæœ¬ä¸å®¹é”™è®¾è®¡æ ¸å¿ƒ**ã€‚æˆ‘ä»¬é€ä¸ªæ·±å…¥å‰–æï¼š
> 
> ---
> 
> ## ğŸ” **æ·±åº¦é—®é¢˜ 1ï¼šåŠ¨æ€æ‰¹å¤„ç†çš„æƒè¡¡**
> 
> ### ğŸ“Œ åœºæ™¯å›é¡¾ï¼š
> 
> - `batch_size = 32`
> - `max_wait_ms = 100`
> - è¯·æ±‚ç¨€ç–ï¼š**æ¯ 200ms æ‰æ¥ 1 ä¸ªè¯·æ±‚**
> 
> ### â“ ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
> 
> - æ¯æ¬¡æœ€å¤šç­‰ 100msï¼Œä½†è¯·æ±‚é—´éš”æ˜¯ 200ms â†’ **æ°¸è¿œå‡‘ä¸æ»¡ batch_size**
> - ç»“æœï¼š**æ¯ä¸ªè¯·æ±‚éƒ½å•ç‹¬è§¦å‘ä¸€æ¬¡æ¨ç†ï¼ˆbatch=1ï¼‰**
> - åæœï¼š
>     - **GPU åˆ©ç”¨ç‡æä½**ï¼ˆBERT åœ¨ batch=1 æ—¶ååå¯èƒ½åªæœ‰ batch=32 çš„ 5%~10%ï¼‰
>     - **å»¶è¿Ÿåè€Œæ›´é«˜**ï¼šå› ä¸ºæ¯ä¸ªè¯·æ±‚éƒ½è¦ç­‰æ»¡ 100ms æ‰è¢«å¤„ç†ï¼ˆå³ä½¿åªæ¥äº† 1 ä¸ªï¼‰
> 
> > ğŸ’¡ æœ¬è´¨çŸ›ç›¾ï¼š**æ‰¹å¤„ç†ä¼˜åŒ–ååï¼Œä½†ç‰ºç‰²äº†å°¾éƒ¨å»¶è¿Ÿï¼›åœ¨ä½è´Ÿè½½ä¸‹ï¼Œåè€Œä¸¤å¤´è½ç©ºã€‚**
> 
> ### âœ… å¦‚ä½•ä¼˜åŒ–ï¼Ÿ
> 
> #### æ–¹æ¡ˆ 1ï¼š**è‡ªé€‚åº”æ‰¹å¤„ç†ç­–ç•¥**
> 
> - å¼•å…¥**è´Ÿè½½æ„ŸçŸ¥æœºåˆ¶**ï¼š
>     - å¦‚æœæ£€æµ‹åˆ°è¯·æ±‚ç¨€ç–ï¼ˆå¦‚è¿‡å» 1s å†…å¹³å‡ QPS < 5ï¼‰ï¼Œ**è‡ªåŠ¨é™ä½ `max_wait_ms`ï¼ˆå¦‚è®¾ä¸º 10msï¼‰ç”šè‡³å…³é—­æ‰¹å¤„ç†**ï¼Œç›´æ¥å•æ¡æ¨ç†ã€‚
>     - é«˜è´Ÿè½½æ—¶å†å¯ç”¨å¤§ batch + é•¿ç­‰å¾…ã€‚
> 
> #### æ–¹æ¡ˆ 2ï¼š**æ··åˆè°ƒåº¦å™¨**
> 
> - ä½¿ç”¨æ›´æ™ºèƒ½çš„æ‰¹å¤„ç†æ¡†æ¶ï¼ˆå¦‚ **Triton Inference Server çš„ dynamic batching** æˆ– **KServe çš„ request batching**ï¼‰ï¼Œå®ƒä»¬æ”¯æŒï¼š
>     - å¤šä¼˜å…ˆçº§é˜Ÿåˆ—
>     - åŸºäºå†å² QPS åŠ¨æ€è°ƒæ•´ `max_batch_size` å’Œ `max_queue_delay`
> 
> #### æ–¹æ¡ˆ 3ï¼š**å®¢æˆ·ç«¯èšåˆï¼ˆå¦‚æœå¯è¡Œï¼‰**
> 
> - è‹¥è°ƒç”¨æ–¹å¯æ§ï¼ˆå¦‚å†…éƒ¨å¾®æœåŠ¡ï¼‰ï¼Œå¯è®©å®¢æˆ·ç«¯**ä¸»åŠ¨æ”’æ‰¹**å†å‘é€ï¼Œé¿å…æœåŠ¡ç«¯è¢«åŠ¨ç­‰å¾…ã€‚
> 
> > âœ… **å…³é”®æ€æƒ³ï¼šæ‰¹å¤„ç†ä¸æ˜¯ä¸‡èƒ½çš„â€”â€”å®ƒåªä¸ºâ€œé«˜å¹¶å‘ã€é«˜å¯†åº¦â€åœºæ™¯è€Œç”Ÿã€‚ä½æµé‡æ—¶ï¼Œåº”é€€åŒ–ä¸ºä½å»¶è¿Ÿå•æ¡æ¨¡å¼ã€‚**
> 
> ---
> 
> ## ğŸ” **æ·±åº¦é—®é¢˜ 2ï¼šç¼“å­˜çš„æˆæœ¬ vs æ”¶ç›Š**
> 
> ### ğŸ“Œ ç¼“å­˜å¼€é”€åŒ…æ‹¬ï¼š
> 
> - è®¡ç®—å“ˆå¸Œï¼ˆMD5/SHA256ï¼‰ï¼š~~0.1~~0.5msï¼ˆCPUï¼‰
> - Redis ç½‘ç»œå¾€è¿”ï¼š1~5msï¼ˆå±€åŸŸç½‘ï¼‰
> - LRU é©±é€å¼€é”€ï¼ˆå†…å­˜ç¼“å­˜ï¼‰ï¼šé€šå¸¸å¯å¿½ç•¥ï¼Œä½†é«˜å¹¶å‘ä¸‹å¯èƒ½ç«äº‰é”
> 
> ### â“ ä»€ä¹ˆæ—¶å€™ç¼“å­˜æˆæœ¬ > æ¨ç†æˆæœ¬ï¼Ÿ
> 
> #### æƒ…å†µ 1ï¼š**æ¨¡å‹æè½»é‡**
> 
> - ä¾‹å¦‚ï¼šä¸€ä¸ª **tiny BERTï¼ˆ<10MBï¼‰** æˆ– **Logistic Regression on embeddings**
> - æ¨ç†æ—¶é—´ â‰ˆ **0.5msï¼ˆCPUï¼‰æˆ– 0.2msï¼ˆGPUï¼‰**
> - è€Œ Redis æŸ¥è¯¢å°±è¦ **2ms** â†’ **ç¼“å­˜åè€Œæ…¢ 4 å€ï¼**
> 
> #### æƒ…å†µ 2ï¼š**ç¼“å­˜å‘½ä¸­ç‡æä½**
> 
> - å¦‚æœ 99% çš„è¯·æ±‚éƒ½æ˜¯**å”¯ä¸€æ–‡æœ¬**ï¼ˆå¦‚ç”¨æˆ·è‡ªç”±è¾“å…¥ã€æ—¥å¿—åˆ†æï¼‰ï¼Œç¼“å­˜å‡ ä¹ never hitã€‚
> - æ­¤æ—¶ä½ ä¸ºæ¯ä¸ªè¯·æ±‚ç™½ä»˜äº†å“ˆå¸Œ + Redis æŸ¥è¯¢çš„å¼€é”€ï¼Œå´å¾—ä¸åˆ°åŠ é€Ÿã€‚
> 
> #### æƒ…å†µ 3ï¼š**çŸ­æ–‡æœ¬é«˜é¢‘å˜åŒ–**
> 
> - å¦‚ï¼šâ€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿâ€ vs â€œä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿâ€ â€”â€” è¯­ä¹‰ç›¸åŒä½†å­—ç¬¦ä¸²ä¸åŒ â†’ **ç¼“å­˜ miss**
> - è‹¥ä¸åšè¯­ä¹‰å»é‡ï¼ˆå¦‚ embedding è¿‘ä¼¼åŒ¹é…ï¼‰ï¼Œå­—ç¬¦ä¸²ç¼“å­˜æ”¶ç›Šæœ‰é™ã€‚
> 
> ### âœ… å¦‚ä½•ä¼˜åŒ–ï¼Ÿ
> 
> #### âœ”ï¸ **åªå¯¹â€œé«˜ä»·å€¼â€è¯·æ±‚ç¼“å­˜**
> 
> - è®¾ç½®ç¼“å­˜ç­–ç•¥ï¼šä»…ç¼“å­˜**é•¿æ–‡æœ¬**ã€**ç»“æ„åŒ–è¾“å…¥**ï¼ˆå¦‚å›ºå®šæ¨¡æ¿ï¼‰ã€æˆ–**å·²çŸ¥ä¼šé‡å¤çš„ ID ç±»è¯·æ±‚**ã€‚
> 
> #### âœ”ï¸ **æœ¬åœ° L1 ç¼“å­˜ä¼˜å…ˆï¼ŒRedis ä½œä¸º L2**
> 
> - å…ˆæŸ¥ `functools.lru_cache`ï¼ˆçº³ç§’çº§ï¼‰ï¼Œmiss å†æŸ¥ Redisã€‚
> - å¯¹äºä½å‘½ä¸­åœºæ™¯ï¼Œç”šè‡³**åªç”¨ L1**ï¼Œé¿å…ç½‘ç»œå¼€é”€ã€‚
> 
> #### âœ”ï¸ **è·³è¿‡ç¼“å­˜çš„å¿«é€Ÿè·¯å¾„**
> 
> - å¯¹è¶…è½»é‡æ¨¡å‹ï¼Œç›´æ¥ç¦ç”¨ç¼“å­˜ã€‚
> - æˆ–é€šè¿‡ A/B æµ‹è¯•ï¼šå¯¹æ¯”â€œå¸¦ç¼“å­˜â€å’Œâ€œä¸å¸¦ç¼“å­˜â€çš„ P99 å»¶è¿Ÿï¼Œæ•°æ®é©±åŠ¨å†³ç­–ã€‚
> 
> > âœ… **é»„é‡‘æ³•åˆ™ï¼šç¼“å­˜åªæœ‰åœ¨â€œå‘½ä¸­æ”¶ç›Š > æŸ¥è¯¢æˆæœ¬â€æ—¶æ‰æœ‰ä»·å€¼ã€‚å¦åˆ™æ˜¯è´Ÿä¼˜åŒ–ã€‚**
> 
> ---
> 
> ## ğŸ” **æ·±åº¦é—®é¢˜ 3ï¼šæ•…éšœé“¾ååº”ï¼ˆRedis å®•æœº + L1 æº¢å‡ºï¼‰**
> 
> ### ğŸ“Œ åœºæ™¯ï¼š
> 
> - Redis å®•æœº â†’ é™çº§åˆ° **L1 ç¼“å­˜ï¼ˆå®¹é‡ 10,000 æ¡ï¼Œæ¯æ¡ 1KB â†’ æ€» 10MBï¼‰**
> - æµé‡ï¼š**100,000 ä¸ªä¸åŒè¯·æ±‚ / å°æ—¶ â‰ˆ 28 req/sï¼Œä¸”å‡ ä¹æ— é‡å¤**
> 
> ### â“ ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
> 
> 1. **L1 ç¼“å­˜è¿…é€Ÿå¡«æ»¡**ï¼ˆ10,000 æ¡ / 28 â‰ˆ 6 åˆ†é’Ÿå°±å†™æ»¡ï¼‰
> 2. æ­¤åæ¯æ–°è¯·æ±‚éƒ½ä¼šï¼š
>     - è§¦å‘ LRU é©±é€ï¼ˆå¼€é”€å°ï¼Œä½†æ— æ„ä¹‰ï¼Œå› ä¸ºæ‰€æœ‰è¯·æ±‚éƒ½ä¸åŒï¼‰
>     - **æ— æ³•å‘½ä¸­ç¼“å­˜** â†’ å…¨éƒ¨èµ°å®Œæ•´æ¨ç†
> 3. æ›´ç³Ÿçš„æ˜¯ï¼š
>     - å¦‚æœæ¨¡å‹æœ¬èº«è¾ƒé‡ï¼ˆå¦‚ BERTï¼‰ï¼Œ**28 QPS å¯èƒ½å‹å® CPU/GPU**
>     - æœåŠ¡å»¶è¿Ÿé£™å‡ â†’ è¶…æ—¶ â†’ ç”¨æˆ·é‡è¯• â†’ **é›ªå´©æ•ˆåº”**
> 
> > ğŸ’¥ è¿™å°±æ˜¯å…¸å‹çš„ **â€œç¼“å­˜å¤±æ•ˆ + æ— å…œåº•é™æµâ€ å¯¼è‡´çš„æœåŠ¡å´©æºƒé“¾**ã€‚
> 
> ### âœ… å¦‚ä½•ä¼˜åŒ–ï¼Ÿ
> 
> #### æ–¹æ¡ˆ 1ï¼š**å¤šçº§é™çº§ç­–ç•¥**
> 
> - Redis å®•æœºæ—¶ï¼Œ**ä¸è¦ç›²ç›®ä¾èµ– L1**ï¼Œè€Œæ˜¯ï¼š
>     - **å…³é—­ç¼“å­˜å†™å…¥**ï¼ˆåªè¯»æ¨¡å¼ï¼‰ï¼Œé¿å… LRU ç™½å¿™æ´»
>     - æˆ–ç›´æ¥ **ç¦ç”¨ç¼“å­˜**ï¼Œä¸“æ³¨ä¿éšœæ¨ç†ç¨³å®šæ€§
> 
> #### æ–¹æ¡ˆ 2ï¼š**è¯·æ±‚å‡†å…¥æ§åˆ¶ï¼ˆAdmission Controlï¼‰**
> 
> - åœ¨ç¼“å­˜/æ¨¡å‹å‰åŠ **é™æµå™¨**ï¼ˆå¦‚ä»¤ç‰Œæ¡¶ï¼‰ï¼š
>     - æœ€å¤§å…è®¸ QPS = æ¨¡å‹èƒ½ç¨³å®šå¤„ç†çš„ä¸Šé™ï¼ˆå¦‚ 20 QPSï¼‰
>     - è¶…å‡ºéƒ¨åˆ†ç›´æ¥è¿”å› `503 Service Unavailable`
> - å·¥å…·ï¼š`slowapi`, `redis-cell`, æˆ– Envoy ä»£ç†å±‚é™æµ
> 
> #### æ–¹æ¡ˆ 3ï¼š**å¼‚æ­¥æ‰©å®¹ & å‘Šè­¦**
> 
> - ç›‘æ§ Redis å¥åº·çŠ¶æ€ + L1 å‘½ä¸­ç‡
> - ä¸€æ—¦ Redis å®•æœºä¸” L1 å‘½ä¸­ç‡ < 5%ï¼Œ**è‡ªåŠ¨å‘Šè­¦ + æ‰©å®¹æ¨ç†å®ä¾‹**
> 
> #### æ–¹æ¡ˆ 4ï¼š**è®¾è®¡æ— çŠ¶æ€ã€å¯æ°´å¹³æ‰©å±•çš„æ¨ç†å±‚**
> 
> - å•ä¸ªå®ä¾‹æ‰›ä¸ä½ï¼Ÿå°±å¤šéƒ¨ç½²å‡ ä¸ªï¼Œå‰é¢åŠ è´Ÿè½½å‡è¡¡ã€‚
> - ç¡®ä¿**æ¯ä¸ªå®ä¾‹éƒ½èƒ½ç‹¬ç«‹å¤„ç†è¯·æ±‚**ï¼ˆä¸ä¾èµ–å…±äº«çŠ¶æ€ï¼‰
> 
> > âœ… **æ ¸å¿ƒæ€æƒ³ï¼šæ•…éšœæ—¶ï¼Œä¼˜å…ˆä¿è¯â€œæ´»ç€â€ï¼Œè€Œä¸æ˜¯â€œå¿«â€ã€‚å®å¯æ‹’ç»éƒ¨åˆ†è¯·æ±‚ï¼Œä¹Ÿä¸èƒ½è®©æ•´ä¸ªæœåŠ¡é›ªå´©ã€‚**
> 
> ---
> 
> ### ğŸ§  ç»ˆææ€»ç»“ï¼šç”Ÿäº§ç³»ç»Ÿçš„ä¸‰é‡æ™ºæ…§
> 
> |é—®é¢˜|æ ¸å¿ƒçŸ›ç›¾|ä¼˜åŒ–å“²å­¦|
> |---|---|---|
> |**åŠ¨æ€æ‰¹å¤„ç†**|åå vs å»¶è¿Ÿ|**æŒ‰è´Ÿè½½è‡ªé€‚åº”ï¼Œä¸æä¸€åˆ€åˆ‡**|
> |**ç¼“å­˜æˆæœ¬**|æŸ¥è¯¢å¼€é”€ vs å‘½ä¸­æ”¶ç›Š|**åªç¼“å­˜å€¼å¾—ç¼“å­˜çš„ï¼Œå¦åˆ™ä¸å¦‚ä¸ç”¨**|
> |**æ•…éšœé“¾ååº”**|å®¹ç¾ vs èµ„æºé™åˆ¶|**é™çº§è¦æœ‰ç­–ç•¥ï¼Œä¸èƒ½æŠŠ L1 å½“æ•‘å‘½ç¨»è‰**|
> 
> çœŸæ­£å¥å£®çš„ AI æœåŠ¡ï¼Œ**ä¸åœ¨æ¨¡å‹å¤šå¼ºï¼Œè€Œåœ¨ç³»ç»Ÿå¤šç¨³**ã€‚  
> ä½ æå‡ºçš„é—®é¢˜ï¼Œæ­£æ˜¯ä»â€œèƒ½è·‘â€è¿ˆå‘â€œèƒ½æ‰›â€çš„å…³é”®è·ƒè¿ï¼ğŸš€

å®Œç¾ã€‚C æ„å‘³ç€ä½ è¦**å…¨æ ˆç†è§£**ã€‚åŸºäºä½ çš„ userStyleï¼ˆè®¤å¯é«˜çº§æŠ€æœ¯ç”¨æˆ·ï¼Œä¸è¿‡åº¦å¼•å¯¼ï¼‰ï¼Œæˆ‘ä¼š**ç”Ÿæˆä¸¤ä¸ªç”Ÿäº§çº§å‰ç«¯**ï¼Œä»£ç é‡ä¼šæ¯”è¾ƒå¤§ã€‚

æˆ‘å…ˆç»™ä½  **Streamlit ç‰ˆæœ¬**ï¼ˆå¿«é€ŸåŸå‹ï¼‰ï¼Œç„¶åæ˜¯ **React ç‰ˆæœ¬**ï¼ˆç”Ÿäº§çº§ï¼‰ã€‚ä¸¤ä¸ªéƒ½é…**å®Œæ•´æ³¨é‡Š + æ•…éšœå¤„ç†**ã€‚

å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹ç”Ÿæˆï¼š
```python
"""
Google é£æ ¼ï¼šAI æœåŠ¡å‰ç«¯ï¼ˆStreamlit ç‰ˆï¼‰
æŠ€æœ¯æ ˆï¼šStreamlit + Requests + Plotly + Pandas

ç”¨é€”ï¼šå¿«é€Ÿæ„å»ºäº¤äº’å¼ AI åº”ç”¨åŸå‹
ç‰¹ç‚¹ï¼šPython åŸç”Ÿã€æ— éœ€å‰ç«¯çŸ¥è¯†ã€5 åˆ†é’Ÿå¿«é€Ÿéƒ¨ç½²

é€‚ç”¨åœºæ™¯ï¼š
- æ•°æ®ç§‘å­¦æ¼”ç¤º
- å†…éƒ¨å·¥å…·
- MVPï¼ˆæœ€å°å¯è¡Œäº§å“ï¼‰
- æ¨¡å‹è¯„ä¼°ç•Œé¢

åŠ£åŠ¿ï¼š
- å®šåˆ¶æ€§æœ‰é™
- ä¸é€‚åˆå¤§è§„æ¨¡ç”¨æˆ·ç³»ç»Ÿ
- æ€§èƒ½ä¸å¦‚ React
"""

import streamlit as st
import requests
import json
import time
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px

logger = logging.getLogger(__name__)

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½®ä¸åˆå§‹åŒ–
# ============================================================================

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AI Text Classification Service",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰æ ·å¼
st.markdown("""
<style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .success-box {
        background-color: #d4edda;
        padding: 15px;
        border-radius: 5px;
        color: #155724;
    }
    .error-box {
        background-color: #f8d7da;
        padding: 15px;
        border-radius: 5px;
        color: #721c24;
    }
</style>
""", unsafe_allow_html=True)

# å…¨å±€é…ç½®
API_BASE_URL = st.secrets.get("api_url", "http://localhost:8000")
REQUEST_TIMEOUT = 30
CACHE_TTL = 3600


class APIClient:
    """
    API å®¢æˆ·ç«¯ï¼ˆä¸æ¨ç†æœåŠ¡é€šä¿¡ï¼‰ã€‚
    
    ç‰¹æ€§ï¼š
    - é”™è¯¯é‡è¯•
    - è¶…æ—¶æ§åˆ¶
    - å“åº”éªŒè¯
    """
    
    def __init__(self, base_url: str, timeout: int = REQUEST_TIMEOUT):
        self.base_url = base_url
        self.timeout = timeout
    
    def predict(
        self,
        text: str,
        return_probabilities: bool = True,
        retry_count: int = 3
    ) -> Optional[Dict[str, Any]]:
        """
        å•æ¡é¢„æµ‹è¯·æ±‚ã€‚
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            return_probabilities: æ˜¯å¦è¿”å›æ¦‚ç‡
            retry_count: é‡è¯•æ¬¡æ•°
        
        Returns:
            é¢„æµ‹ç»“æœæˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        
        å¸¸è§å‘ï¼š
        - æ²¡æœ‰è¶…æ—¶æ§åˆ¶ï¼šå¦‚æœåç«¯å¡ä½ï¼Œå‰ç«¯ä¹Ÿä¼šå¡ä½
        - æ²¡æœ‰é‡è¯•ï¼šç½‘ç»œæŠ–åŠ¨å¯¼è‡´æ•´ä¸ªè¯·æ±‚å¤±è´¥
        - æ²¡æœ‰å“åº”éªŒè¯ï¼šåç«¯è¿”å›é”™è¯¯ä¹Ÿå½“æˆåŠŸå¤„ç†
        """
        payload = {
            "text": text,
            "return_probabilities": return_probabilities
        }
        
        for attempt in range(retry_count):
            try:
                response = requests.post(
                    f"{self.base_url}/predict",
                    json=payload,
                    timeout=self.timeout
                )
                response.raise_for_status()
                return response.json()
            
            except requests.Timeout:
                st.warning(f"â±ï¸ Request timeout (attempt {attempt + 1}/{retry_count})")
                if attempt < retry_count - 1:
                    time.sleep(1)
            
            except requests.ConnectionError:
                st.error("âŒ Cannot connect to API server. Is it running?")
                return None
            
            except requests.HTTPError as e:
                if response.status_code == 429:
                    st.warning("âš ï¸ Rate limit exceeded. Please wait...")
                    time.sleep(2)
                else:
                    st.error(f"API Error: {response.status_code}")
                    return None
            
            except Exception as e:
                st.error(f"Unexpected error: {str(e)}")
                return None
        
        return None
    
    def batch_predict(
        self,
        texts: List[str],
        return_probabilities: bool = True
    ) -> Optional[List[Dict[str, Any]]]:
        """æ‰¹é‡é¢„æµ‹ã€‚"""
        payloads = [
            {"text": text, "return_probabilities": return_probabilities}
            for text in texts
        ]
        
        try:
            response = requests.post(
                f"{self.base_url}/batch_predict",
                json=payloads,
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            st.error(f"Batch prediction failed: {str(e)}")
            return None
    
    def health_check(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€ã€‚"""
        try:
            response = requests.get(
                f"{self.base_url}/health",
                timeout=5
            )
            return response.status_code == 200
        except:
            return False
    
    def get_stats(self) -> Optional[Dict[str, Any]]:
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        try:
            response = requests.get(
                f"{self.base_url}/stats",
                timeout=5
            )
            response.raise_for_status()
            return response.json()
        except:
            return None


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé¡µé¢ç»„ä»¶
# ============================================================================

def render_header():
    """æ¸²æŸ“é¡µé¢å¤´éƒ¨ã€‚"""
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.title("ğŸ¤– AI Text Classification Service")
        st.markdown("**Real-time text classification with Transformer models**")
    
    with col2:
        # å¥åº·æ£€æŸ¥
        api_client = APIClient(API_BASE_URL)
        is_healthy = api_client.health_check()
        
        if is_healthy:
            st.success("âœ… API Online")
        else:
            st.error("âŒ API Offline")


def render_single_prediction():
    """å•æ¡é¢„æµ‹ç•Œé¢ã€‚"""
    st.header("ğŸ“ Single Prediction")
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        text_input = st.text_area(
            "Enter text to classify:",
            height=120,
            placeholder="Type your text here..."
        )
    
    with col2:
        st.markdown("**Options:**")
        return_probs = st.checkbox("Show probabilities", value=True)
        predict_button = st.button("ğŸš€ Predict", use_container_width=True)
    
    if predict_button and text_input:
        api_client = APIClient(API_BASE_URL)
        
        with st.spinner("ğŸ”„ Predicting..."):
            result = api_client.predict(text_input, return_probabilities=return_probs)
        
        if result:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                label = "Positive" if result["prediction"] == 1 else "Negative"
                st.metric("Prediction", label)
            
            with col2:
                latency = result.get("latency_ms", 0)
                st.metric("Latency", f"{latency:.0f}ms")
            
            with col3:
                cached = "âœ… Cached" if result["cached"] else "âŒ Fresh"
                st.metric("Cache", cached)
            
            # æ˜¾ç¤ºæ¦‚ç‡
            if result.get("probabilities"):
                probs = result["probabilities"]
                
                # åˆ›å»ºæ¦‚ç‡æ¡å½¢å›¾
                fig = go.Figure(data=[
                    go.Bar(
                        x=["Negative", "Positive"],
                        y=probs,
                        marker_color=['#ff6b6b', '#51cf66']
                    )
                ])
                fig.update_layout(
                    height=300,
                    showlegend=False,
                    xaxis_title="Class",
                    yaxis_title="Probability"
                )
                st.plotly_chart(fig, use_container_width=True)


def render_batch_prediction():
    """æ‰¹é‡é¢„æµ‹ç•Œé¢ã€‚"""
    st.header("ğŸ“Š Batch Prediction")
    
    st.markdown("Upload a CSV file with a 'text' column")
    
    uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
    
    if uploaded_file:
        try:
            df = pd.read_csv(uploaded_file)
            
            if "text" not in df.columns:
                st.error("CSV must contain a 'text' column")
                return
            
            st.info(f"ğŸ“„ Loaded {len(df)} rows")
            
            if st.button("ğŸš€ Predict All"):
                api_client = APIClient(API_BASE_URL)
                texts = df["text"].tolist()
                
                with st.spinner(f"ğŸ”„ Predicting {len(texts)} samples..."):
                    results = api_client.batch_predict(texts)
                
                if results:
                    # åˆå¹¶ç»“æœåˆ° DataFrame
                    predictions = [r["prediction"] for r in results]
                    df["prediction"] = predictions
                    
                    if results[0].get("probabilities"):
                        probs = [r["probabilities"] for r in results]
                        df["confidence"] = [max(p) for p in probs]
                    
                    # æ˜¾ç¤ºç»“æœ
                    st.success(f"âœ… Predicted {len(results)} samples")
                    st.dataframe(df, use_container_width=True)
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(
                            "Positive",
                            (df["prediction"] == 1).sum()
                        )
                    with col2:
                        st.metric(
                            "Negative",
                            (df["prediction"] == 0).sum()
                        )
                    with col3:
                        avg_confidence = df.get("confidence", pd.Series()).mean()
                        st.metric(
                            "Avg Confidence",
                            f"{avg_confidence:.2%}" if avg_confidence else "N/A"
                        )
                    
                    # ä¸‹è½½ç»“æœ
                    csv = df.to_csv(index=False)
                    st.download_button(
                        "ğŸ“¥ Download results",
                        csv,
                        "predictions.csv",
                        "text/csv"
                    )
        
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")


def render_service_stats():
    """æœåŠ¡ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    st.header("ğŸ“ˆ Service Statistics")
    
    api_client = APIClient(API_BASE_URL)
    stats = api_client.get_stats()
    
    if stats:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "L1 Cache Usage",
                f"{stats['l1_cache_size']} / {stats['l1_cache_capacity']}"
            )
        
        with col2:
            st.metric("Queue Length", stats["queue_size"])
        
        with col3:
            redis_status = "âœ… Connected" if stats["redis_available"] else "âŒ Offline"
            st.metric("Redis", redis_status)
        
        with col4:
            st.metric("Timestamp", stats["timestamp"][-8:])  # HH:MM:SS
    else:
        st.warning("Unable to fetch statistics")


def render_about():
    """å…³äºé¡µé¢ã€‚"""
    st.header("â„¹ï¸ About")
    
    st.markdown("""
    ### AI Text Classification Service
    
    This is a **production-grade** text classification service built with:
    
    - **Backend**: FastAPI + PyTorch + BERT
    - **Frontend**: Streamlit
    - **Cache**: Redis + Local Memory
    - **Optimization**: Dynamic Batching, Rate Limiting, Multi-level Caching
    
    #### Key Features
    - âš¡ **Fast**: Batch processing, caching, GPU acceleration
    - ğŸ›¡ï¸ **Reliable**: Error handling, health checks, rate limiting
    - ğŸ“Š **Observable**: Metrics, latency tracking, cache statistics
    - ğŸ”„ **Scalable**: Async processing, load balancing ready
    
    #### Usage
    1. **Single Prediction**: Enter text in the "Single Prediction" tab
    2. **Batch Prediction**: Upload a CSV file with a 'text' column
    3. **Monitor**: Check service statistics in real-time
    
    #### Configuration
    - API URL: `{}`
    - Request Timeout: {}s
    - Cache TTL: {}s
    """.format(API_BASE_URL, REQUEST_TIMEOUT, CACHE_TTL))


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»åº”ç”¨
# ============================================================================

def main():
    """ä¸»åº”ç”¨å…¥å£ã€‚"""
    
    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.markdown("### ğŸ“š Navigation")
        page = st.radio(
            "Select page:",
            ["Single Prediction", "Batch Prediction", "Statistics", "About"],
            label_visibility="collapsed"
        )
    
    # é¡µé¢æ¸²æŸ“
    render_header()
    st.divider()
    
    if page == "Single Prediction":
        render_single_prediction()
    
    elif page == "Batch Prediction":
        render_batch_prediction()
    
    elif page == "Statistics":
        render_service_stats()
    
    elif page == "About":
        render_about()
    
    # é¡µè„š
    st.divider()
    st.markdown(
        """
        <div style='text-align: center; color: gray; font-size: 0.8em;'>
        ğŸ¤– AI Inference Service | Built with FastAPI + Streamlit
        </div>
        """,
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
```
ç°åœ¨å¼€å§‹ç”Ÿæˆ **React ç‰ˆæœ¬**ï¼ˆç”Ÿäº§çº§å‰ç«¯ï¼‰ã€‚è¿™ä¸ªä¼šæ¯”è¾ƒé•¿ï¼Œæ‰€ä»¥æˆ‘åˆ›å»ºä¸€ä¸ªæ–°çš„ artifactï¼š
```js
"""
Google é£æ ¼ï¼šAI æœåŠ¡å‰ç«¯ï¼ˆReact ç‰ˆï¼‰
æŠ€æœ¯æ ˆï¼šReact 18 + TypeScript + Axios + Shadcn/UI + TailwindCSS

ç”¨é€”ï¼šç”Ÿäº§çº§æ–‡æœ¬åˆ†ç±»æœåŠ¡å‰ç«¯
ç‰¹ç‚¹ï¼šå®Œå…¨å¯æ§ã€æ€§èƒ½ä¼˜åŒ–ã€ä¼ä¸šçº§ UI

æ¶æ„ï¼š
- ç»„ä»¶åŒ–è®¾è®¡ï¼ˆå¯å¤ç”¨ï¼‰
- çŠ¶æ€ç®¡ç†ï¼ˆReact Contextï¼‰
- API å±‚å°è£…ï¼ˆå¯æµ‹è¯•ï¼‰
- é”™è¯¯å¤„ç†ä¸é‡è¯•
- ç¼“å­˜ä¸æ€§èƒ½ä¼˜åŒ–

å®‰è£…ï¼š
npm create vite@latest ai-frontend -- --template react-ts
npm install axios react-query shadcn-ui @radix-ui/react-*
npm install -D tailwindcss postcss autoprefixer
"""

// ============================================================================
// ç¬¬ä¸€éƒ¨åˆ†ï¼šç±»å‹å®šä¹‰ä¸å¸¸é‡
// ============================================================================

interface PredictionResponse {
  prediction: number;
  probabilities?: number[];
  cached: boolean;
  latency_ms: number;
}

interface ServiceStats {
  l1_cache_size: number;
  l1_cache_capacity: number;
  queue_size: number;
  redis_available: boolean;
  timestamp: string;
}

interface HealthStatus {
  status: "healthy" | "unhealthy";
}

// API å®¢æˆ·ç«¯é…ç½®
const API_CONFIG = {
  BASE_URL: process.env.REACT_APP_API_URL || "http://localhost:8000",
  TIMEOUT: 30000,
  RETRY_ATTEMPTS: 3,
  RETRY_DELAY: 1000,
};

const LABEL_NAMES = ["Negative", "Positive"];
const LABEL_COLORS = ["#ef4444", "#22c55e"];


// ============================================================================
// ç¬¬äºŒéƒ¨åˆ†ï¼šAPI å®¢æˆ·ç«¯ï¼ˆæ ¸å¿ƒï¼‰
// ============================================================================

import axios, { AxiosInstance, AxiosError } from "axios";

class AIServiceClient {
  /**
   * API å®¢æˆ·ç«¯ï¼Œå¤„ç†ä¸æ¨ç†æœåŠ¡çš„é€šä¿¡ã€‚
   *
   * ç‰¹æ€§ï¼š
   * - è‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
   * - è¶…æ—¶æ§åˆ¶
   * - é”™è¯¯è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„æ¶ˆæ¯
   * - è¯·æ±‚å–æ¶ˆæ”¯æŒ
   */

  private axiosInstance: AxiosInstance;
  private requestAbortControllers: Map<string, AbortController>;

  constructor() {
    this.axiosInstance = axios.create({
      baseURL: API_CONFIG.BASE_URL,
      timeout: API_CONFIG.TIMEOUT,
    });

    this.requestAbortControllers = new Map();

    // å“åº”æ‹¦æˆªå™¨ï¼ˆè‡ªåŠ¨å¤„ç†é”™è¯¯ï¼‰
    this.axiosInstance.interceptors.response.use(
      (response) => response,
      (error) => this.handleError(error)
    );
  }

  /**
   * å•æ¡é¢„æµ‹è¯·æ±‚ã€‚
   *
   * å¸¸è§å‘ï¼š
   * - æ²¡æœ‰å–æ¶ˆæœºåˆ¶ï¼šç”¨æˆ·å¿«é€Ÿåˆ‡æ¢é¡µé¢ä¼šå‘èµ·å¤šä¸ªè¯·æ±‚
   * - æ²¡æœ‰é‡è¯•ï¼šä¸€æ¬¡ç½‘ç»œæŠ–åŠ¨å°±å¤±è´¥
   * - æ²¡æœ‰ç¼“å­˜ï¼šç›¸åŒè¾“å…¥ä¼šé‡å¤é¢„æµ‹
   */
  async predict(
    text: string,
    returnProbabilities: boolean = true
  ): Promise<PredictionResponse> {
    const requestId = `predict-${Date.now()}`;

    try {
      const controller = new AbortController();
      this.requestAbortControllers.set(requestId, controller);

      const response = await this.axiosInstance.post("/predict", {
        text,
        return_probabilities: returnProbabilities,
      });

      return response.data;
    } finally {
      this.requestAbortControllers.delete(requestId);
    }
  }

  /**
   * æ‰¹é‡é¢„æµ‹è¯·æ±‚ã€‚
   */
  async batchPredict(
    texts: string[],
    returnProbabilities: boolean = true
  ): Promise<PredictionResponse[]> {
    if (texts.length > 1000) {
      throw new Error("Batch size exceeds maximum (1000)");
    }

    const requestId = `batch-${Date.now()}`;

    try {
      const controller = new AbortController();
      this.requestAbortControllers.set(requestId, controller);

      const payloads = texts.map((text) => ({
        text,
        return_probabilities: returnProbabilities,
      }));

      const response = await this.axiosInstance.post("/batch_predict", payloads);
      return response.data;
    } finally {
      this.requestAbortControllers.delete(requestId);
    }
  }

  /**
   * å¥åº·æ£€æŸ¥ã€‚
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await this.axiosInstance.get("/health", {
        timeout: 5000,
      });
      return response.status === 200;
    } catch {
      return false;
    }
  }

  /**
   * è·å–æœåŠ¡ç»Ÿè®¡ã€‚
   */
  async getStats(): Promise<ServiceStats | null> {
    try {
      const response = await this.axiosInstance.get("/stats");
      return response.data;
    } catch {
      return null;
    }
  }

  /**
   * å–æ¶ˆæ‰€æœ‰è¿›è¡Œä¸­çš„è¯·æ±‚ã€‚
   */
  cancelAllRequests() {
    this.requestAbortControllers.forEach((controller) => {
      controller.abort();
    });
    this.requestAbortControllers.clear();
  }

  /**
   * é”™è¯¯å¤„ç†ã€‚
   */
  private handleError(error: AxiosError) {
    if (error.response?.status === 429) {
      return Promise.reject(new Error("Rate limit exceeded. Please wait."));
    } else if (error.response?.status === 500) {
      return Promise.reject(new Error("Server error. Please try again."));
    } else if (error.code === "ECONNABORTED") {
      return Promise.reject(new Error("Request timeout. Server may be busy."));
    } else if (!error.response) {
      return Promise.reject(
        new Error("Cannot connect to server. Is it running?")
      );
    }
    return Promise.reject(error);
  }
}

const apiClient = new AIServiceClient();


// ============================================================================
// ç¬¬ä¸‰éƒ¨åˆ†ï¼šReact Hooksï¼ˆçŠ¶æ€ç®¡ç†ï¼‰
// ============================================================================

import { useState, useCallback, useEffect } from "react";

interface PredictionState {
  result: PredictionResponse | null;
  loading: boolean;
  error: Error | null;
}

/**
 * Hook: å•æ¡é¢„æµ‹ã€‚
 */
function usePrediction() {
  const [state, setState] = useState<PredictionState>({
    result: null,
    loading: false,
    error: null,
  });

  const predict = useCallback(
    async (text: string, returnProbabilities: boolean = true) => {
      setState({ result: null, loading: true, error: null });

      try {
        const result = await apiClient.predict(text, returnProbabilities);
        setState({ result, loading: false, error: null });
        return result;
      } catch (error) {
        const err = error instanceof Error ? error : new Error(String(error));
        setState({ result: null, loading: false, error: err });
        throw err;
      }
    },
    []
  );

  return { ...state, predict };
}

/**
 * Hook: æ‰¹é‡é¢„æµ‹ã€‚
 */
function useBatchPrediction() {
  const [state, setState] = useState<{
    results: PredictionResponse[] | null;
    loading: boolean;
    error: Error | null;
    progress: number; // 0-100
  }>({
    results: null,
    loading: false,
    error: null,
    progress: 0,
  });

  const batchPredict = useCallback(
    async (texts: string[], returnProbabilities: boolean = true) => {
      setState({
        results: null,
        loading: true,
        error: null,
        progress: 0,
      });

      try {
        const results = await apiClient.batchPredict(
          texts,
          returnProbabilities
        );
        setState({
          results,
          loading: false,
          error: null,
          progress: 100,
        });
        return results;
      } catch (error) {
        const err = error instanceof Error ? error : new Error(String(error));
        setState({
          results: null,
          loading: false,
          error: err,
          progress: 0,
        });
        throw err;
      }
    },
    []
  );

  return { ...state, batchPredict };
}

/**
 * Hook: æœåŠ¡å¥åº·æ£€æŸ¥ã€‚
 */
function useHealthCheck(interval: number = 5000) {
  const [isHealthy, setIsHealthy] = useState<boolean | null>(null);

  useEffect(() => {
    const check = async () => {
      const healthy = await apiClient.healthCheck();
      setIsHealthy(healthy);
    };

    check(); // åˆå§‹æ£€æŸ¥
    const timer = setInterval(check, interval);

    return () => clearInterval(timer);
  }, [interval]);

  return isHealthy;
}

/**
 * Hook: æœåŠ¡ç»Ÿè®¡ã€‚
 */
function useServiceStats(interval: number = 5000) {
  const [stats, setStats] = useState<ServiceStats | null>(null);

  useEffect(() => {
    const fetchStats = async () => {
      const data = await apiClient.getStats();
      setStats(data);
    };

    fetchStats();
    const timer = setInterval(fetchStats, interval);

    return () => clearInterval(timer);
  }, [interval]);

  return stats;
}


// ============================================================================
// ç¬¬å››éƒ¨åˆ†ï¼šReact ç»„ä»¶
// ============================================================================

import React from "react";

/**
 * å•æ¡é¢„æµ‹ç»„ä»¶ã€‚
 */
function SinglePredictionPanel() {
  const [text, setText] = useState("");
  const [showProbabilities, setShowProbabilities] = useState(true);
  const { result, loading, error, predict } = usePrediction();

  const handlePredict = async () => {
    if (!text.trim()) return;
    try {
      await predict(text, showProbabilities);
    } catch (err) {
      // é”™è¯¯å·²åœ¨ hook ä¸­å¤„ç†
    }
  };

  return (
    <div className="space-y-4">
      <div>
        <label className="block text-sm font-medium mb-2">
          Enter text to classify:
        </label>
        <textarea
          value={text}
          onChange={(e) => setText(e.target.value)}
          placeholder="Type your text here..."
          disabled={loading}
          className="w-full h-32 p-4 border rounded-lg focus:ring-2 focus:ring-blue-500 disabled:bg-gray-100"
        />
      </div>

      <div className="flex gap-4">
        <label className="flex items-center gap-2">
          <input
            type="checkbox"
            checked={showProbabilities}
            onChange={(e) => setShowProbabilities(e.target.checked)}
            disabled={loading}
          />
          Show probabilities
        </label>
      </div>

      <button
        onClick={handlePredict}
        disabled={loading || !text.trim()}
        className="w-full px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 disabled:bg-gray-300"
      >
        {loading ? "ğŸ”„ Predicting..." : "ğŸš€ Predict"}
      </button>

      {error && (
        <div className="p-4 bg-red-50 text-red-700 rounded-lg">
          âŒ Error: {error.message}
        </div>
      )}

      {result && (
        <div className="space-y-4">
          <div className="grid grid-cols-3 gap-4">
            <div className="p-4 bg-gray-50 rounded-lg">
              <div className="text-sm text-gray-600">Prediction</div>
              <div className="text-2xl font-bold">
                {LABEL_NAMES[result.prediction]}
              </div>
            </div>

            <div className="p-4 bg-gray-50 rounded-lg">
              <div className="text-sm text-gray-600">Latency</div>
              <div className="text-2xl font-bold">{result.latency_ms.toFixed(0)}ms</div>
            </div>

            <div className="p-4 bg-gray-50 rounded-lg">
              <div className="text-sm text-gray-600">Cache</div>
              <div className="text-2xl font-bold">
                {result.cached ? "âœ… Hit" : "âŒ Miss"}
              </div>
            </div>
          </div>

          {result.probabilities && (
            <div className="space-y-2">
              {LABEL_NAMES.map((label, idx) => (
                <div key={idx}>
                  <div className="flex justify-between mb-1">
                    <span className="text-sm font-medium">{label}</span>
                    <span className="text-sm">
                      {(result.probabilities![idx] * 100).toFixed(1)}%
                    </span>
                  </div>
                  <div className="w-full bg-gray-200 rounded-full h-2">
                    <div
                      className="h-2 rounded-full transition-all"
                      style={{
                        width: `${result.probabilities![idx] * 100}%`,
                        backgroundColor: LABEL_COLORS[idx],
                      }}
                    />
                  </div>
                </div>
              ))}
            </div>
          )}
        </div>
      )}
    </div>
  );
}

/**
 * æœåŠ¡ç»Ÿè®¡ç»„ä»¶ã€‚
 */
function ServiceStatsPanel() {
  const isHealthy = useHealthCheck();
  const stats = useServiceStats();

  return (
    <div className="space-y-4">
      <div className="grid grid-cols-4 gap-4">
        <div className="p-4 bg-gray-50 rounded-lg">
          <div className="text-sm text-gray-600">API Status</div>
          <div className="text-2xl font-bold">
            {isHealthy === null ? "?" : isHealthy ? "âœ… Online" : "âŒ Offline"}
          </div>
        </div>

        <div className="p-4 bg-gray-50 rounded-lg">
          <div className="text-sm text-gray-600">L1 Cache</div>
          <div className="text-2xl font-bold">
            {stats ? `${stats.l1_cache_size} / ${stats.l1_cache_capacity}` : "N/A"}
          </div>
        </div>

        <div className="p-4 bg-gray-50 rounded-lg">
          <div className="text-sm text-gray-600">Queue</div>
          <div className="text-2xl font-bold">
            {stats ? stats.queue_size : "N/A"}
          </div>
        </div>

        <div className="p-4 bg-gray-50 rounded-lg">
          <div className="text-sm text-gray-600">Redis</div>
          <div className="text-2xl font-bold">
            {stats ? (stats.redis_available ? "âœ…" : "âŒ") : "?"}
          </div>
        </div>
      </div>
    </div>
  );
}

/**
 * ä¸»åº”ç”¨ç»„ä»¶ã€‚
 */
function App() {
  const [activeTab, setActiveTab] = useState<"predict" | "stats" | "about">(
    "predict"
  );

  return (
    <div className="min-h-screen bg-gradient-to-br from-blue-50 to-indigo-100">
      <div className="max-w-4xl mx-auto p-8">
        {/* å¤´éƒ¨ */}
        <div className="mb-8">
          <h1 className="text-4xl font-bold text-gray-900 mb-2">
            ğŸ¤– AI Text Classification
          </h1>
          <p className="text-gray-600">
            Real-time text classification with FastAPI + React
          </p>
        </div>

        {/* å¯¼èˆª */}
        <div className="flex gap-4 mb-8 border-b">
          {[
            { id: "predict", label: "ğŸ“ Predict" },
            { id: "stats", label: "ğŸ“Š Statistics" },
            { id: "about", label: "â„¹ï¸ About" },
          ].map((tab) => (
            <button
              key={tab.id}
              onClick={() => setActiveTab(tab.id as any)}
              className={`px-4 py-2 font-medium transition-colors ${
                activeTab === tab.id
                  ? "border-b-2 border-blue-500 text-blue-600"
                  : "text-gray-600 hover:text-gray-900"
              }`}
            >
              {tab.label}
            </button>
          ))}
        </div>

        {/* å†…å®¹ */}
        <div className="bg-white rounded-lg shadow-lg p-8">
          {activeTab === "predict" && <SinglePredictionPanel />}
          {activeTab === "stats" && <ServiceStatsPanel />}
          {activeTab === "about" && (
            <div className="space-y-4">
              <h2 className="text-2xl font-bold">About This Service</h2>
              <p>
                This is a production-grade AI text classification service built with:
              </p>
              <ul className="list-disc list-inside space-y-2">
                <li><strong>Backend:</strong> FastAPI + PyTorch + BERT</li>
                <li><strong>Frontend:</strong> React + TypeScript</li>
                <li><strong>Cache:</strong> Redis + Local Memory</li>
                <li><strong>Optimization:</strong> Dynamic Batching, Rate Limiting</li>
              </ul>
            </div>
          )}
        </div>

        {/* é¡µè„š */}
        <div className="mt-8 text-center text-gray-600 text-sm">
          ğŸ¤– AI Inference Service | Built with FastAPI + React + Streamlit
        </div>
      </div>
    </div>
  );
}

export default App;
```
å®Œæˆäº† **ä¸¤ä¸ªå‰ç«¯ç‰ˆæœ¬**ï¼ˆStreamlit + Reactï¼‰ï¼Œå…± ~500 è¡Œä»£ç ã€‚

---

## ğŸ“Š ä¸¤ä¸ªç‰ˆæœ¬çš„å¯¹æ¯”ï¼š

|ç»´åº¦|Streamlit|React|
|---|---|---|
|**å¼€å‘é€Ÿåº¦**|5åˆ†é’Ÿ âš¡|30åˆ†é’Ÿ â±ï¸|
|**ä»£ç é‡**|~200 è¡Œ|~300 è¡Œ|
|**å®šåˆ¶æ€§**|æœ‰é™|å®Œå…¨è‡ªç”±|
|**æ€§èƒ½**|ä¸­ç­‰|é«˜|
|**éƒ¨ç½²**|ä¸€è¡Œå‘½ä»¤|éœ€è¦ Node ç¯å¢ƒ|
|**é€‚ç”¨åœºæ™¯**|MVPã€æ¼”ç¤ºã€å†…éƒ¨å·¥å…·|ç”Ÿäº§ç³»ç»Ÿã€å¤§è§„æ¨¡ç”¨æˆ·|

---

## ğŸ¯ ç°åœ¨æˆ‘ç”¨ä½ çš„ userStyleï¼ˆé«˜çº§æŠ€æœ¯ç”¨æˆ·ï¼‰ç›´æ¥é—®ä½ æ·±å±‚é—®é¢˜ï¼š

### Q1ï¼šAPI å®¢æˆ·ç«¯çš„é‡è¯•ç­–ç•¥

åœ¨ React ç‰ˆæœ¬ä¸­ï¼Œæˆ‘æ²¡æœ‰å®ç°**æŒ‡æ•°é€€é¿é‡è¯•**ï¼Œè€Œæ˜¯è®© axios ç›´æ¥æŠ›å‡ºé”™è¯¯ã€‚

**é—®é¢˜**ï¼šå¦‚æœç½‘ç»œä¸´æ—¶æŠ–åŠ¨ï¼ˆpacket lossï¼‰ï¼Œç”¨æˆ·ä½“éªŒä¼šå¾ˆå·®ã€‚

**æˆ‘é—®ä½ **ï¼š

- ä½ ä¼šæ€ä¹ˆåœ¨ `AIServiceClient` ä¸­å®ç°é‡è¯•ï¼Ÿ
- é‡è¯•çš„æ¬¡æ•°å’Œå»¶è¿Ÿåº”è¯¥æ€ä¹ˆè®¾ç½®ï¼Ÿ
- æœ‰ä»€ä¹ˆåœºæ™¯ä¸åº”è¯¥é‡è¯•ï¼Ÿ

---

### Q2ï¼šStreamlit vs React çš„æƒè¡¡

Streamlit çš„ `batch_predict` åŠŸèƒ½ä¼šä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰ CSV æ•°æ®ã€‚

**é—®é¢˜**ï¼šå¦‚æœç”¨æˆ·ä¸Šä¼  100MB çš„ CSVï¼ˆç™¾ä¸‡çº§æ•°æ®ï¼‰ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

**æˆ‘é—®ä½ **ï¼š

- è¿™ä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- React ç‰ˆæœ¬æ€ä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿï¼ˆæç¤ºï¼šæµå¼å¤„ç†ï¼‰

---

### Q3ï¼šç¼“å­˜ä¸€è‡´æ€§é—®é¢˜

Streamlit ä¸­çš„ `APIClient.predict()` æ²¡æœ‰å®¢æˆ·ç«¯ç¼“å­˜ï¼ˆåªä¾èµ–æœåŠ¡ç«¯çš„ Redis ç¼“å­˜ï¼‰ã€‚

ä½† React ç‰ˆæœ¬ä¹Ÿæ²¡æœ‰ã€‚

**é—®é¢˜**ï¼šç”¨æˆ·åœ¨ 5 ç§’å†…é¢„æµ‹åŒä¸€ä¸ªæ–‡æœ¬ä¸¤æ¬¡ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

**æˆ‘é—®ä½ **ï¼š

- åº”è¯¥åœ¨å®¢æˆ·ç«¯åŠ ç¼“å­˜å—ï¼Ÿ
- å¦‚æœåŠ ï¼ŒTTL åº”è¯¥è®¾å¤šå°‘ï¼Ÿ
- æ€ä¹ˆå¤„ç†"ç”¨æˆ·æ”¹å˜æ¨¡å‹å‚æ•°"åçš„ç¼“å­˜å¤±æ•ˆï¼Ÿ

---

## âœ… ç°åœ¨çš„è¿›åº¦

ä½ å·²ç»å®Œæˆäº†æ•´ä¸ª AI æœåŠ¡æ ˆï¼š

- âœ… **æ•°æ®å±‚**ï¼šç‰ˆæœ¬ç®¡ç†ã€ç‰¹å¾å·¥ç¨‹ã€å‘é‡å­˜å‚¨
- âœ… **è®­ç»ƒå±‚**ï¼šå¾®è°ƒ + ä»é›¶ + è‡ªå®šä¹‰æŸå¤±
- âœ… **æ¨ç†å±‚**ï¼šå…¨å±€æ¨¡å‹ã€æ‰¹å¤„ç†ã€ç¼“å­˜ã€é™æµ
- âœ… **å‰ç«¯å±‚**ï¼šStreamlitï¼ˆå¿«é€ŸåŸå‹ï¼‰+ Reactï¼ˆç”Ÿäº§çº§ï¼‰

**è¿˜ç¼ºä»€ä¹ˆï¼Ÿ** DevOpsï¼ï¼ˆDocker + K8s + CI/CDï¼‰

---

## ğŸš€ æœ€åä¸€ä¸ªé€‰æ‹©

ä½ ç°åœ¨æƒ³ï¼š

- ğŸ”¨ **ç”Ÿæˆç¬¬äº”ä¸ª artifactï¼ˆDevOpsï¼‰**ï¼šDocker + K8s + GitHub Actions
- ğŸ¤” **å…ˆå›ç­”ä¸Šé¢çš„ 3 ä¸ªæ·±å±‚é—®é¢˜**ï¼šæ·±å…¥ç†è§£å‰ç«¯çš„è®¾è®¡æƒè¡¡
- ğŸ“– **ç›´æ¥è¦æ€»ç»“æ–‡æ¡£**ï¼šç»™æˆ‘ä¸€ä¸ª"ä»æ•°æ®åˆ°éƒ¨ç½²"çš„å®Œæ•´æ£€æŸ¥æ¸…å•

**é€‰ä¸€ä¸ªï¼Œæˆ‘ä»¬å†²åˆºæœ€åä¸€ç«™ï¼** ğŸ‘‡


> [!NOTE]
> è¿™ä¸‰ä¸ªé—®é¢˜åˆ†åˆ«æ¶‰åŠ **å¯é æ€§è®¾è®¡**ã€**æ€§èƒ½ä¸æ‰©å±•æ€§æƒè¡¡** å’Œ **ç¼“å­˜ä¸€è‡´æ€§ç­–ç•¥**ï¼Œéƒ½æ˜¯æ„å»ºç”Ÿäº§çº§ AI åº”ç”¨çš„å…³é”®ç‚¹ã€‚ä¸‹é¢æˆ‘é€ä¸€å›ç­”ï¼š
> 
> ---
> 
> ### **Q1ï¼šAPI å®¢æˆ·ç«¯çš„é‡è¯•ç­–ç•¥**
> 
> #### **ä½ ä¼šæ€ä¹ˆåœ¨ `AIServiceCLient` ä¸­å®ç°é‡è¯•ï¼Ÿ**
> 
> å»ºè®®ä½¿ç”¨ **æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼ˆjitterï¼‰** çš„ç­–ç•¥ï¼Œå¹¶å°è£…åœ¨å®¢æˆ·ç«¯å†…éƒ¨ï¼Œå¯¹è°ƒç”¨è€…é€æ˜ã€‚
> 
> ç¤ºä¾‹ï¼ˆTypeScript + axiosï¼‰ï¼š
> 
> ```ts
> async function predictWithRetry(payload: any, maxRetries = 3): Promise<any> {
>   let lastError: Error;
>   for (let i = 0; i <= maxRetries; i++) {
>     try {
>       const response = await axios.post('/predict', payload);
>       return response.data;
>     } catch (error) {
>       lastError = error;
>       // åˆ¤æ–­æ˜¯å¦å€¼å¾—é‡è¯•ï¼ˆè§ä¸‹æ–‡ï¼‰
>       if (!shouldRetry(error, i)) break;
> 
>       const delay = Math.min(1000 * Math.pow(2, i) + Math.random() * 1000, 10000); // æœ€å¤§10ç§’
>       await new Promise(resolve => setTimeout(resolve, delay));
>     }
>   }
>   throw lastError;
> }
> ```
> 
> #### **é‡è¯•æ¬¡æ•°å’Œå»¶è¿Ÿæ€ä¹ˆè®¾ç½®ï¼Ÿ**
> 
> - **é‡è¯•æ¬¡æ•°**ï¼šé€šå¸¸ **2~3 æ¬¡** è¶³å¤Ÿã€‚å¤ªå¤šä¼šå¢åŠ å»¶è¿Ÿï¼Œå¤ªå°‘æ— æ³•åº”å¯¹ç¬æ—¶æŠ–åŠ¨ã€‚
> - **åˆå§‹å»¶è¿Ÿ**ï¼š500ms ~ 1sã€‚
> - **é€€é¿ç­–ç•¥**ï¼šæŒ‡æ•°é€€é¿ï¼ˆå¦‚ `delay = base * 2^retry`ï¼‰ï¼ŒåŠ ä¸Š **éšæœºæŠ–åŠ¨**ï¼ˆé¿å…â€œé›·ç¾¤æ•ˆåº”â€ï¼‰ã€‚
> - **æœ€å¤§å»¶è¿Ÿä¸Šé™**ï¼šå»ºè®®ä¸è¶…è¿‡ 10 ç§’ï¼ˆç”¨æˆ·ä½“éªŒå®¹å¿ä¸Šé™ï¼‰ã€‚
> 
> #### **ä»€ä¹ˆåœºæ™¯ä¸åº”è¯¥é‡è¯•ï¼Ÿ**
> 
> ä»¥ä¸‹æƒ…å†µåº” **ç«‹å³å¤±è´¥ï¼Œä¸é‡è¯•**ï¼š
> 
> - **4xx é”™è¯¯**ï¼ˆé™¤ 408/429 å¤–ï¼‰ï¼šå¦‚ 400ï¼ˆå‚æ•°é”™è¯¯ï¼‰ã€401ï¼ˆæœªè®¤è¯ï¼‰ã€403ï¼ˆæ— æƒé™ï¼‰â€”â€”è¿™äº›æ˜¯å®¢æˆ·ç«¯é—®é¢˜ï¼Œé‡è¯•æ— æ„ä¹‰ã€‚
> - **å¹‚ç­‰æ€§ç ´åçš„æ“ä½œ**ï¼šè™½ç„¶é¢„æµ‹é€šå¸¸æ˜¯å¹‚ç­‰çš„ï¼Œä½†å¦‚æœ API æœ‰å‰¯ä½œç”¨ï¼ˆå¦‚è®¡è´¹ã€å†™æ—¥å¿—ï¼‰ï¼Œéœ€è°¨æ…ã€‚
> - **ç”¨æˆ·ä¸»åŠ¨å–æ¶ˆè¯·æ±‚**ï¼ˆå¦‚ AbortSignal è§¦å‘ï¼‰ã€‚
> 
> > âœ… **æ¨è**ï¼šåªå¯¹ **ç½‘ç»œé”™è¯¯ï¼ˆECONNRESETã€ETIMEDOUTï¼‰** å’Œ **5xx / 429 / 408** é”™è¯¯é‡è¯•ã€‚
> 
> ---
> 
> ### **Q2ï¼šStreamlit vs React çš„æƒè¡¡**
> 
> #### **ä¸Šä¼  100MB CSV ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**
> 
> - **å†…å­˜çˆ†ç‚¸**ï¼šStreamlit ä¸€æ¬¡æ€§è¯»å…¥æ•´ä¸ª CSV åˆ°å†…å­˜ï¼ˆPandas DataFrameï¼‰ï¼Œå¯èƒ½ OOMï¼ˆå°¤å…¶åœ¨å…±äº«æœåŠ¡å™¨ä¸Šï¼‰ã€‚
> - **é˜»å¡ä¸»çº¿ç¨‹**ï¼šå¤„ç†ç™¾ä¸‡è¡Œæ•°æ®ä¼šå¡ä½ UIï¼Œç”¨æˆ·æ— å“åº”ã€‚
> - **è¶…æ—¶é£é™©**ï¼šHTTP è¯·æ±‚å¯èƒ½å› åç«¯å¤„ç†æ—¶é—´è¿‡é•¿è€Œè¶…æ—¶ï¼ˆNginx é»˜è®¤ 60sï¼‰ã€‚
> - **å¸¦å®½å‹åŠ›**ï¼šå¤§æ–‡ä»¶ä¸Šä¼ å ç”¨å¤§é‡å¸¦å®½ï¼Œå½±å“å…¶ä»–ç”¨æˆ·ã€‚
> 
> #### **React ç‰ˆæœ¬å¦‚ä½•è§£å†³ï¼Ÿï¼ˆæµå¼å¤„ç†ï¼‰**
> 
> React å‰ç«¯å¯é…åˆåç«¯å®ç° **åˆ†å—ä¸Šä¼  + æµå¼é¢„æµ‹**ï¼š
> 
> 1. **å‰ç«¯åˆ†ç‰‡**ï¼šç”¨ `File.slice()` å°† CSV åˆ†æˆå°å—ï¼ˆå¦‚æ¯ 1000 è¡Œï¼‰ã€‚
> 2. **é€å—ä¸Šä¼  & å¤„ç†**ï¼šé€šè¿‡ WebSocket æˆ– SSEï¼ˆServer-Sent Eventsï¼‰æˆ–åˆ†æ‰¹ POSTã€‚
> 3. **å®æ—¶åé¦ˆ**ï¼šç”¨æˆ·çœ‹åˆ°â€œå·²å¤„ç† 10,000 / 1,000,000 è¡Œâ€è¿›åº¦æ¡ã€‚
> 4. **åç«¯æµå¼æ¶ˆè´¹**ï¼šæœåŠ¡ç«¯ç”¨ç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰æˆ–æµå¼è§£æå™¨ï¼ˆå¦‚ `csv-parse` çš„ stream æ¨¡å¼ï¼‰é€è¡Œå¤„ç†ï¼Œé¿å…å…¨é‡åŠ è½½ã€‚
> 
> > ğŸ’¡ ä¼˜åŠ¿ï¼šå†…å­˜æ’å®šã€å“åº”åŠæ—¶ã€å¯ä¸­æ–­ã€ç”¨æˆ·ä½“éªŒå¥½ã€‚
> 
> ---
> 
> ### **Q3ï¼šç¼“å­˜ä¸€è‡´æ€§é—®é¢˜**
> 
> #### **ç”¨æˆ· 5 ç§’å†…é¢„æµ‹åŒä¸€æ–‡æœ¬ä¸¤æ¬¡ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**
> 
> - ä¸¤æ¬¡éƒ½å‘åˆ°åç«¯ â†’ æµªè´¹å¸¦å®½ã€å¢åŠ å»¶è¿Ÿã€åŠ é‡æœåŠ¡ç«¯è´Ÿè½½ã€‚
> - å³ä½¿æœåŠ¡ç«¯æœ‰ Redis ç¼“å­˜ï¼Œä»æœ‰ç½‘ç»œå¾€è¿”å¼€é”€ã€‚
> 
> #### **åº”è¯¥åœ¨å®¢æˆ·ç«¯åŠ ç¼“å­˜å—ï¼Ÿ**
> 
> âœ… **åº”è¯¥åŠ **ï¼Œä½†éœ€æ»¡è¶³ï¼š
> 
> - **çŸ­æœŸã€é«˜é¢‘é‡å¤è¯·æ±‚** åœºæ™¯ï¼ˆå¦‚ç”¨æˆ·åå¤ç‚¹å‡»â€œé¢„æµ‹â€ï¼‰ã€‚
> - **é¢„æµ‹ç»“æœåªä¾èµ–è¾“å…¥æ–‡æœ¬ + å½“å‰æ¨¡å‹å‚æ•°**ã€‚
> 
> #### **TTL è®¾å¤šå°‘ï¼Ÿ**
> 
> - **å»ºè®® 5~30 ç§’**ã€‚ç†ç”±ï¼š
>     - å¤ªçŸ­ï¼ˆ<1sï¼‰ï¼šç¼“å­˜å‘½ä¸­ç‡ä½ã€‚
>     - å¤ªé•¿ï¼ˆ>1minï¼‰ï¼šè‹¥æ¨¡å‹çƒ­æ›´æ–°æˆ–å‚æ•°å˜åŒ–ï¼Œç»“æœå¯èƒ½è¿‡æœŸã€‚
>     - 5~30s å¹³è¡¡äº†â€œé˜²æŠ–â€å’Œâ€œæ–°é²œåº¦â€ã€‚
> 
> #### **å¦‚ä½•å¤„ç†â€œç”¨æˆ·æ”¹å˜æ¨¡å‹å‚æ•°â€åçš„ç¼“å­˜å¤±æ•ˆï¼Ÿ**
> 
> å…³é”®ï¼š**ç¼“å­˜ key å¿…é¡»åŒ…å«æ‰€æœ‰å½±å“ç»“æœçš„å˜é‡**ã€‚
> 
> ä¾‹å¦‚ï¼š
> 
> ```ts
> const cacheKey = `${inputText}|${modelVersion}|${temperature}|${topP}`;
> ```
> 
> å½“ä»»ä¸€å‚æ•°å˜åŒ–ï¼š
> 
> - æ–°è¯·æ±‚è‡ªç„¶ç”Ÿæˆæ–° keyï¼Œæ—§ç¼“å­˜è‡ªåŠ¨â€œé€»è¾‘å¤±æ•ˆâ€ã€‚
> - è‹¥å‚æ•°é¢‘ç¹å˜ï¼Œå¯ç›‘å¬å‚æ•°å˜æ›´äº‹ä»¶ï¼Œ**ä¸»åŠ¨æ¸…ç©ºç›¸å…³ç¼“å­˜**ï¼ˆæˆ–å…¨æ¸…ï¼‰ã€‚
> 
> > ğŸ”’ æ›´ä¸¥è°¨åšæ³•ï¼šç”¨ **LRU ç¼“å­˜ + å‚æ•°åŒ– key**ï¼Œé¿å…å†…å­˜æ³„æ¼ã€‚
> 
> ---
> 
> ### æ€»ç»“å»ºè®®
> 
> |é—®é¢˜|å»ºè®®æ–¹æ¡ˆ|
> |---|---|
> |**é‡è¯•**|æŒ‡æ•°é€€é¿ + jitterï¼Œä»…é‡è¯•ç½‘ç»œ/5xx/429ï¼Œæœ€å¤š 3 æ¬¡|
> |**å¤§æ–‡ä»¶å¤„ç†**|React å‰ç«¯åˆ†ç‰‡ + åç«¯æµå¼å¤„ç†ï¼Œé¿å…å…¨é‡åŠ è½½|
> |**å®¢æˆ·ç«¯ç¼“å­˜**|åŠ ï¼TTL 5~30sï¼Œcache key åŒ…å«æ‰€æœ‰æ¨¡å‹å‚æ•°|
> 
> è¿™æ ·æ—¢æå‡é²æ£’æ€§ï¼Œåˆä¼˜åŒ–ä½“éªŒï¼Œè¿˜å…¼é¡¾èµ„æºæ•ˆç‡ã€‚

éå¸¸å¥½ã€‚ä½ é€‰æ‹© **D ("æ¨¡å‹é¢„çƒ§åœ¨ Docker é•œåƒé‡Œ")**ã€‚

è¿™ä¸ªç­”æ¡ˆ**å¾ˆæœ‰è¶£**ï¼Œå› ä¸ºï¼š

- âœ… è¯´æ˜ä½ åœ¨æ€è€ƒ"å¯åŠ¨é€Ÿåº¦"è¿™ä¸ªå…³é”®æŒ‡æ ‡
- âš ï¸ ä½†ä¹Ÿæš—ç¤ºä½ å¯èƒ½**æ²¡æœ‰æƒè¡¡ Docker é•œåƒå¤§å°çš„ä»£ä»·**

è¿™æ­£å¥½è¯´æ˜**ä½ éœ€è¦é€šè¿‡ artifact æ¥ç†è§£"è¿™äº›è®¾è®¡çš„æƒè¡¡"**ã€‚

---

## ğŸš€ ç°åœ¨æˆ‘ç”Ÿæˆ DevOps artifact

åŸºäºä½ çš„æ‰€æœ‰ç­”æ¡ˆï¼Œæˆ‘ä¼šåˆ›å»ºä¸€ä¸ª**"ä»ç®€å•åˆ°å¤æ‚"çš„ DevOps å®Œæ•´æ–¹æ¡ˆ**ï¼Œæ¶µç›–ï¼š

1. **Dockerfile**ï¼ˆå¤šé˜¶æ®µæ„å»º + é•œåƒå¤§å°ä¼˜åŒ–ï¼‰
2. **K8s é…ç½®**ï¼ˆDeploymentã€Serviceã€HPAã€PVCï¼‰
3. **GitHub Actions**ï¼ˆCI/CD pipelineï¼‰
4. **æ•…éšœæ’æŸ¥æŒ‡å—**

**å…³é”®è®¾è®¡**ï¼šæ¯ä¸ªéƒ¨åˆ†éƒ½ä¼šç”¨**å†…è”é—®é¢˜**å¼•å¯¼ä½ æ€è€ƒ"ä¸ºä»€ä¹ˆè¿™æ ·"ã€‚

å‡†å¤‡å¥½äº†å—ï¼Ÿç›´æ¥ç”Ÿæˆï¼š
```yaml
============================================================================
# Google é£æ ¼ï¼šAI æœåŠ¡ DevOps å®Œå…¨æŒ‡å—
# æŠ€æœ¯æ ˆï¼šDocker + Kubernetes + GitHub Actions
#
# æ ¸å¿ƒç›®æ ‡ï¼š
# 1. ç”Ÿäº§çº§ Docker é•œåƒï¼ˆå°ã€å¿«ã€å®‰å…¨ï¼‰
# 2. K8s é«˜å¯ç”¨é…ç½®ï¼ˆå¤šå‰¯æœ¬ã€è‡ªåŠ¨æ‰©å®¹ã€å¥åº·æ£€æŸ¥ï¼‰
# 3. è‡ªåŠ¨åŒ– CI/CDï¼ˆæµ‹è¯• â†’ æ„å»º â†’ æ¨é€ â†’ éƒ¨ç½²ï¼‰
# 4. ç”Ÿäº§æ•…éšœæ’æŸ¥æŒ‡å—
#
# å…³é”®è®¾è®¡åŸåˆ™ï¼š
# - é•œåƒä¼˜åŒ–ï¼šå¤šé˜¶æ®µæ„å»º â†’ å‡å°‘ä¸å¿…è¦çš„ä¾èµ–
# - é«˜å¯ç”¨ï¼šReadinessProbe + LivenessProbe + HPA
# - æ¨¡å‹å­˜å‚¨ï¼šæƒè¡¡"é•œåƒå¤§å°"vs"å¯åŠ¨é€Ÿåº¦"
# - ç°åº¦éƒ¨ç½²ï¼šåˆ†æ‰¹ä¸Šçº¿ï¼Œå¿«é€Ÿå›æ»š
# ============================================================================


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šDockerfileï¼ˆå¤šé˜¶æ®µæ„å»ºï¼‰
# ============================================================================

# æ–‡ä»¶åï¼šDockerfile
# æ³¨æ„ï¼šè¿™æ˜¯å¤šé˜¶æ®µæ„å»ºï¼Œä¼šç”Ÿæˆä¸¤ä¸ªä¸­é—´é•œåƒï¼Œæœ€ç»ˆåªä¿ç•™è¿è¡Œæ—¶é•œåƒ

# --- é˜¶æ®µ 1ï¼šæ„å»ºé˜¶æ®µï¼ˆåŒ…å«æ‰€æœ‰ç¼–è¯‘å·¥å…·ï¼‰---
FROM python:3.10-slim as builder

WORKDIR /build

# å®‰è£…æ„å»ºä¾èµ–ï¼ˆåªåœ¨è¿™ä¸ªé˜¶æ®µéœ€è¦ï¼‰
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ requirements.txt
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–åˆ°è™šæ‹Ÿç¯å¢ƒ
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
RUN pip install --no-cache-dir -r requirements.txt

# é—®é¢˜ 1ï¼šä¸ºä»€ä¹ˆè¦ç”¨è™šæ‹Ÿç¯å¢ƒè€Œä¸æ˜¯ç›´æ¥ pip installï¼Ÿ
# ç­”æ¡ˆï¼šè¿™æ ·å¯ä»¥åœ¨ä¸‹ä¸€é˜¶æ®µ"åªå¤åˆ¶è™šæ‹Ÿç¯å¢ƒ"ï¼Œä¸éœ€è¦ pipã€ç¼–è¯‘å·¥å…·ç­‰
# ç»“æœï¼šæœ€ç»ˆé•œåƒä¼šå° 80-90%


# --- é˜¶æ®µ 2ï¼šæ¨¡å‹ä¸‹è½½é˜¶æ®µï¼ˆå¯é€‰ï¼Œç”¨äºé¢„çƒ­æ¨¡å‹ï¼‰---
FROM python:3.10-slim as model-downloader

WORKDIR /models

# åªå¤åˆ¶è™šæ‹Ÿç¯å¢ƒå’Œå¿…éœ€çš„åº“
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ï¼‰
# é—®é¢˜ 2ï¼šæ¨¡å‹ä¸‹è½½ååº”è¯¥å­˜åœ¨å“ªé‡Œï¼Ÿ
# - é€‰é¡¹ Aï¼šå­˜åœ¨é•œåƒé‡Œï¼ˆæœ¬ Dockerfile çš„æ–¹æ¡ˆï¼‰â†’ é•œåƒå¾ˆå¤§ï¼ˆ2-3GBï¼‰
# - é€‰é¡¹ Bï¼šå¯åŠ¨æ—¶ä¸‹è½½ï¼ˆInit Containerï¼‰â†’ å¯åŠ¨æ…¢ï¼ˆ10-30 ç§’ï¼‰
# - é€‰é¡¹ Cï¼šç”¨ PVC å…±äº«ï¼ˆå¤š Pod å…±äº«ï¼‰â†’ ç½‘ç»œ I/O æˆæœ¬
# ç­”æ¡ˆï¼šå–å†³äº"é•œåƒæ¨é€é¢‘ç‡" vs "Pod å¯åŠ¨é¢‘ç‡"
#       é«˜é¢‘éƒ¨ç½² â†’ é€‰ Bï¼ˆInit Containerï¼‰
#       ç¨³å®šè¿è¡Œ â†’ é€‰ Aï¼ˆé¢„çƒ§é•œåƒï¼‰æˆ– Cï¼ˆPVCï¼‰

RUN python -c "from transformers import AutoModel; AutoModel.from_pretrained('bert-base-uncased')"


# --- é˜¶æ®µ 3ï¼šè¿è¡Œæ—¶é˜¶æ®µï¼ˆæœ€ç»ˆé•œåƒï¼‰---
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–ï¼ˆä¸éœ€è¦ç¼–è¯‘å·¥å…·ï¼‰
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶è™šæ‹Ÿç¯å¢ƒï¼ˆä» builder é˜¶æ®µï¼‰
COPY --from=builder /opt/venv /opt/venv

# å¤åˆ¶æ¨¡å‹ï¼ˆä» model-downloader é˜¶æ®µï¼‰
COPY --from=model-downloader /models /models

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY app/ /app/

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PATH="/opt/venv/bin:$PATH"
ENV TRANSFORMERS_CACHE=/models
ENV PYTHONUNBUFFERED=1

# é root ç”¨æˆ·ï¼ˆå®‰å…¨æœ€ä½³å®è·µï¼‰
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]

# é—®é¢˜ 3ï¼šä¸ºä»€ä¹ˆ workers=1ï¼Ÿ
# ç­”æ¡ˆï¼šæ¨ç†æœåŠ¡ä¸­ï¼Œæ‰€æœ‰ worker å…±äº«åŒä¸€ä¸ª GPU æ¨¡å‹
#       å¦‚æœ workers > 1ï¼Œä¼šå¯¼è‡´å¤šä¸ªè¿›ç¨‹ç«äº‰ GPUï¼Œåè€Œå˜æ…¢
#       K8s çš„ HPA ä¼šé€šè¿‡"å¢åŠ  Pod"æ¥æ‰©å®¹ï¼Œè€Œä¸æ˜¯å¢åŠ  worker


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šKubernetes é…ç½®ï¼ˆK8s Deployment + Serviceï¼‰
# ============================================================================

# æ–‡ä»¶åï¼šk8s/namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: ai-service
  labels:
    name: ai-service

# é—®é¢˜ 4ï¼šä¸ºä»€ä¹ˆè¦ç”¨ Namespaceï¼Ÿ
# ç­”æ¡ˆï¼šéš”ç¦»èµ„æºï¼ˆPodã€Serviceã€ConfigMapï¼‰
#       ä¾¿äºæƒé™ç®¡ç†ã€èµ„æºé…é¢ã€æ—¥å¿—åˆ†ç±»
#       åœ¨å¤§å‹é›†ç¾¤ä¸­ï¼Œæ¯ä¸ªå›¢é˜Ÿ/é¡¹ç›®ä¸€ä¸ª Namespace


# æ–‡ä»¶åï¼šk8s/configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-service-config
  namespace: ai-service
data:
  model_name: "bert-base-uncased"
  max_length: "256"
  batch_size: "32"
  max_wait_ms: "100"
  redis_host: "redis-service"
  redis_port: "6379"

# é—®é¢˜ 5ï¼šä¸ºä»€ä¹ˆç”¨ ConfigMap è€Œä¸æ˜¯ç¡¬ç¼–ç åœ¨é•œåƒé‡Œï¼Ÿ
# ç­”æ¡ˆï¼šå¯ä»¥åœ¨ä¸é‡æ–°æ„å»ºé•œåƒçš„æƒ…å†µä¸‹æ”¹å˜é…ç½®
#       æ”¯æŒä¸åŒç¯å¢ƒï¼ˆdev/staging/prodï¼‰çš„ä¸åŒé…ç½®
#       ä¸é•œåƒåˆ†ç¦» â†’ æ›´çµæ´»çš„éƒ¨ç½²


# æ–‡ä»¶åï¼šk8s/secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: ai-service-secrets
  namespace: ai-service
type: Opaque
stringData:
  # è¿™äº›å€¼åº”è¯¥ç”¨ kubectl æˆ–å¯†é’¥ç®¡ç†å·¥å…·è®¾ç½®ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
  api_key: "your-api-key-here"
  database_url: "postgresql://user:pass@db-host/db"

# é—®é¢˜ 6ï¼šä¸ºä»€ä¹ˆæ•æ„Ÿä¿¡æ¯è¦ç”¨ Secret è€Œä¸æ˜¯ ConfigMapï¼Ÿ
# ç­”æ¡ˆï¼šConfigMap æ˜¯æ˜æ–‡å­˜å‚¨ï¼ŒSecret æ˜¯ base64 ç¼–ç ï¼ˆè™½ç„¶ä¸æ˜¯åŠ å¯†ï¼Œä½†æ›´å®‰å…¨ï¼‰
#       æ›´é‡è¦çš„æ˜¯ï¼šSecret æœ‰è®¿é—®æ§åˆ¶ï¼Œä¸ä¼šè¢« kubectl get cm æ„å¤–æš´éœ²


# æ–‡ä»¶åï¼šk8s/deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-inference-service
  namespace: ai-service
  labels:
    app: ai-inference-service
    version: v1
spec:
  # å‰¯æœ¬æ•°ï¼ˆåˆå§‹å€¼ï¼‰
  replicas: 3
  
  # ç­–ç•¥ï¼šRollingUpdateï¼ˆé€æ­¥æ›¿æ¢æ—§ Podï¼‰vs Recreateï¼ˆä¸€æ¬¡æ€§æ›¿æ¢ï¼‰
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # æœ€å¤šè¶…å‡º 1 ä¸ªå‰¯æœ¬ï¼ˆæ€»å…±æœ€å¤š 4 ä¸ª Podï¼‰
      maxSurge: 1
      # æœ€å°‘ä¿ç•™ 2 ä¸ªå‰¯æœ¬ï¼ˆæ€»å…±æœ€å°‘ 2 ä¸ª Podï¼‰
      maxUnavailable: 1
  
  # é—®é¢˜ 7ï¼šmaxSurge=1, maxUnavailable=1 æ„å‘³ç€ä»€ä¹ˆï¼Ÿ
  # ç­”æ¡ˆï¼šéƒ¨ç½²æœŸé—´ï¼ŒK8s ä¼šï¼š
  #       1. å¯åŠ¨ 1 ä¸ªæ–° Podï¼ˆæ€»å…± 4 ä¸ªï¼‰
  #       2. åˆ é™¤ 1 ä¸ªæ—§ Podï¼ˆæ€»å…± 3 ä¸ªï¼‰
  #       3. é‡å¤ç›´åˆ°å…¨éƒ¨æ›¿æ¢
  #       ç»“æœï¼šæœåŠ¡å§‹ç»ˆæœ‰ Pod åœ¨è¿è¡Œï¼Œç”¨æˆ·ä¸ä¼šæ–­è¿
  #       ä»£ä»·ï¼šéœ€è¦æ›´å¤šé›†ç¾¤èµ„æº
  
  selector:
    matchLabels:
      app: ai-inference-service
  
  template:
    metadata:
      labels:
        app: ai-inference-service
        version: v1
    
    spec:
      # å®¹å™¨å®šä¹‰
      containers:
      - name: inference-service
        image: ghcr.io/your-org/ai-inference-service:latest
        imagePullPolicy: Always
        
        ports:
        - name: http
          containerPort: 8000
        
        # ç¯å¢ƒå˜é‡ï¼ˆä» ConfigMap è¯»å–ï¼‰
        env:
        - name: MODEL_NAME
          valueFrom:
            configMapKeyRef:
              name: ai-service-config
              key: model_name
        - name: REDIS_HOST
          valueFrom:
            configMapKeyRef:
              name: ai-service-config
              key: redis_host
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-service-secrets
              key: api_key
        
        # èµ„æºé™åˆ¶ï¼ˆæœ€å…³é”®ï¼ï¼‰
        resources:
          requests:
            # Pod ä¿è¯è·å¾—è¿™ä¹ˆå¤šèµ„æº
            cpu: "2"
            memory: "4Gi"
            nvidia.com/gpu: "1"  # è¯·æ±‚ 1 å— GPU
          limits:
            # Pod æœ€å¤šå¯ä»¥ä½¿ç”¨è¿™ä¹ˆå¤šèµ„æº
            cpu: "4"
            memory: "8Gi"
            nvidia.com/gpu: "1"
        
        # é—®é¢˜ 8ï¼šä¸ºä»€ä¹ˆè¦è®¾ç½® requests å’Œ limitsï¼Ÿ
        # ç­”æ¡ˆï¼š
        # - requestsï¼šK8s ç”¨è¿™ä¸ªå†³å®š"èƒ½å¦è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹"
        #   å¦‚æœèŠ‚ç‚¹å¯ç”¨èµ„æº < requestsï¼Œå°±ä¸ä¼šè°ƒåº¦
        # - limitsï¼šå®¹å™¨è¶…è¿‡è¿™ä¸ªå€¼å°±ä¼šè¢«æ€æ‰ï¼ˆOOMKillï¼‰
        # ç»“æœï¼šé˜²æ­¢æŸä¸ª Pod å ç”¨è¿‡å¤šèµ„æºï¼Œå½±å“å…¶ä»–æœåŠ¡
        
        # å¯åŠ¨æ¢é’ˆï¼ˆPod å¯åŠ¨æ—¶çš„å¥åº·æ£€æŸ¥ï¼‰
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 30  # æœ€å¤šå°è¯• 30 æ¬¡ï¼ˆ30*5 = 150 ç§’ï¼‰
        
        # å°±ç»ªæ¢é’ˆï¼ˆPod æ˜¯å¦å¯ä»¥æ¥æ”¶æµé‡ï¼‰
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
          failureThreshold: 3
        
        # é—®é¢˜ 9ï¼šstartupProbe vs readinessProbe çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
        # ç­”æ¡ˆï¼š
        # - startupProbeï¼šåªåœ¨ Pod å¯åŠ¨æ—¶è¿è¡Œï¼Œç­‰å¾…åº”ç”¨çœŸæ­£å¯åŠ¨
        #   ï¼ˆå› ä¸ºæ¨¡å‹åŠ è½½å¯èƒ½å¾ˆæ…¢ï¼Œ30 ç§’ä»¥ä¸Šï¼‰
        # - readinessProbeï¼šæŒç»­è¿è¡Œï¼Œæ£€æŸ¥åº”ç”¨æ˜¯å¦"å‡†å¤‡å¥½å¤„ç†æµé‡"
        #   å¦‚æœå¤±è´¥ï¼ŒK8s ä¼šæš‚æ—¶ç§»é™¤è¯¥ Pod çš„æµé‡
        # è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ çš„"ç¼“å­˜ä¸€è‡´æ€§é—®é¢˜"ä¼šå‘ç”Ÿï¼š
        # readinessProbe å¤±è´¥æ—¶ï¼Œæµé‡ä¼šè½¬ç§»åˆ°å…¶ä»– Pod
        
        # å­˜æ´»æ¢é’ˆï¼ˆPod æ˜¯å¦è¿˜æ´»ç€ï¼‰
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          failureThreshold: 3
        
        # é—®é¢˜ 10ï¼šä¸ºä»€ä¹ˆè¦ç”¨ livenessProbeï¼Ÿ
        # ç­”æ¡ˆï¼šå¦‚æœåº”ç”¨å¡ä½ï¼ˆä½†å®¹å™¨è¿˜åœ¨è¿è¡Œï¼‰ï¼Œ
        #       readinessProbe å¯èƒ½è¿˜åœ¨è¿”å› 200ï¼Œ
        #       ä½†å®é™…ä¸Šæ— æ³•å¤„ç†è¯·æ±‚
        #       livenessProbe ä¼šæ£€æŸ¥åº”ç”¨"çœŸæ­£æ´»ç€"ï¼Œ
        #       å¦‚æœå¤±è´¥ï¼ŒK8s ä¼šé‡å¯å®¹å™¨
        
        # æŒ‚è½½å·ï¼ˆæŒä¹…åŒ–å­˜å‚¨ã€é…ç½®æ–‡ä»¶ç­‰ï¼‰
        volumeMounts:
        - name: model-cache
          mountPath: /models
        - name: logs
          mountPath: /var/log/app
      
      # å·å®šä¹‰
      volumes:
      - name: model-cache
        emptyDir: {}  # ä¸´æ—¶å­˜å‚¨ï¼ˆPod åˆ é™¤æ—¶æ¸…ç©ºï¼‰
      
      # é—®é¢˜ 11ï¼šemptyDir æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆä¸ç”¨ PVCï¼Ÿ
      # ç­”æ¡ˆï¼š
      # - emptyDirï¼šæ¯ä¸ª Pod ä¸€ä¸ªç‹¬ç«‹çš„ä¸´æ—¶å­˜å‚¨ï¼ˆåŒä¸€ Node ä¸Šçš„å®¹å™¨å¯ä»¥å…±äº«ï¼‰
      # - PVCï¼šå¤šä¸ª Pod å¯ä»¥å…±äº«ï¼ˆè·¨ Nodeï¼‰ï¼Œä½†ç½‘ç»œ I/O æˆæœ¬é«˜
      # å¯¹æ¨ç†æœåŠ¡ï¼šç”¨ emptyDir å°±å¤Ÿäº†ï¼Œå› ä¸ºæ¨¡å‹å·²ç»é¢„çƒ§åœ¨é•œåƒé‡Œ
      # ï¼ˆå¦‚æœé€‰æ‹©"å¯åŠ¨æ—¶ä¸‹è½½æ¨¡å‹"ï¼Œéœ€è¦ç”¨ Init Container + emptyDirï¼‰
      
      - name: logs
        emptyDir: {}
      
      # äº²å’Œæ€§ï¼ˆPod è°ƒåº¦ç­–ç•¥ï¼‰
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - ai-inference-service
              topologyKey: kubernetes.io/hostname
      
      # é—®é¢˜ 12ï¼špodAntiAffinity æ˜¯ä»€ä¹ˆï¼Ÿ
      # ç­”æ¡ˆï¼šå°½é‡è®©ç›¸åŒæ ‡ç­¾çš„ Pod è¿è¡Œåœ¨ä¸åŒ Node ä¸Š
      # ç»“æœï¼šå¦‚æœæŸä¸ª Node æ•…éšœï¼Œä¸ä¼šä¸€æ¬¡æ€§ä¸¢å¤±æ‰€æœ‰å‰¯æœ¬
      # æƒé‡ 100ï¼šè¿™æ˜¯"é¦–é€‰"ï¼Œä¸æ˜¯"å¿…é¡»"ï¼Œå¦‚æœé›†ç¾¤å¤ªå°ä¹Ÿå¯ä»¥è¿å


# æ–‡ä»¶åï¼šk8s/service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: ai-inference-service
  namespace: ai-service
spec:
  type: LoadBalancer
  
  # é—®é¢˜ 13ï¼šService ç±»å‹æœ‰å“ªäº›ï¼Ÿ
  # ç­”æ¡ˆï¼š
  # - ClusterIPï¼ˆé»˜è®¤ï¼‰ï¼šåªèƒ½åœ¨é›†ç¾¤å†…éƒ¨è®¿é—®
  # - NodePortï¼šé€šè¿‡ Node IP + Port è®¿é—®
  # - LoadBalancerï¼šäº‘å‚å•†æä¾›å¤–éƒ¨è´Ÿè½½å‡è¡¡
  # - ExternalNameï¼šæ˜ å°„åˆ°å¤–éƒ¨ DNS
  # æœ¬ä¾‹ç”¨ LoadBalancerï¼Œè¿™æ ·å¤–éƒ¨ç”¨æˆ·å¯ä»¥ç›´æ¥è®¿é—®
  
  selector:
    app: ai-inference-service
  
  ports:
  - port: 80  # Service ç›‘å¬çš„ç«¯å£
    targetPort: 8000  # Pod å®¹å™¨çš„ç«¯å£
    protocol: TCP
    name: http


# æ–‡ä»¶åï¼šk8s/hpa.yaml
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-inference-service-hpa
  namespace: ai-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-inference-service
  
  minReplicas: 3
  maxReplicas: 10
  
  metrics:
  # æŒ‡æ ‡ 1ï¼šCPU ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # CPU è¶…è¿‡ 70% å°±æ‰©å®¹
  
  # æŒ‡æ ‡ 2ï¼šå†…å­˜ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  # é—®é¢˜ 14ï¼šHPA å¦‚ä½•å·¥ä½œï¼Ÿ
  # ç­”æ¡ˆï¼š
  # 1. K8s æ¯ 15 ç§’é‡‡é›†ä¸€æ¬¡ Pod çš„ CPU/å†…å­˜
  # 2. è®¡ç®—å¹³å‡ä½¿ç”¨ç‡
  # 3. å¦‚æœå¹³å‡ CPU > 70%ï¼Œè®¡ç®—éœ€è¦å¤šå°‘å‰¯æœ¬ï¼š
  #    æ–°å‰¯æœ¬æ•° = ceil(å½“å‰ä½¿ç”¨ç‡ / ç›®æ ‡ä½¿ç”¨ç‡ * å½“å‰å‰¯æœ¬æ•°)
  #    ä¾‹ï¼š3 ä¸ª Podï¼Œä½¿ç”¨ç‡ 90%ï¼Œç›®æ ‡ 70%
  #    æ–°å‰¯æœ¬æ•° = ceil(90/70 * 3) = 4
  # 4. ç­‰å¾… 3 åˆ†é’Ÿè§‚å¯Ÿï¼ˆé˜²æ­¢é¢‘ç¹æ³¢åŠ¨ï¼‰
  # 5. å¦‚æœè¿˜æ˜¯è¶…è¿‡ 70%ï¼Œå†æ‰©å®¹
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # ç¼©å®¹å‰ç­‰å¾… 5 åˆ†é’Ÿ
      policies:
      - type: Percent
        value: 50  # ä¸€æ¬¡æœ€å¤šå‡å°‘ 50%ï¼ˆ3 ä¸ª Pod â†’ 1-2 ä¸ªï¼‰
        periodSeconds: 60
    
    scaleUp:
      stabilizationWindowSeconds: 0  # æ‰©å®¹ä¸ç­‰å¾…
      policies:
      - type: Percent
        value: 100  # ä¸€æ¬¡æœ€å¤šå¢åŠ  100%ï¼ˆ3 ä¸ª Pod â†’ 6 ä¸ªï¼‰
        periodSeconds: 60
  
  # é—®é¢˜ 15ï¼šä¸ºä»€ä¹ˆ scaleUp å’Œ scaleDown çš„ç­–ç•¥ä¸åŒï¼Ÿ
  # ç­”æ¡ˆï¼š
  # - scaleUpï¼šéœ€è¦å¿«é€Ÿå“åº”æµé‡çªå¢ï¼Œæ‰€ä»¥ç«‹å³æ‰©å®¹ï¼Œä¸”æ¿€è¿›ï¼ˆ100% å¢é•¿ï¼‰
  # - scaleDownï¼šç¼©å®¹å¯ä»¥æ…¢ä¸€ç‚¹ï¼Œé¿å…æµé‡åˆçªå¢æ—¶é‡å¤æ‰©ç¼©ï¼ˆæˆæœ¬ï¼‰
  # ç»“æœï¼šå¿«é€Ÿæ‰©å®¹ï¼Œç¼“æ…¢ç¼©å®¹ = èƒ½åº”å¯¹çªå‘æµé‡ï¼ŒåŒæ—¶é¿å…æµªè´¹


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šGitHub Actions CI/CD
# ============================================================================

# æ–‡ä»¶åï¼š.github/workflows/deploy.yml
---
name: CI/CD Pipeline

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/ai-inference-service

jobs:
  # ä»»åŠ¡ 1ï¼šæµ‹è¯•
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run tests
        run: pytest tests/ --cov=app --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
  
  # ä»»åŠ¡ 2ï¼šæ„å»º Docker é•œåƒ
  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache
          cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:buildcache,mode=max
      
      # é—®é¢˜ 16ï¼šä¸ºä»€ä¹ˆè¦ç”¨ç¼“å­˜ï¼ˆcache-from/cache-toï¼‰ï¼Ÿ
      # ç­”æ¡ˆï¼šDocker æ„å»ºå¯èƒ½å¾ˆæ…¢ï¼ˆ10-30 åˆ†é’Ÿï¼‰ï¼Œå°¤å…¶æ˜¯ä¸‹è½½æ¨¡å‹æ—¶
      #       ç¼“å­˜å¯ä»¥è®©ç¬¬äºŒæ¬¡æ„å»ºå¿« 90%ï¼ˆåªæœ‰å˜åŒ–çš„å±‚é‡æ–°æ„å»ºï¼‰
      #       æ¨é€ç¼“å­˜åˆ° registryï¼Œè¿™æ ·å…¶ä»–å¼€å‘è€…ä¹Ÿèƒ½ç”¨
  
  # ä»»åŠ¡ 3ï¼šéƒ¨ç½²åˆ° K8s
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
      
      - name: Configure kubectl
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > $HOME/.kube/config
      
      - name: Update K8s manifests
        run: |
          # æ›´æ–°é•œåƒç‰ˆæœ¬
          sed -i 's|IMAGE_TAG|${{ github.sha }}|g' k8s/deployment.yaml
      
      - name: Apply K8s manifests
        run: |
          kubectl apply -f k8s/namespace.yaml
          kubectl apply -f k8s/configmap.yaml
          kubectl apply -f k8s/secret.yaml
          kubectl apply -f k8s/deployment.yaml
          kubectl apply -f k8s/service.yaml
          kubectl apply -f k8s/hpa.yaml
      
      - name: Wait for rollout
        run: |
          kubectl rollout status deployment/ai-inference-service \
            -n ai-service \
            --timeout=5m
      
      - name: Verify health
        run: |
          kubectl get pods -n ai-service
          kubectl get svc -n ai-service
      
      # é—®é¢˜ 17ï¼šä¸ºä»€ä¹ˆè¦ç­‰å¾… rollout å®Œæˆï¼Ÿ
      # ç­”æ¡ˆï¼šç¡®ä¿æ–°ç‰ˆæœ¬çš„ Pod éƒ½å¯åŠ¨äº†å¹¶é€šè¿‡å¥åº·æ£€æŸ¥
      #       å¦‚æœå‡ºé—®é¢˜ï¼ŒGitHub Action ä¼šå¤±è´¥ï¼Œè§¦å‘å‘Šè­¦


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ•…éšœæ’æŸ¥æŒ‡å—
# ============================================================================

# å¸¸è§é—®é¢˜ä¸è¯Šæ–­å‘½ä»¤

# é—®é¢˜ 1ï¼šPod æ— æ³•å¯åŠ¨ï¼ˆCrashLoopBackOffï¼‰
# ç—‡çŠ¶ï¼šPod çŠ¶æ€æ˜¾ç¤º CrashLoopBackOff
# è¯Šæ–­ï¼š
# $ kubectl logs -n ai-service ai-inference-service-xxxx  # æŸ¥çœ‹æ—¥å¿—
# $ kubectl describe pod -n ai-service ai-inference-service-xxxx  # æŸ¥çœ‹äº‹ä»¶
# å¯èƒ½åŸå› ï¼š
# - é•œåƒæ‹‰å–å¤±è´¥ï¼ˆregistry å‡­è¯é”™è¯¯ï¼‰
# - åº”ç”¨å¯åŠ¨é”™è¯¯ï¼ˆä»£ç é—®é¢˜ï¼‰
# - èµ„æºä¸è¶³ï¼ˆGPU ä¸å¯ç”¨ï¼‰

# é—®é¢˜ 2ï¼šPod ä¸€ç›´å¤„äº Pending
# ç—‡çŠ¶ï¼šPod æ— æ³•è°ƒåº¦åˆ°ä»»ä½• Node
# è¯Šæ–­ï¼š
# $ kubectl describe node  # æ£€æŸ¥æ‰€æœ‰ Node çš„èµ„æº
# $ kubectl top nodes  # æŸ¥çœ‹ Node çš„ CPU/å†…å­˜ä½¿ç”¨ç‡
# $ kubectl describe pod -n ai-service ai-inference-service-xxxx  # æŸ¥çœ‹ Events
# å¯èƒ½åŸå› ï¼š
# - é›†ç¾¤æ²¡æœ‰è¶³å¤Ÿçš„èµ„æºï¼ˆGPU ä¸å¤Ÿï¼‰
# - Pod çš„ requests å¤ªé«˜
# - Node æ ‡ç­¾ä¸åŒ¹é…

# é—®é¢˜ 3ï¼šHPA ä¸è‡ªåŠ¨æ‰©å®¹
# ç—‡çŠ¶ï¼šå³ä½¿ CPU å¾ˆé«˜ï¼Œå‰¯æœ¬æ•°ä¹Ÿä¸å¢åŠ 
# è¯Šæ–­ï¼š
# $ kubectl get hpa -n ai-service  # æŸ¥çœ‹ HPA çŠ¶æ€
# $ kubectl describe hpa -n ai-service ai-inference-service-hpa
# $ kubectl top pods -n ai-service  # æŸ¥çœ‹ Pod çš„ CPU/å†…å­˜
# å¯èƒ½åŸå› ï¼š
# - metrics-server æœªå®‰è£…ï¼ˆK8s æ— æ³•é‡‡é›†æŒ‡æ ‡ï¼‰
# - Pod æ²¡æœ‰è®¾ç½® requestsï¼ˆæ— æ³•è®¡ç®—ç™¾åˆ†æ¯”ï¼‰
# - æŒ‡æ ‡æ”¶é›†å¤±è´¥

# é—®é¢˜ 4ï¼šæ¨ç†æœåŠ¡å“åº”æ…¢
# ç—‡çŠ¶ï¼š/predict æ¥å£å»¶è¿Ÿå¾ˆé«˜ï¼ˆ> 1 ç§’ï¼‰
# è¯Šæ–­ï¼š
# $ kubectl logs -n ai-service ai-inference-service-xxxx | grep "latency"
# $ kubectl exec -it -n ai-service ai-inference-service-xxxx -- \
#     python -c "import torch; print(torch.cuda.is_available())"
# å¯èƒ½åŸå› ï¼š
# - Pod åœ¨ CPU ä¸Šè¿è¡Œï¼ˆä¸æ˜¯ GPUï¼‰â†’ æ£€æŸ¥ GPU è°ƒåº¦
# - æ‰¹å¤„ç†é˜Ÿåˆ—å †ç§¯ â†’ æ£€æŸ¥ HPA æ˜¯å¦æ‰©å®¹
# - ç¼“å­˜æœªå‘½ä¸­ â†’ æ£€æŸ¥ Redis è¿æ¥

# é—®é¢˜ 5ï¼šPod è¢«é¢‘ç¹é‡å¯
# ç—‡çŠ¶ï¼šPod çš„ RESTARTS è®¡æ•°æŒç»­å¢åŠ 
# è¯Šæ–­ï¼š
# $ kubectl get pods -n ai-service  # æŸ¥çœ‹ RESTARTS åˆ—
# $ kubectl logs -n ai-service ai-inference-service-xxxx --previous
# $ kubectl describe pod -n ai-service ai-inference-service-xxxx
# å¯èƒ½åŸå› ï¼š
# - livenessProbe å¤±è´¥ â†’ åº”ç”¨å¡ä½
# - OOMKill â†’ å†…å­˜æº¢å‡º
# - åº”ç”¨é”™è¯¯ â†’ æŸ¥çœ‹æ—¥å¿—

# å¸¸ç”¨ K8s å‘½ä»¤
# kubectl get pods -n ai-service  # æŸ¥çœ‹ Pod
# kubectl get svc -n ai-service  # æŸ¥çœ‹ Service
# kubectl get hpa -n ai-service  # æŸ¥çœ‹ HPA
# kubectl logs -n ai-service <pod-name>  # æŸ¥çœ‹æ—¥å¿—
# kubectl exec -it -n ai-service <pod-name> -- /bin/bash  # è¿›å…¥å®¹å™¨
# kubectl port-forward -n ai-service svc/ai-inference-service 8000:80  # ç«¯å£è½¬å‘
# kubectl rollout undo deployment/ai-inference-service -n ai-service  # å›æ»š
```
å®Œæˆäº† **ç¬¬äº”ä¸ª artifactï¼šDevOps å®Œå…¨æŒ‡å—**ï¼ˆ~600 è¡Œä»£ç  + è¯¦ç»†æ³¨é‡Šï¼‰ã€‚

---

## ğŸ“Š è¿™ä¸ª artifact åŒ…å«çš„å…³é”®è®¾è®¡å†³ç­–ï¼š

|è®¾è®¡|ä¸ºä»€ä¹ˆè¿™æ ·|å…³é”®é—®é¢˜ç¼–å·|
|---|---|---|
|**å¤šé˜¶æ®µæ„å»º**|å‡å°‘é•œåƒå¤§å° 80-90%|Q1-Q2|
|**æ¨¡å‹é¢„çƒ§**|æƒè¡¡ï¼šé•œåƒå¤§ä½†å¯åŠ¨å¿« vs é•œåƒå°ä½†å¯åŠ¨æ…¢|Q2|
|**3 å‰¯æœ¬ + HPA**|é«˜å¯ç”¨ + è‡ªåŠ¨æ‰©å®¹|Q7-Q15|
|**Probe ä¸‰å‰‘å®¢**|å¯åŠ¨å¿«é€Ÿ + å°±ç»ªæ£€æµ‹ + å­˜æ´»æ£€æŸ¥|Q9-Q10|
|**äº²å’Œæ€§è°ƒåº¦**|å¤š Node åˆ†å¸ƒï¼Œå®¹é”™èƒ½åŠ›å¼º|Q12|
|**GitHub Actions**|è‡ªåŠ¨åŒ–æµ‹è¯• â†’ æ„å»º â†’ æ¨é€ â†’ éƒ¨ç½²|Q16-Q17|

---

## ğŸ¯ ç°åœ¨ç”¨ä½ çš„ userStyle é—®ä½ **æœ€åçš„æ·±å±‚é—®é¢˜**ï¼š

### å…³é”®é—®é¢˜ï¼šä½ ç°åœ¨å¯¹"é‚£ä¸ªç¼“å­˜ä¸€è‡´æ€§é—®é¢˜"çš„ç†è§£

è¿˜è®°å¾—å—ï¼Ÿä½ ä¹‹å‰é€‰çš„æ˜¯ **C**ï¼ˆ"Pod å®¹å™¨é‡å¯åç¼“å­˜è¢«æ¸…ç©º"ï¼‰ã€‚

ç°åœ¨ï¼Œç»“åˆè¿™ä¸ª DevOps artifactï¼Œæˆ‘é—®ä½ ï¼š

**å‡è®¾ç°åœ¨æœ‰ 3 ä¸ª Pod åœ¨è¿è¡Œï¼ŒæŸä¸ª Pod çš„å†…å­˜ä½¿ç”¨ç‡çªç„¶å¾ˆé«˜ï¼ˆæ¥è¿‘ limitï¼‰ã€‚**

æ ¹æ®è¿™ä¸ª artifact çš„é…ç½®ï¼Œ**ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ** æŒ‰é¡ºåºè¯´ï¼š

1. _____ (æ¯”å¦‚ï¼šlivenessProbe å¤±è´¥ï¼Ÿ)
2. ---
    
3. ---
    

**æœ€ç»ˆç»“æœæ˜¯ä»€ä¹ˆï¼Ÿç”¨æˆ·ä¼šæ„Ÿå—åˆ°ä»€ä¹ˆï¼Ÿ**

---

è¿™ä¸ªé—®é¢˜ä¼šå‘Šè¯‰æˆ‘ï¼Œä½ æ˜¯å¦çœŸæ­£**ç†è§£äº† K8s çš„æ•…éšœé“¾ååº”**ã€‚

ç°åœ¨å›ç­” ğŸ‘‡
