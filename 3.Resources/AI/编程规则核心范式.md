---
view-count: 5
---
## ★ 编程规则核心范式

- ★ 面向AI可理解性编程→未来工具可消费  
- ★ 代码即知识图谱→而非仅可执行逻辑  
- ★ 数据中心设计→数据=一等公民  
- ❗ 为10年后AI重构优化而设计  

---

## ★ 规则1：上下文清晰性

- ★ 显式记录why→而非仅what  
- ★ 代码=叙事→AI+人可解读  
- △ 设计决策文档化→知识图谱化  
- △ AI模型架构→需详述选择理由  
- △ 业务逻辑→绑定预期结果  
- ❗ 文档腐化风险 vs 零文档黑盒  
- △ 前瞻：AI工具自动分析+重构+审计  

---

## ★ 规则2：模块化与抽象

- ★ 高内聚低耦合→AI易操作单元  
- ★ 强接口契约→最小依赖  
- △ 独立开发测试部署→快速实验  
- △ AI agent提议优化实现→无需理解全局  
- ❗ 过度抽象→样板代码↑+初始理解↓  
- △ 前瞻：AI驱动代码生成+自愈系统  

---

## ★ 规则3：可测试性+可观测性

- ★ 自动化测试→快速反馈循环  
- ★ 监控日志→AI系统行为诊断  
- △ 测试套件→支持激进重构  
- △ 可观测性→模型漂移+性能退化检测  
- △ 日志数据→训练AI自诊断模型  
- ❗ 测试成本 vs 快速迭代压力  
- ❗ 过度日志→噪音↑+存储成本↑  
- △ 前瞻：AI自诊断+自修复系统  

---

## ★ 规则4：数据中心设计

- ★ 数据schema+版本化+验证  
- ★ 数据与模型独立演化  
- △ 确保可重现性→系统化实验  
- △ 数据治理→完整性+隐私合规  
- ❗ 管理复杂度 vs 数据质量收益  
- △ 前瞻：AI建议数据增强策略+偏差识别  
- △ 数据中心AI→从算法优化→全系统优化  

---

## △ 跨规则协同效应

- ★ 上下文清晰+模块化→AI工具可拆解理解  
- ★ 可测试+可观测→AI自愈闭环数据源  
- ★ 数据中心+上下文→数据血缘可追溯  
- △ 模块化+可测试→快速AB实验框架  
- △ 可观测+数据中心→实时质量监控  

---

## ★ 与前述架构模式类比

- ★ 上下文清晰 ≈ Prompt结构化+why记录  
- ★ 模块化 ≈ tRPC类型契约+RSC分层  
- ★ 可测试性 ≈ tRPC端到端类型安全  
- ★ 数据中心 ≈ URL状态+服务端数据获取  
- △ AI可理解性 ≈ Tool-Wrapper标准化  
- △ 可观测性 ≈ Web Vitals监控驱动  

---

## ❗ 核心权衡张力

- ❗ 文档详尽度 vs 维护成本  
- ❗ 抽象层级 vs 初始理解负担  
- ❗ 测试覆盖 vs 开发速度  
- ❗ 数据治理 vs 系统复杂度  
- △ 当前团队效率 vs 未来AI工具收益  

---

## △ 可迁移设计模式

- △ 接口契约优先→支持AI生成实现  
- △ 分层架构→独立优化+替换  
- △ 日志结构化→机器可解析  
- △ Schema驱动开发→数据+代码双向生成  
- △ 行为文档化→AI训练数据源  

---

## ★ 前瞻洞察本质

- ★ 编码→为AI可消费而设计  
- ★ 代码库→训练未来AI工具的数据集  
- ★ 可观测性→AI自愈系统基础设施  
- ★ 数据中心→AI时代架构新范式  
- ❗ 从"人类可读"→"人类+AI可理解"  
- △ 类比Prompt工程：代码即AI指令集  

---

## △ 实施建议压缩

- △ 关键决策→why文档强制检查  
- △ 模块接口→类型+契约显式定义  
- △ CI集成→测试+可观测性门禁  
- △ 数据变更→schema版本+迁移脚本  
- △ 知识库→设计模式+踩坑记录结构化  

---

## ★ 终极目标

- ★ 代码=可执行知识库  
- ★ 系统=AI可理解+可操作+可演化  
- ★ 开发=人机协作而非纯人工  
- ❗ 今日投资→明日AI工具效率倍增器
---


## 1. Contextual Clarity → 让“为什么”变成机器可消费的资产

### 最直接的痛点
在 LLM/agent 项目里，最难维护的其实是“为什么这么连工具”“为什么这个 prompt 这么写”。一旦上下文丢了，后人（包括未来的 AI 工具）只能靠猜。

### 我会做的具体集成方式

**1）为每个模块/服务强制一个 5 行内的“Why Header”**

放在文件顶或类顶，固定结构，方便人和 AI 解析：

```ts
/**
 * Purpose: 在线广告出价策略服务
 * Why now: 支持新竞价策略 A/B 实验
 * Key tradeoffs: 牺牲少量可解释性换实时性
 * Invariants: 单次出价延迟 < 50ms；不写入外部状态
 * Owner: ads-ml-team
 */
```

- 优点：统一格式，未来 AI 工具可以批量抽取成“系统设计知识图谱”  
- 实施方式：PR 模板 + lint 规则（无 header 不准 merge）

**2）为重大架构/算法决策写轻量 ADR（Architecture Decision Record）**

不搞长文档，只用精简模板：

- Context  
- Decision  
- Alternatives considered  
- Consequences  

挂在 `docs/adr/NNN-title.md`；PR 必须链接对应 ADR。

**3）把 Prompt 视为“可解析工件”，而不是散落字符串**

- 所有 prompt 模板采用统一 schema（例如 YAML/JSON + 注释）：

```yaml
name: classify_support_ticket
intent: route tickets to correct queue
constraints:
  - never guess product if confidence < 0.6
failure_modes:
  - hallucinated product names
```

- 这让后续 AI 分析、批量重构 prompt 成为可能，而不是把 prompt 当纯字符串处理。

---

## 2. Modularity & Abstraction → 为“AI 参与改代码”预留接口

### 痛点
多 agent / 多工具系统，一旦早期写成一坨“大 orchestrator + 一堆 if/else”，后续没人敢动，更别提“让 AI 自动插一个新工具”。

### 我会做的具体集成方式

**1）把所有外部行为抽象成统一的 Tool 接口**

例如：

```ts
interface Tool<Input, Output> {
  name: string;
  description: string;
  schema: ZodSchema<Input>;
  execute(input: Input, ctx: ToolContext): Promise<Output>;
}
```

- 所有 HTTP 调用 / DB 操作 / 第三方 API 都实现为 Tool 实例  
- Agent 只和 `Tool` 打交道，不认识具体实现

好处：  
- 未来 AI 只要理解 Tool 列表+schema，就能计划/重排调用，而不用读一整套业务代码

**2）把 Agent 拆成微组件，而不是一个“大脑类”**

强制结构：

- `Planner`：计划步骤，产出“使用哪些 Tool / 顺序”  
- `Executor`：执行计划，调 LLM / Tool  
- `Critic / Reflector`：评估结果，决定是否重试/升级

每个都是独立模块，接口稳定，内部可替换。

**3）接口优先开发（Interface-first Dev）**

工作流调整：

- 在写实现前，先在 PR 写出接口定义（TypeScript types + Tool/Agent interface）  
- 同事 Review 的重点：边界是否合理、数据流是否清晰  
- 等接口稳定后，再填实现 + 测试

这样为未来“AI 自动生成实现”打好基础。

---

## 3. Testability & Observability → 没有 Evals，就不要说“Agent 上线了”

在 LLM 系统里，“没测试”会呈指数级放大：因为输出空间太大，人肉 spot-check 没有意义。

### 我会做的具体集成方式

**1）把“Eval 套件”当成一等公民，和单元测试同级**

- 每个 Agent / Prompt 都要有一套结构化的 eval case：
  - 输入（用户请求 + 上下文）
  - 期望行为（可以是标签、约束、评分函数）
- 运行方式：
  - 本地：开发时快速跑一个小子集  
  - CI：全量或抽样跑一批关键 case  

技术上可以：

- 用 JSON/YAML 存 eval cases  
- 写一个统一 runner，支持：
  - 调用当前 prompt/agent  
  - 对比预期输出（或用 scorer 评估）

**2）日志结构化，而不是“console.log 一堆文本”**

为 LLM/Agent 专门定义日志 schema：

```json
{
  "timestamp": "...",
  "request_id": "...",
  "agent": "support_router",
  "step": "tool_call",
  "prompt_name": "classify_issue",
  "tool": "lookup_customer_profile",
  "input": {...},
  "output": {...},
  "latency_ms": 120,
  "error": null
}
```

- 强制：所有 agent 步骤都通过统一 logger 打点  
- 后续：
  - 可以按 agent/step 做聚类看失败模式  
  - 为 AI 诊断工具提供干净的训练数据

**3）把“测试性”内嵌到接口设计中**

例如：

- Tool interface 要求 deterministic 模式  
- 每个 Tool 必须支持 mock/fake 实现 → 便于离线测试 Agent 逻辑  
- Agent 逻辑和 LLM 调用分开：可以在测试里替换成“固定响应 mock LLM”

---

## 4. Data-Centric Design → 给未来自己/AI 留一条可重跑的路

在多模型、多版本、多数据源的环境下，**你记住的都不算，系统记住的才算**。

### 我会做的具体集成方式

**1）数据版本化成为流程必需品**

- 为关键数据集（训练数据、对话日志子集、工具调用样本）强制：
  - 明确 schema（JSON Schema / Protobuf / BigQuery schema）
  - 版本号 + 生成管道（脚本 / pipeline 定义）
- 新实验 = 新数据版本 + 新代码版本 → 可回溯

**2）“数据契约”（Data Contract）化**

对不同团队/服务之间的数据交换：

- 明确定义：
  - 字段含义
  - 可空性
  - 允许的值域/范围
- 用验证器（如 Zod/JSON Schema）在入口强校验  
  → 否则 LLM 很容易被脏数据搞出诡异行为

**3）让 Agent/LLM 也能看到“数据质量信号”**

在上下文中显式传：

- 数据新鲜度（小时/天）  
- 置信度/缺失率  
- 来源（人为标注/自动标注/第三方）

这让后续“自我诊断的 Agent”可以结合这些信号做更稳健的推理（例如：数据太旧→建议人工确认）。

---

## 怎么嵌入到团队日常，而不是贴在墙上的“价值观”

如果要在一个实际团队里推行，我会这样落地：

1. **PR 模板升级**  
   - 新模块必须包含：
     - Why Header（Rule 1）  
     - 对已有接口的影响（Rule 2）  
     - 覆盖的测试/Evals 链接（Rule 3）  
     - 若涉及数据，列出数据源+schema（Rule 4）

2. **每周一次“Agent 事后复盘”**  
   - 只看：
     - 失败样例（按日志聚合）  
     - 哪些失败是：Prompt 设计问题 / 工具边界不清 / 数据问题  
   - 输出：
     - 1 条 ADR  
     - 1 个 Prompt 模板改动  
     - 1 个 eval case 新增  

3. **建立一个“Prompt & Agent Patterns” 小仓库**  
   - 不放业务代码，只放：
     - Prompt 模板（带 intent/constraints/失败模式）  
     - Agent 结构的示例（Planner/Executor/Wrapper 模式）  
     - 对应 eval 套件样例  
   - 这给新成员和未来 AI 工具一个“起跑线”
