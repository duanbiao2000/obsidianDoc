{
	"nodes":[
		{"id":"53a56d26246f96ce","x":-140,"y":-300,"width":625,"height":370,"type":"text","text":"The user wants to understand the concept, scope, and application areas of web crawlers.\nI need to:\n1.  Generate search queries in English to find information about web crawlers (also known as web spiders or bots).\n2.  Use the search results to explain the concept (what it is), scope (what it covers/its capabilities and limitations), and applications (where it's used).\n3.  Present the final answer in Chinese, based on the information gathered from English sources.\n\n**Search Queries:**\n*   What is a web crawler definition?\n*   web crawler concept scope applications\n*   web crawler limitations\n*   web crawler examples uses\n\nLet's execute the search."},
		{"id":"5441d2f1259498b7","x":-140,"y":-420,"width":625,"height":100,"type":"text","text":"主题网络爬虫的概念,范畴和应用领域\n\n只参考英文资源,并用中文输出\n"},
		{"id":"5aa08739d824b6b0","x":-140,"y":120,"width":625,"height":600,"type":"file","file":"2.Sphere/Coding/网络爬虫.md"},
		{"id":"626a9b9edcd676d1","x":-140,"y":740,"width":625,"height":260,"type":"text","text":"好的，我们可以使用 Playwright（一个强大的浏览器自动化库，类似于 Puppeteer，但提供跨语言支持，包括 Python）来实现一个聚焦于“AI 大模型领域最新业界动态”的网络爬虫。\n\n这个聚焦爬虫的特点是：\n\n1.  **目标明确:** 只关心与 AI 大模型相关的最新新闻、博客文章或研究发布。\n2.  **选择性访问:** 从一组与 AI 相关的种子 URL 开始，然后只跟踪那些看起来与主题相关的链接。\n3.  **内容筛选:** 在访问页面后，会检查页面内容（如标题、摘要、URL）是否包含相关关键词，以判断是否为目标信息。\n\n**请注意:**\n\n*   网络爬虫需要遵守目标网站的 `robots.txt` 规则和服务条款。\n*   频繁或不友好的爬取可能会给目标服务器带来负担，甚至导致 IP 被封禁。请设置适当的延迟。\n*   网页结构经常变化，爬虫代码（尤其是 CSS 选择器）可能需要定期维护。\n*   以下代码是一个**基本示例**，用于演示核心逻辑。实际应用中需要更健壮的错误处理、更复杂的链接过滤逻辑、更完善的数据存储和更精细的配置。\n*   你需要先安装 Playwright 及其浏览器驱动：\n    ```bash\n    pip install playwright\n    playwright install\n    ```\n\n**Python 代码示例 (使用 Playwright):**\n\n```python\nimport asyncio\nimport re\nfrom playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\nfrom urllib.parse import urljoin, urlparse\nfrom collections import deque\nimport logging\nfrom datetime import datetime\n\n# --- 配置 ---\n# 种子 URL：从这些高质量的 AI 新闻/博客源开始\nSEED_URLS = [\n    \"https://techcrunch.com/category/artificial-intelligence/\",\n    \"https://www.theverge.com/tech/ai-artificial-intelligence\",\n    \"https://openai.com/blog/\",\n    \"https://deepmind.google/blog/\", # Google DeepMind Blog\n    \"https://venturebeat.com/category/ai/\",\n    # 可以添加更多相关源, 如 Hugging Face Blog, Anthropic News, etc.\n]\n\n# 聚焦关键词：用于判断页面内容或链接是否相关（不区分大小写）\nFOCUS_KEYWORDS = [\n    \"large language model\", \"llm\", \"gpt-4\", \"gpt-5\", \"openai\", \"anthropic\",\n    \"claude\", \"gemini\", \"transformer\", \"generative ai\", \"ai regulation\",\n    \"deep learning\", \"machine learning\", \"neural network\", \"ai safety\",\n    \"google ai\", \"meta ai\", \"foundation model\", \"ai chip\", \"nvidia ai\",\n    \"ai research\", \"artificial intelligence\"\n]\n\n# 爬虫限制\nMAX_PAGES_TO_VISIT = 50  # 最多访问多少个页面\nREQUEST_DELAY = 2      # 每个请求之间的延迟（秒），保持礼貌\nCONNECTION_TIMEOUT = 30000 # 页面加载超时时间 (毫秒)\n\n# --- 日志设置 ---\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# --- 辅助函数 ---\ndef is_relevant(text, keywords):\n    \"\"\"检查文本是否包含任何关键词 (忽略大小写)\"\"\"\n    if not text:\n        return False\n    text_lower = text.lower()\n    return any(keyword.lower() in text_lower for keyword in keywords)\n\ndef normalize_url(url):\n    \"\"\"规范化 URL，移除片段标识符等\"\"\"\n    parsed = urlparse(url)\n    return parsed._replace(fragment=\"\").geturl()\n\n# --- 核心爬虫逻辑 ---\nasync def focused_crawler(seed_urls, keywords, max_pages, delay, timeout):\n    \"\"\"\n    使用 Playwright 的聚焦爬虫函数\n    \"\"\"\n    visited_urls = set()\n    urls_to_visit = deque(normalize_url(url) for url in seed_urls)\n    relevant_articles = []\n    pages_visited_count = 0\n\n    async with async_playwright() as p:\n        # 推荐使用 chromium，但也可以换成 firefox 或 webkit\n        browser = await p.chromium.launch(headless=True) # headless=False 可以看到浏览器界面\n        context = await browser.new_context(\n            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 FocusedCrawlerBot/1.0\"\n            # 可以设置代理、cookies 等\n        )\n        page = await context.new_page()\n        page.set_default_timeout(timeout) # 设置页面操作超时\n\n        while urls_to_visit and pages_visited_count < max_pages:\n            current_url = urls_to_visit.popleft()\n\n            if current_url in visited_urls:\n                continue\n\n            logging.info(f\"Visiting ({pages_visited_count + 1}/{max_pages}): {current_url}\")\n            visited_urls.add(current_url)\n            pages_visited_count += 1\n\n            try:\n                await page.goto(current_url, wait_until='domcontentloaded') # 等待 DOM 加载完成\n\n                # 1. 检查当前页面内容是否相关\n                page_title = await page.title()\n                page_content_snippet = await page.locator('body').inner_text(timeout=5000) # 获取部分文本内容\n                page_relevant = is_relevant(page_title, keywords) or is_relevant(page_content_snippet[:1000], keywords) # 检查标题和内容开头\n\n                if page_relevant:\n                    logging.info(f\"  -> Relevant content found: '{page_title}'\")\n                    # 尝试提取发布日期 (需要针对不同网站定制选择器)\n                    publish_date = None\n                    # 示例: 尝试常见的 meta 标签 (不一定存在或准确)\n                    try:\n                         date_meta = await page.locator('meta[property=\"article:published_time\"]').get_attribute('content')\n                         if date_meta:\n                             publish_date = date_meta.split('T')[0]\n                    except Exception:\n                        pass # 忽略提取失败\n\n                    article_data = {\n                        \"title\": page_title,\n                        \"url\": current_url,\n                        \"fetch_time\": datetime.now().isoformat(),\n                        \"publish_date\": publish_date if publish_date else \"N/A\",\n                        # 可以进一步提取摘要等，但这需要更复杂的、针对网站的选择器\n                        # \"summary\": await page.locator('meta[name=\"description\"]').get_attribute('content')\n                    }\n                    relevant_articles.append(article_data)\n                    logging.debug(f\"  -> Collected: {article_data}\")\n\n\n                # 2. 查找页面上的链接，并筛选出可能相关的链接加入队列\n                links = await page.locator('a[href]').evaluate_all('''\n                    (links) => links.map(a => ({href: a.href, text: a.innerText}))\n                ''')\n\n                current_domain = urlparse(current_url).netloc\n\n                for link_info in links:\n                    href = link_info.get('href')\n                    link_text = link_info.get('text', '')\n\n                    if not href or href.startswith('#') or href.startswith('javascript:') or href.startswith('mailto:'):\n                        continue\n\n                    # 将相对 URL 转换为绝对 URL\n                    absolute_url = normalize_url(urljoin(current_url, href))\n                    parsed_absolute_url = urlparse(absolute_url)\n\n                    # 基本过滤：确保是 http/https 协议，且未访问过\n                    if parsed_absolute_url.scheme not in ['http', 'https'] or absolute_url in visited_urls or absolute_url in urls_to_visit:\n                        continue\n\n                    # 聚焦过滤：\n                    # a) 链接文本是否相关?\n                    link_text_relevant = is_relevant(link_text, keywords)\n                    # b) URL本身是否包含关键词 (比如 /blog/llm-research)?\n                    url_path_relevant = is_relevant(parsed_absolute_url.path, keywords)\n                    # c) (可选) 是否限制在初始种子域或相关域内? (这里暂时不加，允许跨域探索相关链接)\n                    # is_same_domain = parsed_absolute_url.netloc == current_domain\n\n                    # 只有当链接文本或 URL 路径看起来相关时，才加入待访问队列\n                    if link_text_relevant or url_path_relevant:\n                         # 避免队列过长\n                        if len(urls_to_visit) < max_pages * 2: # 限制队列大小\n                            logging.debug(f\"  -> Adding relevant link to queue: {absolute_url}\")\n                            urls_to_visit.append(absolute_url)\n\n\n            except PlaywrightTimeoutError:\n                logging.warning(f\"Timeout error visiting {current_url}\")\n            except Exception as e:\n                logging.error(f\"Error processing {current_url}: {e}\")\n            finally:\n                # 在每次请求后添加延迟\n                logging.debug(f\"Waiting for {delay} seconds before next request...\")\n                await asyncio.sleep(delay)\n\n        await browser.close()\n        logging.info(f\"Crawler finished. Visited {pages_visited_count} pages. Found {len(relevant_articles)} relevant articles.\")\n        return relevant_articles\n\n# --- 主程序入口 ---\nasync def main():\n    logging.info(\"Starting focused crawler for AI Large Model News...\")\n    results = await focused_crawler(\n        seed_urls=SEED_URLS,\n        keywords=FOCUS_KEYWORDS,\n        max_pages=MAX_PAGES_TO_VISIT,\n        delay=REQUEST_DELAY,\n        timeout=CONNECTION_TIMEOUT\n    )\n\n    print(\"\\n--- Collected Relevant Articles ---\")\n    if results:\n        # 可以按抓取时间或尝试解析的发布日期排序\n        # sorted_results = sorted(results, key=lambda x: x.get('publish_date') or x['fetch_time'], reverse=True)\n        sorted_results = sorted(results, key=lambda x: x['fetch_time'], reverse=True) # 按抓取时间降序\n\n        for i, article in enumerate(sorted_results):\n            print(f\"{i+1}. Title: {article['title']}\")\n            print(f\"   URL: {article['url']}\")\n            print(f\"   Published: {article['publish_date']}\")\n            print(f\"   Fetched: {article['fetch_time']}\")\n            print(\"-\" * 10)\n    else:\n        print(\"No relevant articles found matching the criteria.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n**代码解释:**\n\n1.  **配置:** 设置起始 URL (`SEED_URLS`)、核心关键词 (`FOCUS_KEYWORDS`)、爬取限制 (`MAX_PAGES_TO_VISIT`, `REQUEST_DELAY`, `CONNECTION_TIMEOUT`)。\n2.  **日志:** 使用 `logging` 模块记录爬虫的运行过程和潜在问题。\n3.  **`is_relevant` 函数:** 一个简单的辅助函数，用于检查给定文本（如标题、链接文本、内容片段）是否包含列表中的任何关键词。\n4.  **`normalize_url` 函数:** 清理 URL，例如移除 `#fragment` 部分，以避免重复访问本质上相同的页面。\n5.  **`focused_crawler` 函数:**\n    *   初始化 Playwright、浏览器和页面。设置了 User-Agent 模拟真实浏览器，并设置了超时。\n    *   使用 `deque` (双端队列) 作为待访问 URL 列表，`set` 存储已访问 URL 以提高查找效率。\n    *   **主循环:** 当待访问队列不为空且未达到最大页面限制时执行。\n    *   **获取与访问:** 从队列取出 URL，检查是否已访问，然后使用 `page.goto()` 访问。`wait_until='domcontentloaded'` 通常比等待网络完全空闲 (`networkidle`) 更快。\n    *   **内容相关性检查:** 获取页面标题 (`page.title()`) 和部分正文内容 (`page.locator('body').inner_text()`)。使用 `is_relevant` 判断页面是否与 AI 大模型主题相关。如果相关，记录标题、URL 和抓取时间，并尝试提取发布日期（这部分通常需要针对特定网站编写 CSS 选择器）。\n    *   **链接提取与过滤:** 使用 `page.locator('a[href]').evaluate_all()` 获取页面上所有链接的 `href` 和 `innerText`。\n    *   **链接处理:**\n        *   忽略无效链接（如锚点、JavaScript 调用）。\n        *   使用 `urljoin` 将相对路径转换为绝对 URL。\n        *   使用 `normalize_url` 清理 URL。\n        *   **聚焦过滤:** 检查链接的文本 (`link_text`) 或 URL 路径 (`parsed_absolute_url.path`) 是否包含关键词。只有相关的链接才会被添加到待访问队列 (`urls_to_visit`) 中。这里没有强制要求链接必须指向同一域名，允许爬虫“跳”到其他可能包含相关信息的网站。\n        *   避免将已访问或已在队列中的 URL 再次添加。\n    *   **错误处理:** 使用 `try...except` 捕获页面加载超时 (`PlaywrightTimeoutError`) 和其他潜在错误。\n    *   **礼貌延迟:** 在每次页面请求后使用 `asyncio.sleep(delay)` 暂停，避免对服务器造成过大压力。\n    *   **清理:** 循环结束后关闭浏览器。\n    *   返回收集到的相关文章信息列表。\n6.  **`main` 函数:** 异步程序的入口点，调用 `focused_crawler` 并打印结果。结果按抓取时间排序。\n7.  **运行:** 使用 `asyncio.run(main())` 启动事件循环并执行爬虫。\n\n**如何改进和扩展:**\n\n*   **更精确的日期提取:** 为每个目标网站编写特定的 CSS 选择器来定位发布日期元素。\n*   **内容摘要提取:** 同样，使用选择器提取文章摘要或前几段内容。\n*   **`robots.txt` 解析:** 在访问网站前，先获取并解析其 `robots.txt` 文件，遵守其中定义的规则。可以使用 `urllib.robotparser` 库。\n*   **数据存储:** 将结果保存到文件（CSV, JSON）或数据库中，而不是仅仅打印。\n*   **更复杂的聚焦逻辑:**\n    *   使用更高级的文本分析（如 TF-IDF 或嵌入向量）来判断页面和链接的相关性，而不仅仅是关键词匹配。\n    *   根据链接在页面上的位置（如主要内容区域 vs. 页脚）赋予不同优先级。\n    *   限制爬取深度。\n*   **并发:** 使用 Playwright 的 Browser Context 可以实现一定程度的并发爬取（同时打开多个页面），但这会增加资源消耗和对目标服务器的压力，需要更小心地管理延迟和请求速率。\n*   **处理动态加载内容:** 如果新闻是动态加载的（滚动加载），可能需要模拟滚动 (`page.evaluate('window.scrollBy(0, document.body.scrollHeight)')`) 或等待特定元素出现 (`page.wait_for_selector(...)`)。\n*   **用户代理轮换/代理:** 对于需要大规模爬取的场景，可能需要轮换 User-Agent 或使用代理 IP 来避免被封锁。\n\n这个示例提供了一个使用 Playwright 实现聚焦爬虫的基础框架，你可以根据具体需求进行调整和扩展。"}
	],
	"edges":[]
}