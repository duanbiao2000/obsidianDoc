在自然语言处理（NLP）任务中，尤其是在使用[[Transformer]]模型时，`Pooling`是一种常用的技术，用于对序列的输出进行降维或聚合。这是因为Transformer模型通常会产生一个序列的向量表示，其中每个词或标记都有对应的向量。然而，在许多情况下，我们需要将整个句子或文档转化为单个向量以供进一步处理，比如分类、回归等任务。这时，`Pooling`就派上了用场。

### 常见的Pooling方法

1. **平均池化（Average Pooling）**：
   - 平均池化通过对序列中的所有向量取平均值来生成一个单一的向量。这种方法简单且能够捕捉到序列的整体信息。
   - 公式：$$ \text{avg\_pool} = \frac{1}{N} \sum_{i=1}^{N} x_i $$
     - 其中 \(x_i\) 是第 \(i\) 个位置的向量，\(N\) 是序列长度。

2. **最大池化（Max Pooling）**：
   - 最大池化通过选取序列中每个维度的最大值来生成一个新的向量。这种方法可以突出序列中最显著的特征。
   - 对于每一维度，选择该维度上所有向量的最大值，构成新的向量。
   - 公式：
   $$ \text{max\_pool}_j = \max(x_{1,j}, x_{2,j}, ..., x_{N,j}) $$
     - 其中 \(x_{i,j}\) 表示第 \(i\) 个位置向量的第 \(j\) 维度。

3. **CLS Token Pooling**：
   - 在某些预训练模型如BERT中，会在输入序列的开头添加一个特殊的`[CLS]`标记，其最终的隐藏状态被用来代表整个序列的信息。这相当于直接取第一个位置的向量作为整个序列的表示。
   - 这种方法特别适用于那些已经针对特定任务进行了微调的模型。

4. **Attention-based Pooling**：
   - 注意力机制可以用于加权求和序列中的各个向量，从而得到一个综合考虑了不同部分重要性的向量。
   - 通过计算每个位置的重要性权重，然后根据这些权重对向量进行加权平均。
   - 公式：
   
 $$ \text{attn\_pool} = \sum_{i=1}^{N} w_i x_i $$
     - 其中 \(w_i\) 是位置 \(i\) 的权重，可以通过注意力机制计算得出。

### 应用场景
- **文本分类**：将句子或文档转换为固定长度的向量，以便输入到分类器中。
- **情感分析**：从文本中提取情绪相关的特征，形成一个整体的情感表示。
- **问答系统**：生成问题与答案之间的匹配得分，帮助判断答案的相关性。

不同的任务可能适合不同类型的Pooling策略，选择哪种方式取决于具体的应用需求以及数据特性。