---
aliases: Untitled
createdAt: 2024-12-10 21:35
updateAt: 2024-12-10 21:35
categories:
  - Mindset
tags:
  - Mindset/Reflection
---
# 类比

如果把满腹经纶的 Chatbot 比喻成人，那么大语言模型可以看成是 Chatbot 在大学毕业前从所有书本和各领域公开资料所获得的知识和学习[[推理能力]]。

所以基于大语言模型，Chatbot 能够回答截止到其毕业前相关的问题，但如果问题涉及到特定专业领域（相关资料为企业组织专有，非公开）或者是新出现的物种概念（大学毕业时尚未诞生），仅靠在学校的知识所得（对应预训练的大语言模型）则无法从容应对，需要具备毕业后持续获得新知识的渠道（如工作相关专业学习资料库），结合本身的学习推理能力，来做出专业应对。 同样，Chatbot 需要结合大语言模型的学习推理能力，和像ADB-PG 这样包含向量检索和全文检索能力的一站式数据库（存储了企业组织专有的以及最新的知识文档和向量特征），在应对问题时具备基于该数据库中的知识内容来提供更专业更具时效性的回答。

2021 年 8 月，李飞飞、Percy Liang 等百来位学者联名发布了文章：On the Opportunitiesand Risks of Foundation Models[1]，提出“基础模型”(Foundation Models)的概念：基于自监督学习的模型在学习过程中会体现出来各个不同方面的能力，这些能力为下游的应用提供了动力和理论基础，称这些大模型为“基础模型”。 “小模型”：针对特定应用场景需求进行训练，能完成特定任务，但是换到另外一个应用场景中可能并不适用，需要重新训练（我们现在用的大多数模型都是这样）。这些模型训练基本是“手工作坊式”，并且模型训练需要大规模的标注数据，如果某些应用场景的数据量少，训练出的模型精度就会不理想。 “大模型”：在大规模无标注数据上进行训练，学习出一种特征和规则。基于大模型进行应用开发时，将大模型进行微调（在下游小规模有标注数据进行二次训练）或者不进行微调，就可以完成多个应用场景的任务，实现通用的智能能力。

[玩转AIGC与应用部署 (陈翾)](obsidian://open?vault=obsidianDoc&file=5.%20Misc(Public)%2FEbooks%3C3%2F%E7%8E%A9%E8%BD%ACAIGC%E4%B8%8E%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%20(%E9%99%88%E7%BF%BE)%20(Z-Library).pdf)