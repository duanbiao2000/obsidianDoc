{
  "new_user": false,
  "chat_folder": "smart chat",
  "smart_sources_embed_model": "TaylorAI/bge-micro-v2",
  "smart_blocks_embed_model": "TaylorAI/bge-micro-v2",
  "smart_connections_folder": ".smart-connections",
  "smart_connections_folder_last": ".smart-connections",
  "system_prompts_folder": "smart prompts",
  "smart_chat_folder": "smart-chats",
  "smart_chat_folder_last": "smart-chats",
  "local_embedding_max_tokens": "512",
  "embedding_file_per_note": false,
  "chat_model_platform_key": "google_gemini",
  "open_router": {
    "api_key": "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",
    "model_name": "mistralai/mistral-7b-instruct:free",
    "description": "Mistral 7B Instruct (free)",
    "type": "API",
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
    "streaming": true,
    "fetch_models": true,
    "default_model": "mistralai/mistral-7b-instruct:free",
    "signup_url": "https://accounts.openrouter.ai/sign-up?redirect_url=https%3A%2F%2Fopenrouter.ai%2Fkeys",
    "key": "mistralai/mistral-7b-instruct:free",
    "max_input_tokens": 32768,
    "actions": false,
    "multimodal": false,
    "raw": {
      "id": "mistralai/mistral-7b-instruct:free",
      "name": "Mistral 7B Instruct (free)",
      "description": "A 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, with optimizations for speed and context length.\n\nThis is v0.1 of Mistral 7B Instruct. For v0.2, use [this model](/models/mistralai/mistral-7b-instruct:nitro).\n\nNote: this is a free, rate-limited version of [this model](/models/mistralai/mistral-7b-instruct). Outputs may be cached. Read about rate limits [here](/docs#limits).",
      "pricing": {
        "prompt": "0",
        "completion": "0",
        "image": "0",
        "request": "0"
      },
      "context_length": 32768,
      "architecture": {
        "modality": "text",
        "tokenizer": "Mistral",
        "instruct_type": "mistral"
      },
      "top_provider": {
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null
    }
  },
  "embed_input_min_chars": 50,
  "multi_heading_blocks": true,
  "enable_mobile": true,
  "actions": {
    "lookup": true
  },
  "api_key": "",
  "excluded_headings": "",
  "file_exclusions": "Untitled",
  "folder_exclusions": ".git .trash,  4. Archives, 5. Misc, Atlas, Extras",
  "show_full_path": true,
  "expanded_view": false,
  "language": "en",
  "log_render": false,
  "log_render_files": false,
  "recently_sent_retry_notice": false,
  "version": "2.2.76",
  "smart_notes_embed_model": "TaylorAI/bge-micro-v2",
  "muted_notices": {
    "restart embedding": true,
    "Notice muted": true,
    "initial scan progress": true,
    "start embedding": true,
    "done embedding": true,
    "embedding_progress": true
  },
  "google_gemini": {
    "api_key": "AIzaSyBwLHbup00ktpENOO-FUxTiwf5qSgpMH_c",
    "model_name": "gemini-1.5-flash-latest",
    "description": "Google Gemini",
    "type": "API",
    "api_key_header": "none",
    "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/MODEL_NAME:generateContent",
    "endpoint_streaming": "https://generativelanguage.googleapis.com/v1beta/models/MODEL_NAME:streamGenerateContent",
    "streaming": true,
    "actions": true,
    "adapter": "Gemini",
    "fetch_models": true,
    "default_model": "gemini-1.5-pro",
    "signup_url": "https://ai.google.dev/",
    "key": "gemini-1.5-flash-latest",
    "max_input_tokens": 1048576,
    "multimodal": true,
    "raw": {
      "name": "models/gemini-1.5-flash-latest",
      "version": "001",
      "displayName": "Gemini 1.5 Flash Latest",
      "description": "Fast and versatile multimodal model for scaling across diverse tasks",
      "inputTokenLimit": 1048576,
      "outputTokenLimit": 8192,
      "supportedGenerationMethods": [
        "generateContent",
        "countTokens"
      ],
      "temperature": 1,
      "topP": 0.95,
      "topK": 64
    }
  }
}