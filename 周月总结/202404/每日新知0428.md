# 宇多田光
![image.png](https://cdn.jsdelivr.net/gh/duanbiao2000/BlogGallery@main/picture/20240427232623.png)


1. **もしも願い一つだけ 叶うなら**  
   *もしもねがいひとつだけ かなうなら*
   カタカナ：もしもねがいヒトツだけ かなうなら
   罗马音：*mo shi mo ne ga i hi to tsu da ke ka na u na ra*
   意为：如果只能实现一个愿望，如果能成真的话。

2. **君の侧で眠らせて**  
   *きみのそばでねむらせて*
   カタカナ：キミノソバデネムラセテ
   罗马音：*ki mi no so ba de ne mu ra se te*
   意为：让我睡在你的身边。

3. **ビューティフルワールド (Beautiful World)**  
   カタカナ：ビューティフルワールド
   罗马音：ビーユティフルワールド (Bītī furu wārudo)
   意为：美丽的世界。

4. **ビューティフルボーイ (Beautiful Boy)**  
   カタカナ：ビューティフルボーイ
   罗马音：ビーユティフルボーイ (Bītī furu bōī)
   意为：美丽的男孩。

5. **自分の美しさまだ知らないの**  
   *じぶんのうつくしさまだしらないの*
   カタカナ：ジブンノウツクシサマダシラナイノ
   罗马音：*ji bun no u tsu ku shi sa ma da shi ra na i no*
   意为：你还没有意识到自己的美丽。

![]https://www.bilibili.com/video/BV1Bp4y1D747?p=5&vd_source=7038f96b6bb3b14743531b102b109c43


# MECE原则
使用MECE原则学习Python爬虫可以按照以下步骤进行问题分解：

1. **定义目标**：首先明确你想用Python爬虫实现什么功能，比如抓取特定网站的数据。

2. **分解任务**：将爬虫项目分解为几个主要的子任务，确保这些任务相互独立且完全穷尽。

   - **技术研究**：了解目标网站的结构，确定使用Python进行爬取的可行性。
   - **环境搭建**：安装Python，配置开发环境，如安装IDE（如PyCharm）。
   - **库的选择**：选择适合的库，如Requests用于HTTP请求，BeautifulSoup或lxml用于解析HTML。
   - **编写爬虫**：编写代码实现爬取逻辑。
     - 请求网页：使用Requests发送HTTP请求获取网页内容。
     - 解析内容：使用解析库提取所需数据。
     - 存储数据：将提取的数据保存到合适的格式或数据库中。
   - **异常处理**：编写异常处理代码，确保爬虫的稳定性。
   - **性能优化**：考虑爬取效率和网站的负载，优化代码。

3. **详细规划**：对每个子任务进一步细化，分解成更具体的步骤。

   - **技术研究**
     - 分析网站的robots.txt文件，了解爬虫政策。
     - 查看网站的HTML结构，确定数据的存放位置。
   - **环境搭建**
     - 安装Python解释器。
     - 安装并配置虚拟环境。
   - **库的选择**
     - 比较不同库的优缺点。
     - 安装选定的库。
   - **编写爬虫**
     - 设计函数，如发送请求、解析页面、存储数据。
     - 编写主函数，按顺序调用设计好的函数。
   - **异常处理**
     - 编写异常处理函数，处理网络请求错误、解析错误等。
   - **性能优化**
     - 实现延迟加载，避免给网站造成过大压力。
     - 使用多线程或异步IO提高爬取效率。

4. **执行和测试**：按照规划的步骤执行每个任务，并进行测试以确保功能正确实现。

5. **复审和调整**：在执行过程中根据测试结果和性能反馈对爬虫进行调整。

6. **结果分析**：完成爬虫编写后，分析结果数据，确保数据的准确性和完整性。

7. **文档和维护**：编写使用文档，为未来的维护工作提供指导。

通过MECE原则，你可以确保学习Python爬虫的每个步骤都被考虑到，没有遗漏，并且每个步骤都是独立的，这样可以提高学习效率，避免重复工作。

# 爬虫简化版
下面是一个简化的Python爬虫示例，它演示了上述一些最佳实践：

```python
import requests
from bs4 import BeautifulSoup
import logging
from urllib.robotparser import RobotFileParser

# 配置文件
config = {
    "user_agent": "MyCoolBot/1.0",
    "base_url": "http://example.com",
    "allowed_domains": ["http://example.com"],
    "disallowed_paths": ["/login", "/terms"]
}

# 日志配置
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class MyCoolBot:
    def __init__(self, base_url, user_agent):
        self.base_url = base_url
        self.user_agent = user_agent
        self.visited_urls = set()
        self.robot_parser = RobotFileParser()

    def is_allowed(self, url):
        self.robot_parser.set_url(f"{self.base_url}/robots.txt")
        self.robot_parser.read()
        return self.robot_parser.can_fetch(self.user_agent, url)

    def fetch(self, url):
        if url in self.visited_urls or not self.is_allowed(url):
            logging.info(f"Skipping {url}")
            return None

        logging.info(f"Fetching {url}")
        try:
            response = requests.get(url, headers={"User-Agent": self.user_agent})
            response.raise_for_status()
            self.visited_urls.add(url)
            return response.text
        except requests.RequestException as e:
            logging.error(f"Failed to fetch {url}: {e}")
            return None

    def parse(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        # 假设我们只提取所有的段落文本
        return [p.get_text() for p in soup.find_all('p')]

    def run(self):
        urls_to_visit = [self.base_url]
        while urls_to_visit:
            current_url = urls_to_visit.pop(0)
            html = self.fetch(current_url)
            if html:
                data = self.parse(html)
                # 处理数据，例如保存到文件或数据库
                logging.info(f"Extracted data from {current_url}")
                # 找到下一页的链接，这里只是示例，实际情况可能需要更复杂的逻辑
                for link in ['http://example.com/page2', 'http://example.com/page3']:
                    if link not in self.visited_urls and self.is_allowed(link):
                        urls_to_visit.append(link)

if __name__ == "__main__":
    logging.info("Starting the web crawler")
    bot = MyCoolBot(config["base_url"], config["user_agent"])
    bot.run()
    logging.info("Crawler has finished")
```

这个示例包括了以下最佳实践：

- **模块化设计**：使用类`MyCoolBot`来封装爬虫逻辑。
- **清晰的函数和方法**：`fetch`, `parse`, `run`等方法具有清晰的功能描述。
- **异常处理**：在`fetch`方法中使用try-except来处理请求异常。
- **日志记录**：使用`logging`模块记录关键信息和错误。
- **配置文件**：使用字典`config`来存储配置信息。
- **遵守robots.txt**：通过`RobotFileParser`和`is_allowed`方法来检查是否允许爬取某个URL。
- **使用注释**：代码中包含了必要的注释。
- **代码格式化**：代码风格遵循PEP 8指南。

请注意，这个示例是一个简化的版本，实际的爬虫可能需要更复杂的逻辑来处理各种情况，例如递归爬取、处理JavaScript生成的内容、模拟登录等。此外，对于大规模的爬取任务，可能还需要考虑使用更高级的技术，如分布式爬虫架构。

# 优化
要优化爬虫以提高爬取效率并减少对目标网站的负担，可以采取以下措施：

1. **遵守robots.txt**：确保你的爬虫遵守目标网站的robots.txt文件中的规则，不要爬取网站所有者不希望被爬取的页面。

2. **设置合理的请求头**：使用合适的请求头，如`User-Agent`，来模拟真实用户的浏览器行为。

3. **请求间隔**：在请求之间设置延迟，避免过快地发送请求，这样可以减少对服务器的压力。

4. **使用会话Session**：使用`requests.Session()`来保持某些参数的一致性，同时减少重复的连接建立。

5. **使用异步请求**：利用`asyncio`和`aiohttp`等库进行异步请求，可以提高并发性，从而提高爬取效率。

6. **多线程/多进程**：使用多线程或多进程来并行处理请求，但要注意不要过多，以免给服务器带来过大压力。

7. **使用代理**：使用代理服务器可以隐藏你的真实IP地址，同时也可以分散请求，降低被封禁的风险。

8. **数据解析优化**：只提取需要的数据，避免不必要的解析，使用CSS选择器或XPath等高效的解析方法。

9. **缓存机制**：对于不经常变动的页面，可以缓存结果，避免重复爬取。

10. **错误处理**：正确处理各种HTTP错误和异常，如404、503等，避免因为错误处理不当导致的额外请求。

11. **条件GET请求**：使用`If-Modified-Since`或`Etag`头部来请求页面，如果页面自上次抓取以来没有更改，则服务器可以返回304状态码，节省带宽。

12. **减少资源使用**：避免下载不必要的资源，如图片、视频、大文件等。

13. **优先级队列**：根据页面的重要性设置不同的爬取优先级。

14. **智能重试**：对于失败的请求，可以实施智能重试策略，而不是立即重试。

15. **使用CDN**：如果可能，利用内容分发网络（CDN）来减少对源服务器的请求。

16. **带宽限制**：限制单个请求的带宽使用，避免影响网站的其他用户。

17. **尊重网站资源**：避免在网站的高峰时段进行爬取。

18. **用户反馈**：根据网站管理员的反馈调整爬虫行为。

19. **数据存储优化**：在将数据存储到数据库之前，对其进行压缩和优化，减少存储空间的使用。

20. **使用数据库索引**：对数据库进行适当的索引，加快数据存储和检索速度。

21. **前端模拟**：对于使用JavaScript动态生成内容的网站，可以使用Selenium或Puppeteer等工具模拟浏览器行为。

22. **API使用**：如果可能，使用网站提供的API来获取数据，这通常是最高效且最友好的方式。

# 代码实例
通过这些优化措施，你的爬虫可以在不干扰网站正常运行的前提下，更高效地完成数据爬取任务。
以下是一些可以通过代码实例讲解的爬虫优化措施：

1. **遵守robots.txt**：
   ```python
   import requests
   from urllib.robotparser import RobotFileParser

   def can_fetch(url, user_agent='*'):
       rp = RobotFileParser()
       rp.set_url('http://example.com/robots.txt')
       rp.read()
       return rp.can_fetch(user_agent, url)

   url_to_fetch = 'http://example.com/somepage.html'
   if can_fetch(url_to_fetch):
       response = requests.get(url_to_fetch)
       # Process the response
   ```

2. **设置合理的请求头**：
   ```python
   headers = {
       'User-Agent': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://mywebsite.com/bot)'
   }
   response = requests.get('http://example.com', headers=headers)
   ```

3. **请求间隔**：
   ```python
   import time

   def fetch_with_delay(url, delay=1):
       time.sleep(delay)
       return requests.get(url)

   response = fetch_with_delay('http://example.com')
   ```

4. **使用会话Session**：
   ```python
   with requests.Session() as session:
       session.headers.update({'User-Agent': 'MyBot/1.0'})
       response = session.get('http://example.com')
   ```

5. **使用异步请求**（需要使用`aiohttp`库）：
   ```python
   import aiohttp
   import asyncio

   async def fetch(url):
       async with aiohttp.ClientSession() as session:
           async with session.get(url) as response:
               return await response.text()

   loop = asyncio.get_event_loop()
   pages = ['http://example.com/page1', 'http://example.com/page2']
   responses = loop.run_until_complete(asyncio.gather(*(fetch(page) for page in pages)))
   ```

6. **多线程/多进程**（使用`concurrent.futures`）：
   ```python
   from concurrent.futures import ThreadPoolExecutor

   def fetch_url(url):
       return requests.get(url).text

   with ThreadPoolExecutor(max_workers=5) as executor:
       futures = [executor.submit(fetch_url, page) for page in pages]
       responses = [future.result() for future in futures]
   ```

7. **使用代理**：
   ```python
   proxies = {
       'http': 'http://10.10.1.10:3128',
       'https': 'http://10.10.1.11:1080',
   }
   response = requests.get('http://example.com', proxies=proxies)
   ```

8. **数据解析优化**：
   ```python
   from bs4 import BeautifulSoup

   soup = BeautifulSoup(response.text, 'html.parser')
   # Extract only the data you need, for example, all paragraph texts
   paragraphs = [p.get_text() for p in soup.find_all('p')]
   ```

9. **缓存机制**：
   ```python
   import hashlib
   import requests

   def fetch_with_cache(url, cache_file):
       cache_key = hashlib.md5(url.encode()).hexdigest()
       try:
           with open(cache_file, 'r') as f:
               if f.read() == cache_key:
                   return  # Return cached data if available
       except FileNotFoundError:
           pass  # No cache file, fetch the data

       response = requests.get(url)
       with open(cache_file, 'w') as f:
           f.write(cache_key)  # Write cache key

       return response

   response = fetch_with_cache('http://example.com', 'cache_file.txt')
   ```

10. **错误处理**：
    ```python
    try:
        response = requests.get('http://example.com')
        response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code
    except requests.exceptions.HTTPError as e:
        print(e)
    ```

请注意，以上代码示例仅用于演示目的，实际使用时可能需要根据具体情况进行调整。
# CDN应对策略

当目标网站使用了内容分发网络（CDN）时，你的爬虫可能需要进行一些调整以适应这种变化。以下是一些可能的策略：

1. **尊重CDN提供商的规则**：CDN提供商可能有特定的爬虫策略或指南，你应该遵循这些规则。

2. **使用多个IP地址**：CDN可能会限制单个IP地址的流量。使用多个IP地址或使用代理服务可以避免被限制。

3. **减少请求频率**：由于CDN设计用于处理大量流量，过快的请求可能会触发保护机制。适当降低请求频率。

4. **使用会话**：通过`requests.Session()`使用会话可以保持连接活跃，减少开销。

5. **缓存内容**：CDN缓存了网站内容，你的爬虫也应该实现缓存机制，避免重复请求相同的数据。

6. **使用边缘节点**：如果可能，尝试使用CDN的边缘节点来访问数据，这可能比直接从源服务器请求更快。

7. **避免触发安全机制**：CDN通常有安全机制来防止恶意流量。避免使用自动化行为的模式，如连续请求或快速请求。

8. **遵守robots.txt**：确保你的爬虫遵守CDN上的robots.txt文件，即使它们可能与主网站的有所不同。

9. **使用合适的请求头**：确保你的请求头是合适的，模拟真实用户的浏览器行为。

10. **使用HTTPS**：如果CDN支持，使用HTTPS可以提高安全性，并可能提高请求成功率。

11. **错误处理**：实现健壮的错误处理机制，以应对可能的网络波动和CDN的保护措施。

12. **监控爬虫行为**：密切监控你的爬虫，确保它没有被CDN的保护机制所阻止。

13. **适应CDN的变化**：CDN可能会更改其IP地址范围或策略，你的爬虫需要能够适应这些变化。

14. **使用API**：如果CDN提供商提供了API，尽量使用API来获取数据，这通常是最有效且最不容易触发安全机制的方式。

15. **合法合规**：确保你的爬虫行为合法且符合CDN提供商和目标网站的使用条款。

16. **联系网站管理员**：如果可能，与网站管理员沟通你的爬虫需求，获得他们的许可或建议。

17. **用户代理轮换**：考虑使用不同的用户代理字符串来避免被CDN识别为爬虫。

18. **避免高峰时段**：尽量在网站的非高峰时段进行爬取，减少对CDN的负担。

19. **使用Web性能API**：如果CDN支持，使用Web性能API来优化请求。

20. **智能重试**：实现智能重试机制，避免在CDN暂时不可用时进行无效的请求。

通过这些策略，你的爬虫可以更好地与使用CDN的网站交互，同时减少对网站和CDN的负担。

# 代码示例
针对目标网站使用内容分发网络（CDN）的情况，以下是一些相应的代码实例，展示如何调整爬虫策略：

1. **使用多个IP地址**（通过代理服务）：
   ```python
   proxies = {
       'http': 'http://proxy1.example.com:8080',
       'https': 'http://proxy1.example.com:8080'
   }
   response = requests.get('http://cdn.example.com', proxies=proxies)
   ```

2. **减少请求频率**（使用时延）：
   ```python
   import time
   import random

   def fetch_with_delay(url):
       delay = random.uniform(1, 3)  # 1到3秒之间的随机延迟
       time.sleep(delay)
       return requests.get(url)

   response = fetch_with_delay('http://cdn.example.com')
   ```

3. **使用会话Session**：
   ```python
   with requests.Session() as session:
       session.headers.update({'User-Agent': 'MyBot/1.0'})
       response = session.get('http://cdn.example.com')
   ```

4. **缓存内容**（使用简单的文件缓存机制）：
   ```python
   def fetch_with_cache(url, cache_file):
       try:
           with open(cache_file, 'r') as f:
               return f.read()  # 尝试从缓存读取
       except FileNotFoundError:
           pass  # 缓存文件不存在，继续请求

       response = requests.get(url)
       with open(cache_file, 'w') as f:
           f.write(response.text)  # 将响应内容写入缓存
       return response.text

   content = fetch_with_cache('http://cdn.example.com', 'cache_file.txt')
   ```

5. **避免触发安全机制**（使用会话和随机延迟）：
   ```python
   from requests import Session
   import time

   session = Session()
   session.headers.update({'User-Agent': 'MyBot/1.0'})

   def safe_fetch(url):
       try:
           response = session.get(url, timeout=5)
           response.raise_for_status()  # 检查响应状态码
       except requests.exceptions.RequestException as e:
           print(f'Request failed: {e}')
           time.sleep(3)  # 遇到错误时稍作等待
           return None
       return response

   response = safe_fetch('http://cdn.example.com')
   ```

6. **使用HTTPS**：
   ```python
   response = requests.get('https://cdn.example.com', verify=True)  # 使用HTTPS连接
   ```

7. **错误处理**（处理HTTP错误）：
   ```python
   try:
       response = requests.get('http://cdn.example.com')
       response.raise_for_status()  # 如果响应状态码不是200，将抛出HTTPError
   except requests.exceptions.HTTPError as e:
       print(f'HTTP Error: {e}')
   ```

8. **监控爬虫行为**（使用日志记录）：
   ```python
   import logging

   logging.basicConfig(level=logging.INFO)

   try:
       response = requests.get('http://cdn.example.com')
       response.raise_for_status()
   except requests.exceptions.RequestException as e:
       logging.error(f'Request failed: {e}')
   ```

9. **适应CDN的变化**（动态获取并使用CDN的IP地址）：
   ```python
   # 假设我们有方法获取CDN的IP地址列表
   cdn_ips = get_cdn_ips('cdn.example.com')
   proxies = {'http': '', 'https': ''}
   for ip in cdn_ips:
       proxies['http'] = f'http://{ip}:8080'
       proxies['https'] = f'https://{ip}:8080'
       # 使用代理进行请求
       response = requests.get('http://cdn.example.com', proxies=proxies)
       if response.ok:
           break  # 如果请求成功，停止循环
   ```

10. **智能重试**（使用重试机制）：
    ```python
    from requests.adapters import HTTPAdapter
    from requests.packages.urllib3.util.retry import Retry
    import requests

    s = requests.Session()
    retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])
    s.mount('http://', HTTPAdapter(max_retries=retries))
    s.mount('https://', HTTPAdapter(max_retries=retries))

    response = s.get('http://cdn.example.com')
    ```

请注意，上述代码示例仅用于演示目的，实际使用时可能需要根据具体情况进行调整。特别是代理服务和CDN IP获取部分，通常需要额外的服务或API来实现。此外，确保你的爬虫行为合法且符合CDN提供商和目标网站的使用条款是非常重要的。
