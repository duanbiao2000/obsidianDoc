---
date: 2025-06-07 21:42
tags:
  - Domain/Creativity/Product
---

# GPT Researcher ç³»ç»Ÿåˆ†æ

## 1. ä¸»è¦åŠŸèƒ½

GPT Researcher æ˜¯ä¸€ä¸ªå¼€æºçš„è‡ªä¸»ç ”ç©¶ä»£ç†ç³»ç»Ÿï¼Œä¸“ä¸ºæ·±åº¦ç ”ç©¶ä»»åŠ¡è®¾è®¡ã€‚å…¶æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š

- **è‡ªåŠ¨åŒ–ç ”ç©¶æµç¨‹**ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢è‡ªåŠ¨è¿›è¡Œç½‘ç»œæœç´¢ã€ä¿¡æ¯æ”¶é›†ã€åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
- **å¤šä»£ç†åä½œ**ï¼šä½¿ç”¨ LangGraph æ¡†æ¶å®ç°å¤šä¸ªä¸“ä¸šåŒ– AI ä»£ç†ååŒå·¥ä½œ
- **æ·±åº¦ç ”ç©¶æŠ¥å‘Š**ï¼šç”Ÿæˆè¯¦ç»†ã€æœ‰äº‹å®ä¾æ®ä¸”æ— åè§çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…å«å¼•ç”¨æ¥æº
- **å¤šæ ¼å¼è¾“å‡º**ï¼šæ”¯æŒ PDFã€Wordã€Markdown ç­‰å¤šç§æ ¼å¼çš„æŠ¥å‘Šå¯¼å‡º
- **Web å’Œæœ¬åœ°æ–‡æ¡£ç ”ç©¶**ï¼šèƒ½å¤Ÿä»äº’è”ç½‘å’Œæœ¬åœ°æ–‡æ¡£ä¸­æ”¶é›†ä¿¡æ¯
- **æ™ºèƒ½å›¾åƒå¤„ç†**ï¼šä¸ºæŠ¥å‘ŠæŠ“å–å’Œç­›é€‰ç›¸å…³å›¾åƒ

## 2. å®æ–½æ–¹æ³•

GPT Researcher çš„å®æ–½åŸºäºä»¥ä¸‹å…³é”®ç»„ä»¶ï¼š

### åç«¯æ¶æ„

1. **åŸºç¡€ç ”ç©¶å¼•æ“**ï¼š
   - ä½¿ç”¨ Python å’Œ FastAPI æ„å»º
   - æ”¯æŒå¤šç§ LLMï¼ˆOpenAIã€Claudeã€Geminiã€Mistral ç­‰ï¼‰
   - æ”¯æŒå¤šç§æœç´¢å¼•æ“ï¼ˆTavilyã€Googleã€Bingã€DuckDuckGo ç­‰ï¼‰
2. **å¤šä»£ç†ç³»ç»Ÿ**ï¼ˆLangGraphï¼‰ï¼š
   - åŸºäº LangChain çš„ LangGraph æ¡†æ¶
   - å®ç°äº† 8 ä¸ªä¸“ä¸šåŒ–ä»£ç†ï¼šChief Editorã€Researcherã€Editorã€Reviewerã€Revisorã€Writerã€Publisher å’Œ Human
   - ä½¿ç”¨æœ‰å‘å›¾å®šä¹‰ä»£ç†ä¹‹é—´çš„å·¥ä½œæµå’Œé€šä¿¡

### å‰ç«¯å®ç°

1. **è½»é‡çº§ç•Œé¢**ï¼š
   - åŸºäº HTML/CSS/JavaScript çš„ç®€å•ç•Œé¢
   - é€šè¿‡ FastAPI æä¾›æœåŠ¡
2. **é«˜çº§ NextJS åº”ç”¨**ï¼š
   - ä½¿ç”¨ NextJS å’Œ Tailwind CSS æ„å»º
   - å®æ—¶ç ”ç©¶è¿›åº¦è·Ÿè¸ª
   - äº¤äº’å¼ç»“æœæ˜¾ç¤º
   - å¯å®šåˆ¶çš„ç ”ç©¶è®¾ç½®

### éƒ¨ç½²é€‰é¡¹

1. **æœ¬åœ°éƒ¨ç½²**ï¼š
   - é€šè¿‡ pip å®‰è£…ï¼ˆ`pip install gpt-researcher`ï¼‰
   - Docker å®¹å™¨æ”¯æŒ
   - ç›´æ¥ä»æºä»£ç è¿è¡Œ
2. **LangGraph äº‘éƒ¨ç½²**ï¼š
   - ä½¿ç”¨Â `langgraph-cli`Â å·¥å…·
   - æ”¯æŒæµå¼å’Œå¼‚æ­¥ç«¯ç‚¹

## 3. æ–°é¢–éƒ¨åˆ†

GPT Researcher çš„åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

1. **å¤šä»£ç†åä½œæ¡†æ¶**ï¼š
   - å— [[STORM è®ºæ–‡]]å¯å‘ï¼Œå®ç°äº†ä¸“ä¸šåŒ–ä»£ç†å›¢é˜Ÿ
   - ä»£ç†é—´çš„åŠ¨æ€åä½œå’Œåé¦ˆå¾ªç¯
   - æ¨¡æ‹Ÿäººç±»ç ”ç©¶å›¢é˜Ÿçš„å·¥ä½œæµç¨‹
2. **ç ”ç©¶æ·±åº¦å’Œè´¨é‡**ï¼š
   - èƒ½å¤Ÿèšåˆ 20+ æ¥æºå½¢æˆå®¢è§‚ç»“è®º
   - ç”Ÿæˆè¶…è¿‡ 2,000 å­—çš„è¯¦ç»†æŠ¥å‘Š
   - ä¿æŒä¸Šä¸‹æ–‡å’Œè®°å¿†è´¯ç©¿æ•´ä¸ªç ”ç©¶è¿‡ç¨‹
3. **æ¨¡å—åŒ–å’Œå¯æ‰©å±•è®¾è®¡**ï¼š
   - æ”¯æŒå¤šç§ LLM å’Œæœç´¢å¼•æ“
   - å¯è‡ªå®šä¹‰ç ”ç©¶æµç¨‹å’ŒæŠ¥å‘Šæ ¼å¼
   - æä¾› API å’Œ SDK ä¾›å¼€å‘è€…é›†æˆ
4. **å®æ—¶åé¦ˆå’Œäººæœºåä½œ**ï¼š
   - æ”¯æŒäººç±»åœ¨å¾ªç¯ä¸­æä¾›åé¦ˆ
   - å®æ—¶æ˜¾ç¤ºç ”ç©¶è¿›åº¦å’Œä¸­é—´ç»“æœ

## 4. ä»£ç å·¥ä½œæµç¨‹

### åŸºæœ¬ç ”ç©¶æµç¨‹

1. **åˆå§‹åŒ–**ï¼š

   Copy

   ç”¨æˆ·æŸ¥è¯¢Â â†’Â åˆ›å»ºç‰¹å®šé¢†åŸŸä»£ç†Â â†’Â ç”Ÿæˆç ”ç©¶é—®é¢˜é›†

2. **æ•°æ®æ”¶é›†**ï¼š

   Copy

   ç ”ç©¶é—®é¢˜Â â†’Â è§¦å‘çˆ¬è™«ä»£ç†Â â†’Â æœç´¢ç›¸å…³ä¿¡æ¯Â â†’Â æ±‡æ€»å’Œè·Ÿè¸ªæ¥æº

3. **åˆ†æå’ŒæŠ¥å‘Š**ï¼š

   Copy

   è¿‡æ»¤å’Œèšåˆä¿¡æ¯Â â†’Â ç”Ÿæˆç ”ç©¶æŠ¥å‘ŠÂ â†’Â å¯¼å‡ºä¸ºæ‰€éœ€æ ¼å¼

### LangGraph å¤šä»£ç†æµç¨‹

1. **è§„åˆ’é˜¶æ®µ**ï¼š

   Copy

   ChiefÂ EditorÂ æ¥æ”¶ä»»åŠ¡Â â†’Â BrowserÂ è¿›è¡Œåˆæ­¥ç ”ç©¶Â â†’Â EditorÂ è§„åˆ’æŠ¥

   å‘Šç»“æ„

2. **å¹¶è¡Œç ”ç©¶**ï¼š

   Copy

   å¯¹æ¯ä¸ªå¤§çº²ä¸»é¢˜ï¼š

   Â Â ResearcherÂ æ·±å…¥ç ”ç©¶Â â†’Â ReviewerÂ éªŒè¯è‰ç¨¿Â â†’Â RevisorÂ æ ¹æ®åé¦ˆ

   Â Â ä¿®æ”¹

3. **æ•´åˆå’Œå‘å¸ƒ**ï¼š

   Copy

   WriterÂ ç¼–å†™æœ€ç»ˆæŠ¥å‘ŠÂ â†’Â PublisherÂ ä»¥å¤šç§æ ¼å¼å‘å¸ƒ

### å‰ç«¯ä¸åç«¯äº¤äº’

1. **ç”¨æˆ·è¾“å…¥**ï¼š

   Copy

   ç”¨æˆ·æäº¤æŸ¥è¯¢Â â†’Â InputAreaÂ ç»„ä»¶å¤„ç†Â â†’Â å‘é€åˆ°åç«¯Â API

2. **ç ”ç©¶æ‰§è¡Œ**ï¼š

   Copy

   åç«¯æ¥æ”¶è¯·æ±‚Â â†’Â å¯åŠ¨ç ”ç©¶æµç¨‹Â â†’Â é€šè¿‡Â WebSocketÂ æµå¼è¿”å›è¿›åº¦å’Œç»“æœ

3. **ç»“æœå±•ç¤º**ï¼š

   Copy

   å‰ç«¯æ¥æ”¶æµå¼æ•°æ®Â â†’Â å®æ—¶æ›´æ–°Â UIÂ â†’Â æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Šå’Œæ¥æº

### å…³é”®æ§åˆ¶æµç¨‹

1. **ç ”ç©¶ä»£ç†åˆå§‹åŒ–**ï¼š

   Copy

   researcherÂ =Â GPTResearcher(query=query,Â 

   report_type="research_report")

   research_resultÂ =Â awaitÂ researcher.conduct_research()

   reportÂ =Â awaitÂ researcher.write_report()

2. **LangGraph å·¥ä½œæµ**ï¼š

   Copy

   #Â å®šä¹‰å·¥ä½œæµå›¾

   workflowÂ =Â StateGraph(nodes=[...])

   #Â å®šä¹‰èŠ‚ç‚¹é—´è½¬æ¢

   workflow.add_edge("chief_editor",Â "browser")

   workflow.add_edge("browser",Â "editor")

   #Â ...

   #Â å¹¶è¡Œå¤„ç†

   forÂ topicÂ inÂ outline:

   Â Â Â Â workflow.add_parallel_tasks(...)

3. **å‰ç«¯çŠ¶æ€ç®¡ç†**ï¼š

```javascript
   //Â çŠ¶æ€åˆå§‹åŒ–

   constÂ [promptValue,Â setPromptValue]Â =Â useState("");

   constÂ [results,Â setResults]Â =Â useState<TResearchResult[]>

   ([]);

   //Â æäº¤å¤„ç†

   constÂ handleSubmitÂ =Â asyncÂ (query:Â string)Â =>Â {

   Â Â setLoading(true);

   Â Â constÂ responseÂ =Â awaitÂ fetch("/api/research",Â {Â ...Â });

   Â Â constÂ streamÂ =Â response.body;

   Â Â //Â å¤„ç†æµå¼å“åº”...

   };
```

GPT Researcher é€šè¿‡è¿™äº›ç²¾å¿ƒè®¾è®¡çš„ç»„ä»¶å’Œæµç¨‹ï¼Œå®ç°äº†ä¸€ä¸ªå¼ºå¤§ã€çµæ´»ä¸”ç”¨æˆ·å‹å¥½çš„è‡ªåŠ¨åŒ–ç ”ç©¶ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ç ”ç©¶æ•ˆç‡å’Œè´¨é‡ã€‚

---
# GPT Researcher ä¸­çš„è®¾è®¡æ¨¡å¼ä¸ç®—æ³•åˆ†æ

GPT Researcher é¡¹ç›®è¿ç”¨äº†å¤šç§è®¾è®¡æ¨¡å¼å’Œç®—æ³•æ¥å®ç°å…¶å¤æ‚çš„ç ”ç©¶è‡ªåŠ¨åŒ–åŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯å¯¹é¡¹ç›®ä¸­ä¸»è¦è®¾è®¡æ¨¡å¼å’Œç®—æ³•çš„è¯¦ç»†åˆ†æï¼š

## è®¾è®¡æ¨¡å¼

### 1. å·¥å‚æ¨¡å¼ (Factory Pattern)

scraper.py

gpt_researcher/scraper

Copy

defÂ get_scraper(self,Â link):

Â Â Â Â SCRAPER_CLASSESÂ =Â {

Â Â Â Â Â Â Â Â "pdf":Â PyMuPDFScraper,

Â Â Â Â Â Â Â Â "arxiv":Â ArxivScraper,

Â Â Â Â Â Â Â Â "bs":Â BeautifulSoupScraper,

Â Â Â Â Â Â Â Â "web_base_loader":Â WebBaseLoaderScraper,

Â Â Â Â Â Â Â Â "browser":Â BrowserScraper,

Â Â Â Â Â Â Â Â "nodriver":Â NoDriverScraper,

Â Â Â Â Â Â Â Â "tavily_extract":Â TavilyExtract,

Â Â Â Â Â Â Â Â "firecrawl":Â FireCrawl,

Â Â Â Â }

Â Â Â Â scraper_keyÂ =Â None

Â Â Â Â ifÂ link.endswith(".pdf"):

Â Â Â Â Â Â Â Â scraper_keyÂ =Â "pdf"

Â Â Â Â elifÂ "arxiv.org"Â inÂ link:

Â Â Â Â Â Â Â Â scraper_keyÂ =Â "arxiv"

Â Â Â Â else:

Â Â Â Â Â Â Â Â scraper_keyÂ =Â self.scraper

- **å®ç°**ï¼š`Scraper`Â ç±»ä¸­çš„Â `get_scraper`Â æ–¹æ³•æ ¹æ®é“¾æ¥ç±»å‹åˆ›å»ºé€‚å½“çš„çˆ¬è™«å®ä¾‹
- **ä¼˜åŠ¿**ï¼šå…è®¸ç³»ç»Ÿæ ¹æ®å†…å®¹ç±»å‹åŠ¨æ€é€‰æ‹©åˆé€‚çš„çˆ¬è™«ï¼Œæ— éœ€ä¿®æ”¹å®¢æˆ·ç«¯ä»£ç 

### 2. ç­–ç•¥æ¨¡å¼ (Strategy Pattern)

base.py

gpt_researcher/llm_provider/generic

Copy

@classmethod

defÂ from_provider(cls,Â provider,Â **kwargs):

Â Â Â Â ifÂ providerÂ ==Â "openai":

Â Â Â Â Â Â Â Â _check_pkg("langchain_openai")

Â Â Â Â Â Â Â Â fromÂ langchain_openaiÂ importÂ ChatOpenAI

Â Â Â Â Â Â Â Â llmÂ =Â ChatOpenAI(**kwargs)

Â Â Â Â elifÂ providerÂ ==Â "anthropic":

Â Â Â Â Â Â Â Â _check_pkg("langchain_anthropic")

Â Â Â Â Â Â Â Â fromÂ langchain_anthropicÂ importÂ ChatAnthropic

Â Â Â Â Â Â Â Â llmÂ =Â ChatAnthropic(**kwargs)

Â Â Â Â #Â ...Â å…¶ä»–æä¾›å•†

- **å®ç°**ï¼š`GenericLLMProvider`Â ç±»å…è®¸åœ¨è¿è¡Œæ—¶é€‰æ‹©ä¸åŒçš„ LLM æä¾›å•†
- **ä¼˜åŠ¿**ï¼šå°† LLM æä¾›å•†çš„å…·ä½“å®ç°ä¸ä½¿ç”¨ä»£ç åˆ†ç¦»ï¼Œä¾¿äºæ·»åŠ æ–°çš„æä¾›å•†

### 3. è§‚å¯Ÿè€…æ¨¡å¼ (Observer Pattern)

websocket_manager.py

backend/server

Copy

Loading...

- **å®ç°**ï¼šWebSocket è¿æ¥ç”¨äºå®æ—¶å‘å‰ç«¯æ¨é€ç ”ç©¶è¿›åº¦å’Œç»“æœ
- **ä¼˜åŠ¿**ï¼šå®ç°äº†å‰ç«¯ä¸åç«¯çš„æ¾è€¦åˆé€šä¿¡ï¼Œæ”¯æŒå®æ—¶æ›´æ–°

### 4. å‘½ä»¤æ¨¡å¼ (Command Pattern)

orchestrator.py

multi_agents/agents

Copy

Loading...

- **å®ç°**ï¼šæ¯ä¸ªä»£ç†çš„æ“ä½œè¢«å°è£…ä¸ºå¯æ‰§è¡Œçš„å‘½ä»¤ï¼Œç”±Â `StateGraph`Â åè°ƒæ‰§è¡Œ
- **ä¼˜åŠ¿**ï¼šæ”¯æŒæ“ä½œçš„æ’é˜Ÿã€æ’¤é”€å’Œé‡åšï¼Œä¾¿äºå®ç°å¤æ‚çš„å·¥ä½œæµ

### 5. è£…é¥°å™¨æ¨¡å¼ (Decorator Pattern)

chat.py

backend/chat

Copy

Loading...

- **å®ç°**ï¼šä½¿ç”¨Â `@tool`Â è£…é¥°å™¨å°†å‡½æ•°è½¬æ¢ä¸º LangChain å·¥å…·
- **ä¼˜åŠ¿**ï¼šåŠ¨æ€æ‰©å±•å‡½æ•°åŠŸèƒ½ï¼Œæ— éœ€ä¿®æ”¹åŸå§‹ä»£ç 

### 6. é€‚é…å™¨æ¨¡å¼ (Adapter Pattern)

online_document.py

gpt_researcher/document

Copy

Loading...

- **å®ç°**ï¼šä¸ºä¸åŒæ–‡æ¡£æ ¼å¼æä¾›ç»Ÿä¸€çš„åŠ è½½æ¥å£
- **ä¼˜åŠ¿**ï¼šå…è®¸ç³»ç»Ÿå¤„ç†å¤šç§æ–‡æ¡£æ ¼å¼ï¼Œè€Œæ— éœ€äº†è§£å…·ä½“å®ç°ç»†èŠ‚

## ç®—æ³•

### 1. å‘é‡ç›¸ä¼¼åº¦æœç´¢

compression.py

gpt_researcher/context

Copy

Loading...

- **å®ç°**ï¼šä½¿ç”¨åµŒå…¥å‘é‡å’Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ¥æ£€ç´¢ç›¸å…³æ–‡æ¡£
- **å¤æ‚åº¦**ï¼šO(n) ç”¨äºçº¿æ€§æ‰«æï¼Œæˆ– O(log n) ä½¿ç”¨è¿‘ä¼¼æœ€è¿‘é‚»ç®—æ³•
- **åº”ç”¨**ï¼šåœ¨ç ”ç©¶è¿‡ç¨‹ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯

### 2. æ–‡æœ¬åˆ†å—ç®—æ³•

chat.py

backend/chat

Copy

Loading...

- **å®ç°**ï¼šé€’å½’å­—ç¬¦åˆ†å‰²å™¨å°†é•¿æ–‡æœ¬åˆ†å‰²æˆé‡å çš„å°å—
- **å¤æ‚åº¦**ï¼šO(n) å…¶ä¸­ n æ˜¯æ–‡æœ¬é•¿åº¦
- **åº”ç”¨**ï¼šå°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚åˆ LLM å¤„ç†çš„å°å—

### 3. é˜²æŠ–ç®—æ³• (Debounce)

InputArea.tsx

frontend/nextjs/components/ResearchBlocks/elements

Copy

Loading...

- **å®ç°**ï¼šå»¶è¿Ÿå‡½æ•°æ‰§è¡Œç›´åˆ°ç”¨æˆ·åœæ­¢è¾“å…¥
- **å¤æ‚åº¦**ï¼šO(1)
- **åº”ç”¨**ï¼šä¼˜åŒ–æ–‡æœ¬åŒºåŸŸé«˜åº¦è°ƒæ•´ï¼Œå‡å°‘ä¸å¿…è¦çš„ DOM æ“ä½œ

### 4. æœ‰å‘å›¾éå†ç®—æ³•

editor.py

multi_agents/agents

Copy

Loading...

- **å®ç°**ï¼šä½¿ç”¨ LangGraph çš„Â `StateGraph`Â å®ç°æœ‰å‘å›¾å·¥ä½œæµ
- **å¤æ‚åº¦**ï¼šO(V+E) å…¶ä¸­ V æ˜¯èŠ‚ç‚¹æ•°ï¼ŒE æ˜¯è¾¹æ•°
- **åº”ç”¨**ï¼šå®šä¹‰å’Œæ‰§è¡Œå¤šä»£ç†ç ”ç©¶å·¥ä½œæµ

### 5. ä»¤ç‰Œè®¡æ•°å’Œæˆæœ¬ä¼°ç®—ç®—æ³•

costs.py

gpt_researcher/utils

Copy

defÂ estimate_llm_cost(input_content:Â str,Â output_content:Â str)Â 

->Â float:

Â Â Â Â encodingÂ =Â tiktoken.get_encoding(ENCODING_MODEL)

Â Â Â Â input_tokensÂ =Â encoding.encode(input_content)

Â Â Â Â output_tokensÂ =Â encoding.encode(output_content)

Â Â Â Â input_costsÂ =Â len(input_tokens)Â *Â INPUT_COST_PER_TOKEN

Â Â Â Â output_costsÂ =Â len(output_tokens)Â *Â OUTPUT_COST_PER_TOKEN

Â Â Â Â returnÂ input_costsÂ +Â output_costs

- **å®ç°**ï¼šä½¿ç”¨ tiktoken åº“è®¡ç®—è¾“å…¥å’Œè¾“å‡ºçš„ä»¤ç‰Œæ•°ï¼Œå¹¶ä¼°ç®—æˆæœ¬
- **å¤æ‚åº¦**ï¼šO(n) å…¶ä¸­ n æ˜¯æ–‡æœ¬é•¿åº¦
- **åº”ç”¨**ï¼šè·Ÿè¸ªå’Œä¼˜åŒ– LLM API è°ƒç”¨æˆæœ¬

### 6. é€Ÿç‡é™åˆ¶ç®—æ³•

base.py

gpt_researcher/llm_provider/generic

Copy

rate_limiterÂ =Â InMemoryRateLimiter(

Â Â Â Â requests_per_second=rps,

Â Â Â Â check_every_n_seconds=0.1,

Â Â Â Â max_bucket_size=10,

)

- **å®ç°**ï¼šä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•å®ç° API è°ƒç”¨çš„é€Ÿç‡é™åˆ¶
- **å¤æ‚åº¦**ï¼šO(1) ç”¨äºæ£€æŸ¥å’Œæ›´æ–°ä»¤ç‰Œæ¡¶
- **åº”ç”¨**ï¼šé˜²æ­¢è¶…è¿‡ API æä¾›å•†çš„é€Ÿç‡é™åˆ¶ï¼Œé¿å…è¯·æ±‚å¤±è´¥

## æ€»ç»“

GPT Researcher é¡¹ç›®å·§å¦™åœ°ç»“åˆäº†å¤šç§è®¾è®¡æ¨¡å¼å’Œç®—æ³•ï¼Œå®ç°äº†ä¸€ä¸ªçµæ´»ã€å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è‡ªåŠ¨åŒ–ç ”ç©¶ç³»ç»Ÿï¼š

1. **è®¾è®¡æ¨¡å¼**ä¾§é‡äºç³»ç»Ÿæ¶æ„ï¼Œæä¾›äº†æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„æ¡†æ¶
2. **ç®—æ³•**ä¼˜åŒ–äº†å…·ä½“æ“ä½œï¼Œç¡®ä¿ç³»ç»Ÿé«˜æ•ˆå¤„ç†å¤§é‡æ–‡æœ¬å’Œå¤æ‚å·¥ä½œæµ
3. **ç»„åˆä½¿ç”¨**è¿™äº›æ¨¡å¼å’Œç®—æ³•ä½¿ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å„ç§ç ”ç©¶ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒä»£ç çš„å¯ç»´æŠ¤æ€§

è¿™ç§è®¾è®¡ä½¿ GPT Researcher ä¸ä»…èƒ½å¤Ÿå®Œæˆå½“å‰çš„ç ”ç©¶ä»»åŠ¡ï¼Œè¿˜èƒ½å¤Ÿè½»æ¾é€‚åº”æœªæ¥çš„æ‰©å±•å’Œæ”¹è¿›ã€‚

---

# GPT Researcher é¡¹ç›®å­¦ä¹ è·¯çº¿å›¾

åŸºäºå¯¹é¡¹ç›®çš„æ€»ä½“åˆ†æå’Œè®¾è®¡æ¨¡å¼/ç®—æ³•ç†è§£ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„å­¦ä¹ è·¯çº¿å›¾ï¼Œå¸®åŠ©æ‚¨æœ€å¿«æŒæ¡ GPT Researcher é¡¹ç›®ã€‚

## é˜¶æ®µ 1: åŸºç¡€æ¶æ„ç†è§£ï¼ˆ1-2 å¤©ï¼‰

### ä»»åŠ¡ 1.1: æ ¸å¿ƒç»„ä»¶åˆ†æ

- ç ”ç©¶Â 
    
    Â `gpt_researcher`
    
    Â ä¸»æ¨¡å—ç»“æ„
- ç†è§£Â `GPTResearcher`Â ç±»çš„åˆå§‹åŒ–å’Œä¸»è¦æ–¹æ³•
- åˆ†æé…ç½®ç³»ç»Ÿå’Œç¯å¢ƒå˜é‡è®¾ç½®

### ä»»åŠ¡ 1.2: LLM é›†æˆæœºåˆ¶

- ç ”ç©¶Â `llm_provider`Â æ¨¡å—
- äº†è§£ä¸åŒ LLM æä¾›å•†çš„é›†æˆæ–¹å¼
- åˆ†æ API è°ƒç”¨ã€æµå¼å¤„ç†å’Œé”™è¯¯å¤„ç†æœºåˆ¶

### ä»»åŠ¡ 1.3: æœç´¢å’Œçˆ¬è™«ç³»ç»Ÿ

- ç ”ç©¶Â `scraper`Â æ¨¡å—å’Œå„ç§çˆ¬è™«å®ç°
- äº†è§£æœç´¢å¼•æ“é›†æˆï¼ˆTavilyã€Google ç­‰ï¼‰
- åˆ†æç½‘é¡µå†…å®¹æå–å’Œå¤„ç†æµç¨‹

## é˜¶æ®µ 2: ç ”ç©¶æµç¨‹æ·±å…¥ï¼ˆ2-3 å¤©ï¼‰

### ä»»åŠ¡ 2.1: åŸºæœ¬ç ”ç©¶æµç¨‹

- è·Ÿè¸ªÂ `conduct_research`Â æ–¹æ³•çš„å®Œæ•´æ‰§è¡Œæµç¨‹
- åˆ†ææŸ¥è¯¢å¤„ç†å’Œå­æŸ¥è¯¢ç”Ÿæˆé€»è¾‘
- äº†è§£ä¸Šä¸‹æ–‡ç®¡ç†å’Œä¿¡æ¯èšåˆæœºåˆ¶

### ä»»åŠ¡ 2.2: æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ

- ç ”ç©¶Â `report_type`Â æ¨¡å—ä¸­çš„ä¸åŒæŠ¥å‘Šç±»å‹
- åˆ†æÂ `write_report`Â æ–¹æ³•çš„å®ç°
- äº†è§£æ ¼å¼åŒ–ã€å¼•ç”¨å’Œå›¾åƒå¤„ç†æœºåˆ¶

### ä»»åŠ¡ 2.3: æ„å»ºç®€å•ç ”ç©¶ä»£ç†

- åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„ç ”ç©¶è„šæœ¬ï¼Œä½¿ç”¨ GPT Researcher çš„æ ¸å¿ƒåŠŸèƒ½
- æµ‹è¯•ä¸åŒçš„æŸ¥è¯¢ç±»å‹å’ŒæŠ¥å‘Šæ ¼å¼
- åˆ†æç»“æœè´¨é‡å’Œæ€§èƒ½

## é˜¶æ®µ 3: å¤šä»£ç†ç³»ç»Ÿï¼ˆ2-3 å¤©ï¼‰

### ä»»åŠ¡ 3.1: LangGraph æ¡†æ¶ç†è§£

- ç ”ç©¶Â 
    
    Â `multi_agents`
    
    Â æ¨¡å—ç»“æ„
- äº†è§£Â `StateGraph`Â çš„å·¥ä½œåŸç†
- åˆ†æèŠ‚ç‚¹å’Œè¾¹çš„å®šä¹‰æ–¹å¼

### ä»»åŠ¡ 3.2: ä»£ç†è§’è‰²å’ŒèŒè´£

- ç ”ç©¶å„ä¸ªä»£ç†ç±»ï¼ˆEditorã€Researcherã€Reviewer ç­‰ï¼‰
- åˆ†æä»£ç†é—´çš„é€šä¿¡å’ŒçŠ¶æ€ä¼ é€’
- äº†è§£æ¡ä»¶è½¬æ¢å’Œå¹¶è¡Œæ‰§è¡Œæœºåˆ¶

### ä»»åŠ¡ 3.3: æ„å»ºè‡ªå®šä¹‰å¤šä»£ç†å·¥ä½œæµ

- åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„å¤šä»£ç†ç ”ç©¶å·¥ä½œæµ
- æµ‹è¯•ä¸åŒçš„ä»£ç†é…ç½®å’Œå·¥ä½œæµç»“æ„
- åˆ†æç»“æœè´¨é‡å’Œæ‰§è¡Œæ•ˆç‡

## é˜¶æ®µ 4: å‰ç«¯å’Œç”¨æˆ·äº¤äº’ï¼ˆ1-2 å¤©ï¼‰

### ä»»åŠ¡ 4.1: åŸºæœ¬å‰ç«¯ç†è§£

- ç ”ç©¶ HTML/JS ç®€å•å‰ç«¯çš„å®ç°
- äº†è§£ WebSocket é€šä¿¡æœºåˆ¶
- åˆ†æç”¨æˆ·è¾“å…¥å¤„ç†å’Œç»“æœå±•ç¤º

### ä»»åŠ¡ 4.2: NextJS å‰ç«¯æ·±å…¥

- ç ”ç©¶ NextJS åº”ç”¨çš„ç»„ä»¶ç»“æ„
- åˆ†æçŠ¶æ€ç®¡ç†å’Œæ•°æ®æµ
- äº†è§£å®æ—¶æ›´æ–°å’Œè¿›åº¦æ˜¾ç¤ºæœºåˆ¶

### ä»»åŠ¡ 4.3: æ„å»ºè‡ªå®šä¹‰ UI

- åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ç ”ç©¶ç•Œé¢
- å®ç°åŸºæœ¬çš„ç”¨æˆ·äº¤äº’åŠŸèƒ½
- æµ‹è¯•ä¸åŒçš„ç ”ç©¶åœºæ™¯å’Œç»“æœå±•ç¤º

## é˜¶æ®µ 5: é«˜çº§åŠŸèƒ½å’Œæ‰©å±•ï¼ˆ2-3 å¤©ï¼‰

### ä»»åŠ¡ 5.1: æ·±åº¦ç ”ç©¶æ¨¡å¼

- ç ”ç©¶Â `deep_research`Â æ¨¡å—çš„å®ç°
- åˆ†ææ·±åº¦å’Œå¹¿åº¦å‚æ•°çš„å½±å“
- äº†è§£è¿›åº¦è·Ÿè¸ªå’Œæˆæœ¬ä¼˜åŒ–æœºåˆ¶

### ä»»åŠ¡ 5.2: è¯¦ç»†æŠ¥å‘Šç³»ç»Ÿ

- ç ”ç©¶Â `detailed_report`Â æ¨¡å—çš„å®ç°
- åˆ†æå­ä¸»é¢˜ç”Ÿæˆå’Œå¹¶è¡Œç ”ç©¶æœºåˆ¶
- äº†è§£æŠ¥å‘Šç»“æ„åŒ–å’Œæ•´åˆæµç¨‹

### ä»»åŠ¡ 5.3: è‡ªå®šä¹‰æ‰©å±•å¼€å‘

- åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çˆ¬è™«æˆ– LLM æä¾›å•†é€‚é…å™¨
- å®ç°ä¸€ä¸ªæ–°çš„æŠ¥å‘Šç±»å‹æˆ–ç ”ç©¶æµç¨‹
- æµ‹è¯•æ‰©å±•çš„é›†æˆå’Œæ€§èƒ½

## é˜¶æ®µ 6: éƒ¨ç½²å’Œä¼˜åŒ–ï¼ˆ1-2 å¤©ï¼‰

### ä»»åŠ¡ 6.1: æœ¬åœ°éƒ¨ç½²

- è®¾ç½®å®Œæ•´çš„æœ¬åœ°å¼€å‘ç¯å¢ƒ
- é…ç½®æ‰€æœ‰å¿…è¦çš„ API å¯†é’¥å’ŒæœåŠ¡
- æµ‹è¯•ç«¯åˆ°ç«¯çš„ç ”ç©¶æµç¨‹

### ä»»åŠ¡ 6.2: Docker éƒ¨ç½²

- ç ”ç©¶ Docker é…ç½®å’Œæ„å»ºæµç¨‹
- åˆ›å»ºè‡ªå®šä¹‰ Docker é•œåƒ
- æµ‹è¯•å®¹å™¨åŒ–éƒ¨ç½²çš„æ€§èƒ½

### ä»»åŠ¡ 6.3: æ€§èƒ½ä¼˜åŒ–

- åˆ†æç³»ç»Ÿç“¶é¢ˆå’Œèµ„æºä½¿ç”¨
- å®ç°ç¼“å­˜å’Œå¹¶è¡Œå¤„ç†ä¼˜åŒ–
- æµ‹è¯•ä¼˜åŒ–åçš„æ€§èƒ½æå‡

## å®è·µé¡¹ç›®å»ºè®®

å®Œæˆä¸Šè¿°å­¦ä¹ è·¯å¾„åï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹å®è·µé¡¹ç›®æ¥å·©å›ºçŸ¥è¯†ï¼š

1. **ä¸“ä¸šé¢†åŸŸç ”ç©¶åŠ©æ‰‹**ï¼šä¸ºç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦ã€æ³•å¾‹ã€é‡‘èï¼‰å®šåˆ¶ç ”ç©¶æµç¨‹å’ŒæŠ¥å‘Šæ ¼å¼
2. **å¤šè¯­è¨€ç ”ç©¶ç³»ç»Ÿ**ï¼šæ‰©å±•ç³»ç»Ÿæ”¯æŒå¤šè¯­è¨€æŸ¥è¯¢å’ŒæŠ¥å‘Šç”Ÿæˆ
3. **äº¤äº’å¼ç ”ç©¶å¯¹è¯**ï¼šå®ç°ä¸€ä¸ªå¯¹è¯å¼ç ”ç©¶ç•Œé¢ï¼Œå…è®¸ç”¨æˆ·åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­æä¾›åé¦ˆå’Œè°ƒæ•´æ–¹å‘
4. **ç ”ç©¶è´¨é‡è¯„ä¼°ç³»ç»Ÿ**ï¼šå¼€å‘ä¸€ä¸ªæ¨¡å—æ¥è¯„ä¼°ç ”ç©¶ç»“æœçš„è´¨é‡ã€å…¨é¢æ€§å’Œå‡†ç¡®æ€§
5. **æœ¬åœ°çŸ¥è¯†åº“é›†æˆ**ï¼šæ‰©å±•ç³»ç»Ÿä»¥ç»“åˆæœ¬åœ°çŸ¥è¯†åº“å’Œç½‘ç»œæœç´¢ç»“æœ

## å­¦ä¹ èµ„æº

åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹èµ„æºï¼š

1. **é¡¹ç›®æ–‡æ¡£**ï¼š
    
    Â `docs`
    
    Â ç›®å½•ä¸­çš„è¯¦ç»†æ–‡æ¡£
2. **ç¤ºä¾‹ä»£ç **ï¼š`examples`Â ç›®å½•ä¸­çš„ç¤ºä¾‹è„šæœ¬
3. **LangChain æ–‡æ¡£**ï¼šäº†è§£ LangChain å’Œ LangGraph çš„åŸºç¡€çŸ¥è¯†
4. **ç›¸å…³è®ºæ–‡**ï¼šç‰¹åˆ«æ˜¯ STORM è®ºæ–‡ï¼Œäº†è§£å¤šä»£ç†ç³»ç»Ÿçš„ç†è®ºåŸºç¡€
5. **ç¤¾åŒºè®¨è®º**ï¼šGitHub Issues å’Œ Discussions ä¸­çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

æŒ‰ç…§è¿™ä¸ªè·¯çº¿å›¾ï¼Œæ‚¨å¯ä»¥åœ¨çº¦ 9-15 å¤©å†…å…¨é¢æŒæ¡ GPT Researcher é¡¹ç›®ï¼Œä»åŸºç¡€ç»„ä»¶åˆ°é«˜çº§åŠŸèƒ½ï¼Œå†åˆ°å®é™…éƒ¨ç½²å’Œä¼˜åŒ–ã€‚

---
# GPT Researcher çš„Â `conduct_research`Â æ–¹æ³•æ‰§è¡Œæµç¨‹åˆ†æ

## 1. æ–¹æ³•æ¦‚è¿°

`conduct_research` æ˜¯ GPT Researcher çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè´Ÿè´£æ‰§è¡Œæ•´ä¸ªç ”ç©¶è¿‡ç¨‹ï¼Œä»æ¥æ”¶æŸ¥è¯¢åˆ°ç”Ÿæˆç ”ç©¶ä¸Šä¸‹æ–‡ã€‚ä»¥ä¸‹æ˜¯å¯¹å…¶å®Œæ•´æ‰§è¡Œæµç¨‹çš„è¯¦ç»†åˆ†æã€‚

## 2. æ‰§è¡Œæµç¨‹è·Ÿè¸ª

### 2.1 å…¥å£ç‚¹åˆ†æ

agent.py

gpt_researcher

Copy

asyncÂ defÂ conduct_research(self,Â on_progress=None):

Â Â Â Â awaitÂ self._log_event("research",Â step="start",Â details={

Â Â Â Â Â Â Â Â "query":Â self.query,

Â Â Â Â Â Â Â Â "report_type":Â self.report_type,

Â Â Â Â Â Â Â Â "agent":Â self.agent,

Â Â Â Â Â Â Â Â "role":Â self.role

Â Â Â Â })

Â Â Â Â #Â HandleÂ deepÂ researchÂ separately

Â Â Â Â ifÂ self.report_typeÂ ==Â ReportType.DeepResearch.valueÂ andÂ 

Â Â Â Â self.deep_researcher:

Â Â Â Â Â Â Â Â returnÂ awaitÂ self._handle_deep_research(on_progress)

Â Â Â Â ifÂ notÂ (self.agentÂ andÂ self.role):

Â Â Â Â Â Â Â Â awaitÂ self._log_event("action",Â action="choose_agent")

Â Â Â Â Â Â Â Â self.agent,Â self.roleÂ =Â awaitÂ choose_agent(

Â Â Â Â Â Â Â Â Â Â Â Â query=self.query,

Â Â Â Â Â Â Â Â Â Â Â Â cfg=self.cfg,

Â Â Â Â Â Â Â Â Â Â Â Â parent_query=self.parent_query,

Â Â Â Â Â Â Â Â Â Â Â Â cost_callback=self.add_costs,

Â Â Â Â Â Â Â Â Â Â Â Â headers=self.headers,

Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â awaitÂ self._log_event("action",Â 

Â Â Â Â Â Â Â Â action="agent_selected",Â details={

Â Â Â Â Â Â Â Â Â Â Â Â "agent":Â self.agent,

Â Â Â Â Â Â Â Â Â Â Â Â "role":Â self.role

Â Â Â Â Â Â Â Â })

Â Â Â Â awaitÂ self._log_event("research",Â 

Â Â Â Â step="conducting_research",Â details={

Â Â Â Â Â Â Â Â "agent":Â self.agent,

Â Â Â Â Â Â Â Â "role":Â self.role

Â Â Â Â })

Â Â Â Â self.contextÂ =Â awaitÂ self.research_conductor.

Â Â Â Â conduct_research()

Â Â Â Â awaitÂ self._log_event("research",Â 

Â Â Â Â step="research_completed",Â details={

Â Â Â Â Â Â Â Â "context_length":Â len(self.context)

Â Â Â Â })

Â Â Â Â returnÂ self.context

æ–¹æ³•æ‰§è¡Œçš„ä¸»è¦æ­¥éª¤ï¼š

1. è®°å½•ç ”ç©¶å¼€å§‹äº‹ä»¶
2. æ£€æŸ¥æ˜¯å¦ä¸ºæ·±åº¦ç ”ç©¶æ¨¡å¼
3. å¦‚æœéœ€è¦ï¼Œé€‰æ‹©åˆé€‚çš„ä»£ç†å’Œè§’è‰²
4. è°ƒç”¨Â `research_conductor.conduct_research()`Â æ‰§è¡Œå®é™…ç ”ç©¶
5. è®°å½•ç ”ç©¶å®Œæˆäº‹ä»¶å¹¶è¿”å›ä¸Šä¸‹æ–‡

### 2.2 æ·±åº¦ç ”ç©¶å¤„ç†

å½“æŠ¥å‘Šç±»å‹ä¸º `DeepResearch` æ—¶ï¼Œä¼šè°ƒç”¨ä¸“é—¨çš„å¤„ç†æ–¹æ³•ï¼š

deep_research.py

gpt_researcher/skills

Copy

asyncÂ defÂ run(self,Â on_progress=None)Â ->Â str:

Â Â Â Â """RunÂ theÂ deepÂ researchÂ processÂ andÂ generateÂ finalÂ 

Â Â Â Â report"""

Â Â Â Â start_timeÂ =Â time.time()

Â Â Â Â #Â LogÂ initialÂ costs

Â Â Â Â initial_costsÂ =Â self.researcher.get_costs()

Â Â Â Â follow_up_questionsÂ =Â awaitÂ self.generate_research_plan

Â Â Â Â (self.researcher.query)

Â Â Â Â answersÂ =Â ["AutomaticallyÂ proceedingÂ withÂ research"]Â *Â len

Â Â Â Â (follow_up_questions)

Â Â Â Â qa_pairsÂ =Â [f"Q:Â {q}\nA:Â {a}"Â forÂ q,Â aÂ inÂ zip
<!--ID: 1761111102176-->


Â Â Â Â (follow_up_questions,Â answers)]

Â Â Â Â combined_queryÂ =Â f"""

Â Â Â Â InitialÂ Query:Â {self.researcher.query}\nFollowÂ -Â upÂ 
<!--ID: 1761111102183-->


Â Â Â Â QuestionsÂ andÂ Answers:\n

Â Â Â Â """Â +Â "\n".join(qa_pairs)

Â Â Â Â resultsÂ =Â awaitÂ self.deep_research(

Â Â Â Â Â Â Â Â query=combined_query,

Â Â Â Â Â Â Â Â breadth=self.breadth,

Â Â Â Â Â Â Â Â depth=self.depth,

Â Â Â Â Â Â Â Â on_progress=on_progress

Â Â Â Â )

Â Â Â Â #Â ...Â å¤„ç†ç»“æœå’Œè¿”å›ä¸Šä¸‹æ–‡

Â Â Â Â returnÂ self.researcher.context

æ·±åº¦ç ”ç©¶æµç¨‹ï¼š

1. ç”Ÿæˆç ”ç©¶è®¡åˆ’å’Œè·Ÿè¿›é—®é¢˜
2. æ„å»ºå¢å¼ºæŸ¥è¯¢
3. æ‰§è¡Œæ·±åº¦ç ”ç©¶ï¼ŒæŒ‡å®šå¹¿åº¦å’Œæ·±åº¦å‚æ•°
4. å¤„ç†ç»“æœå¹¶è¿”å›ä¸Šä¸‹æ–‡

### 2.3 æ ‡å‡†ç ”ç©¶æµç¨‹

æ ‡å‡†ç ”ç©¶æµç¨‹ç”± `Researcher` ç±»çš„ `conduct_research` æ–¹æ³•å®ç°ï¼š

researcher.py

gpt_researcher/skills

Copy

asyncÂ defÂ conduct_research(self):

Â Â Â Â """RunsÂ theÂ GPTÂ ResearcherÂ toÂ conductÂ research"""

Â Â Â Â ifÂ self.json_handler:

Â Â Â Â Â Â Â Â self.json_handler.update_content("query",Â self.

Â Â Â Â Â Â Â Â researcher.query)

Â Â Â Â self.logger.info(f"StartingÂ researchÂ forÂ query:Â {self.

Â Â Â Â researcher.query}")

Â Â Â Â #Â ResetÂ visited_urlsÂ andÂ source_urlsÂ atÂ theÂ startÂ ofÂ eachÂ 

Â Â Â Â researchÂ task

Â Â Â Â self.researcher.visited_urls.clear()

Â Â Â Â research_dataÂ =Â []

Â Â Â Â ifÂ self.researcher.verbose:

Â Â Â Â Â Â Â Â awaitÂ stream_output(

Â Â Â Â Â Â Â Â Â Â Â Â "logs",

Â Â Â Â Â Â Â Â Â Â Â Â "starting_research",

Â Â Â Â Â Â Â Â Â Â Â Â f"ğŸ”Â StartingÂ theÂ researchÂ taskÂ forÂ '{self.

Â Â Â Â Â Â Â Â Â Â Â Â researcher.query}'...",

Â Â Â Â Â Â Â Â Â Â Â Â self.researcher.websocket,

Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â awaitÂ stream_output(

Â Â Â Â Â Â Â Â Â Â Â Â "logs",

Â Â Â Â Â Â Â Â Â Â Â Â "agent_generated",

Â Â Â Â Â Â Â Â Â Â Â Â self.researcher.agent,

Â Â Â Â Â Â Â Â Â Â Â Â self.researcher.websocket

Â Â Â Â Â Â Â Â )

Â Â Â Â #Â ResearchÂ forÂ relevantÂ sourcesÂ basedÂ onÂ sourceÂ typesÂ below

Â Â Â Â ifÂ self.researcher.source_urls:

Â Â Â Â Â Â Â Â self.logger.info("UsingÂ providedÂ sourceÂ URLs")

Â Â Â Â Â Â Â Â research_dataÂ =Â awaitÂ self._get_context_by_urls(self.

Â Â Â Â Â Â Â Â researcher.source_urls)

Â Â Â Â Â Â Â Â #Â ...Â å¤„ç†æºÂ URLÂ å’Œè¡¥å……æœç´¢

Â Â Â Â elifÂ self.researcher.report_sourceÂ ==Â ReportSource.Web.

Â Â Â Â value:

Â Â Â Â Â Â Â Â self.logger.info("UsingÂ webÂ search")

Â Â Â Â Â Â Â Â research_dataÂ =Â awaitÂ self._get_context_by_web_search

Â Â Â Â Â Â Â Â (self.researcher.query,Â [],Â self.researcher.

Â Â Â Â Â Â Â Â query_domains)

Â Â Â Â elifÂ self.researcher.report_sourceÂ ==Â ReportSource.Local.

Â Â Â Â value:

Â Â Â Â Â Â Â Â self.logger.info("UsingÂ localÂ search")

Â Â Â Â Â Â Â Â #Â ...Â å¤„ç†æœ¬åœ°æ–‡æ¡£æœç´¢

Â Â Â Â #Â RankÂ andÂ curateÂ theÂ sources

Â Â Â Â self.researcher.contextÂ =Â research_data

Â Â Â Â ifÂ self.researcher.cfg.curate_sources:

Â Â Â Â Â Â Â Â self.logger.info("CuratingÂ sources")

Â Â Â Â Â Â Â Â self.researcher.contextÂ =Â awaitÂ self.researcher.

Â Â Â Â Â Â Â Â source_curator.curate_sources(research_data)

Â Â Â Â #Â ...Â è®°å½•å’Œè¿”å›ç»“æœ

Â Â Â Â returnÂ self.researcher.context

æ ‡å‡†ç ”ç©¶æµç¨‹ï¼š

1. åˆå§‹åŒ–ç ”ç©¶ç¯å¢ƒå’Œè®°å½•
2. æ ¹æ®é…ç½®é€‰æ‹©ç ”ç©¶æ¥æºï¼ˆæä¾›çš„ URLã€ç½‘ç»œæœç´¢æˆ–æœ¬åœ°æ–‡æ¡£ï¼‰
3. è·å–å’Œå¤„ç†ç ”ç©¶æ•°æ®
4. å¯é€‰åœ°å¯¹æ¥æºè¿›è¡Œæ’åå’Œç­›é€‰
5. è¿”å›ç ”ç©¶ä¸Šä¸‹æ–‡

## 3. æŸ¥è¯¢å¤„ç†å’Œå­æŸ¥è¯¢ç”Ÿæˆé€»è¾‘

### 3.1 å­æŸ¥è¯¢ç”Ÿæˆ

å­æŸ¥è¯¢ç”Ÿæˆæ˜¯é€šè¿‡ `plan_research` æ–¹æ³•å®ç°çš„ï¼š

researcher.py

gpt_researcher/skills

Copy

asyncÂ defÂ plan_research(self,Â query,Â query_domains=None):

Â Â Â Â self.logger.info(f"PlanningÂ researchÂ forÂ query:Â {query}")
<!--ID: 1761111102199-->


Â Â Â Â ifÂ query_domains:

Â Â Â Â Â Â Â Â self.logger.info(f"QueryÂ domains:Â {query_domains}")
<!--ID: 1761111102216-->


Â Â Â Â awaitÂ stream_output(

Â Â Â Â Â Â Â Â "logs",

Â Â Â Â Â Â Â Â "planning_research",

Â Â Â Â Â Â Â Â f"ğŸŒÂ BrowsingÂ theÂ webÂ toÂ learnÂ moreÂ aboutÂ theÂ task:Â 

Â Â Â Â Â Â Â Â {query}...",
<!--ID: 1761111102224-->


Â Â Â Â Â Â Â Â self.researcher.websocket,

Â Â Â Â )

Â Â Â Â search_resultsÂ =Â awaitÂ get_search_results(query,Â self.

Â Â Â Â researcher.retrievers[0],Â query_domains)

Â Â Â Â self.logger.info(f"InitialÂ searchÂ resultsÂ obtained:Â {len

Â Â Â Â (search_results)}Â results")

Â Â Â Â awaitÂ stream_output(

Â Â Â Â Â Â Â Â "logs",

Â Â Â Â Â Â Â Â "planning_research",

Â Â Â Â Â Â Â Â f"ğŸ¤”Â PlanningÂ theÂ researchÂ strategyÂ andÂ subtasks...",

Â Â Â Â Â Â Â Â self.researcher.websocket,

Â Â Â Â )

Â Â Â Â outlineÂ =Â awaitÂ plan_research_outline(

Â Â Â Â Â Â Â Â query=query,

Â Â Â Â Â Â Â Â search_results=search_results,

Â Â Â Â Â Â Â Â agent_role_prompt=self.researcher.role,

Â Â Â Â Â Â Â Â cfg=self.researcher.cfg,

Â Â Â Â Â Â Â Â parent_query=self.researcher.parent_query,

Â Â Â Â Â Â Â Â report_type=self.researcher.report_type,

Â Â Â Â Â Â Â Â cost_callback=self.researcher.add_costs,

Â Â Â Â )

Â Â Â Â self.logger.info(f"ResearchÂ outlineÂ planned:Â {outline}")
<!--ID: 1761111102238-->


Â Â Â Â returnÂ outline

å­æŸ¥è¯¢ç”Ÿæˆæµç¨‹ï¼š

1. æ‰§è¡Œåˆå§‹æœç´¢ä»¥è·å–åŸºæœ¬ä¸Šä¸‹æ–‡
2. ä½¿ç”¨Â `plan_research_outline`Â å‡½æ•°ç”Ÿæˆç ”ç©¶å¤§çº²
3. å¤§çº²ä¸­çš„æ¯ä¸ªé¡¹ç›®æˆä¸ºä¸€ä¸ªå­æŸ¥è¯¢

### 3.2 å­æŸ¥è¯¢ç”Ÿæˆçš„åº•å±‚å®ç°

å­æŸ¥è¯¢ç”Ÿæˆçš„æ ¸å¿ƒé€»è¾‘åœ¨

`query_processing.py`

ä¸­ï¼š

query_processing.py

gpt_researcher/actions

Copy

asyncÂ defÂ generate_sub_queries(

Â Â Â Â query:Â str,

Â Â Â Â parent_query:Â str,

Â Â Â Â report_type:Â str,

Â Â Â Â context:Â List[Dict[str,Â Any]],

Â Â Â Â cfg:Â Config,

Â Â Â Â cost_callback:Â callableÂ =Â None

)Â ->Â List[str]:

Â Â Â Â """

Â Â Â Â GenerateÂ sub-queriesÂ usingÂ theÂ specifiedÂ LLMÂ model.

Â Â Â Â """

Â Â Â Â gen_queries_promptÂ =Â generate_search_queries_prompt(

Â Â Â Â Â Â Â Â query,

Â Â Â Â Â Â Â Â parent_query,

Â Â Â Â Â Â Â Â report_type,

Â Â Â Â Â Â Â Â max_iterations=cfg.max_iterationsÂ orÂ 3,

Â Â Â Â Â Â Â Â context=context

Â Â Â Â )

Â Â Â Â #Â ...Â LLMÂ è°ƒç”¨å’Œå¤„ç†

Â Â Â Â returnÂ sub_queries

asyncÂ defÂ plan_research_outline(

Â Â Â Â query:Â str,

Â Â Â Â search_results:Â List[Dict[str,Â Any]],

Â Â Â Â agent_role_prompt:Â str,

Â Â Â Â cfg:Â Config,

Â Â Â Â parent_query:Â str,

Â Â Â Â report_type:Â str,

Â Â Â Â cost_callback:Â callableÂ =Â None,

)Â ->Â List[str]:

Â Â Â Â """

Â Â Â Â PlanÂ theÂ researchÂ outlineÂ byÂ generatingÂ sub-queries.

Â Â Â Â """

Â Â Â Â sub_queriesÂ =Â awaitÂ generate_sub_queries(

Â Â Â Â Â Â Â Â query,

Â Â Â Â Â Â Â Â parent_query,

Â Â Â Â Â Â Â Â report_type,

Â Â Â Â Â Â Â Â search_results,

Â Â Â Â Â Â Â Â cfg,

Â Â Â Â Â Â Â Â cost_callback

Â Â Â Â )

Â Â Â Â returnÂ sub_queries

å­æŸ¥è¯¢ç”Ÿæˆçš„å…³é”®ç‚¹ï¼š

1. ä½¿ç”¨åˆå§‹æŸ¥è¯¢å’Œæœç´¢ç»“æœä½œä¸ºè¾“å…¥
2. ç”Ÿæˆä¸“é—¨çš„æç¤ºä»¥å¼•å¯¼ LLM åˆ›å»ºå­æŸ¥è¯¢
3. è€ƒè™‘æŠ¥å‘Šç±»å‹å’Œçˆ¶æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
4. è¿”å›å­æŸ¥è¯¢åˆ—è¡¨ç”¨äºåç»­ç ”ç©¶

## 4. ä¸Šä¸‹æ–‡ç®¡ç†å’Œä¿¡æ¯èšåˆæœºåˆ¶

### 4.1 ç½‘ç»œæœç´¢å’Œä¸Šä¸‹æ–‡è·å–

researcher.py

gpt_researcher/skills

Copy

asyncÂ defÂ _get_context_by_web_search(self,Â query,Â 

scraped_data:Â listÂ |Â NoneÂ =Â None,Â query_domains:Â listÂ |Â NoneÂ =Â 

None):

Â Â Â Â """

Â Â Â Â GeneratesÂ theÂ contextÂ forÂ theÂ researchÂ taskÂ byÂ searchingÂ 

Â Â Â Â theÂ queryÂ andÂ scrapingÂ theÂ results

Â Â Â Â """

Â Â Â Â self.logger.info(f"StartingÂ webÂ searchÂ forÂ query:Â {query}")
<!--ID: 1761111102254-->


Â Â Â Â ifÂ scraped_dataÂ isÂ None:

Â Â Â Â Â Â Â Â scraped_dataÂ =Â []

Â Â Â Â ifÂ query_domainsÂ isÂ None:

Â Â Â Â Â Â Â Â query_domainsÂ =Â []

Â Â Â Â #Â GenerateÂ Sub-QueriesÂ includingÂ originalÂ query

Â Â Â Â sub_queriesÂ =Â awaitÂ self.plan_research(query,Â 

Â Â Â Â query_domains)

Â Â Â Â self.logger.info(f"GeneratedÂ sub-queries:Â {sub_queries}")
<!--ID: 1761111102263-->


Â Â Â Â #Â IfÂ thisÂ isÂ notÂ partÂ ofÂ aÂ subÂ researcher,Â addÂ originalÂ 

Â Â Â Â queryÂ toÂ researchÂ forÂ betterÂ results

Â Â Â Â ifÂ self.researcher.report_typeÂ !=Â "subtopic_report":

Â Â Â Â Â Â Â Â sub_queries.append(query)

Â Â Â Â #Â UsingÂ asyncio.gatherÂ toÂ processÂ theÂ sub_queriesÂ 

Â Â Â Â asynchronously

Â Â Â Â try:

Â Â Â Â Â Â Â Â contextÂ =Â awaitÂ asyncio.gather(

Â Â Â Â Â Â Â Â Â Â Â Â *[

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â self._process_sub_query(sub_query,Â 

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â scraped_data,Â query_domains)

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â forÂ sub_queryÂ inÂ sub_queries

Â Â Â Â Â Â Â Â Â Â Â Â ]

Â Â Â Â Â Â Â Â )

Â Â Â Â Â Â Â Â #Â ...Â å¤„ç†å’Œè¿”å›ç»“æœ

Â Â Â Â exceptÂ ExceptionÂ asÂ e:

Â Â Â Â Â Â Â Â self.logger.error(f"ErrorÂ duringÂ webÂ search:Â {e}",Â 
<!--ID: 1761111102277-->


Â Â Â Â Â Â Â Â exc_info=True)

Â Â Â Â Â Â Â Â returnÂ []

ç½‘ç»œæœç´¢æµç¨‹ï¼š

1. ç”Ÿæˆå­æŸ¥è¯¢åˆ—è¡¨
2. å¯¹æ¯ä¸ªå­æŸ¥è¯¢å¹¶è¡Œæ‰§è¡ŒÂ `_process_sub_query`
3. èšåˆæ‰€æœ‰å­æŸ¥è¯¢çš„ç»“æœ
4. å¤„ç†å¼‚å¸¸å¹¶è¿”å›ä¸Šä¸‹æ–‡

### 4.2 å­æŸ¥è¯¢å¤„ç†

researcher.py

gpt_researcher/skills

Copy

asyncÂ defÂ _process_sub_query(self,Â sub_query:Â str,Â 

scraped_data:Â listÂ =Â [],Â query_domains:Â listÂ =Â []):

Â Â Â Â """TakesÂ inÂ aÂ subÂ queryÂ andÂ scrapesÂ urlsÂ basedÂ onÂ itÂ andÂ 

Â Â Â Â gathersÂ context."""

Â Â Â Â ifÂ self.json_handler:

Â Â Â Â Â Â Â Â self.json_handler.log_event("sub_query",Â {

Â Â Â Â Â Â Â Â Â Â Â Â "query":Â sub_query,

Â Â Â Â Â Â Â Â Â Â Â Â "scraped_data_size":Â len(scraped_data)

Â Â Â Â Â Â Â Â })

Â Â Â Â ifÂ self.researcher.verbose:

Â Â Â Â Â Â Â Â awaitÂ stream_output(

Â Â Â Â Â Â Â Â Â Â Â Â "logs",

Â Â Â Â Â Â Â Â Â Â Â Â "running_subquery_research",

Â Â Â Â Â Â Â Â Â Â Â Â f"\nğŸ”Â RunningÂ researchÂ forÂ '{sub_query}'...",
<!--ID: 1761111102292-->


Â Â Â Â Â Â Â Â Â Â Â Â self.researcher.websocket,

Â Â Â Â Â Â Â Â )

Â Â Â Â try:

Â Â Â Â Â Â Â Â ifÂ notÂ scraped_data:

Â Â Â Â Â Â Â Â Â Â Â Â scraped_dataÂ =Â awaitÂ self._scrape_data_by_urls

Â Â Â Â Â Â Â Â Â Â Â Â (sub_query,Â query_domains)

Â Â Â Â Â Â Â Â Â Â Â Â self.logger.info(f"ScrapedÂ dataÂ size:Â {len

Â Â Â Â Â Â Â Â Â Â Â Â (scraped_data)}")

Â Â Â Â Â Â Â Â contentÂ =Â awaitÂ self.researcher.context_manager.

Â Â Â Â Â Â Â Â get_similar_content_by_query(sub_query,Â scraped_data)

Â Â Â Â Â Â Â Â self.logger.info(f"ContentÂ foundÂ forÂ sub-query:Â {len

Â Â Â Â Â Â Â Â (str(content))Â ifÂ contentÂ elseÂ 0}Â chars")

Â Â Â Â Â Â Â Â #Â ...Â å¤„ç†å’Œè¿”å›ç»“æœ

Â Â Â Â Â Â Â Â returnÂ content

Â Â Â Â exceptÂ ExceptionÂ asÂ e:

Â Â Â Â Â Â Â Â self.logger.error(f"ErrorÂ processingÂ sub-queryÂ 

Â Â Â Â Â Â Â Â {sub_query}:Â {e}",Â exc_info=True)
<!--ID: 1761111102309-->


Â Â Â Â Â Â Â Â returnÂ ""

å­æŸ¥è¯¢å¤„ç†æµç¨‹ï¼š

1. å¦‚æœæ²¡æœ‰é¢„å…ˆæŠ“å–çš„æ•°æ®ï¼Œåˆ™æŠ“å–ç›¸å…³ URL
2. ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–ä¸å­æŸ¥è¯¢ç›¸å…³çš„å†…å®¹
3. å¤„ç†å¼‚å¸¸å¹¶è¿”å›å†…å®¹

### 4.3 ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦å’Œç­›é€‰

ä¸Šä¸‹æ–‡ç®¡ç†å™¨è´Ÿè´£æ ¹æ®æŸ¥è¯¢ç­›é€‰ç›¸å…³å†…å®¹ï¼š

context_manager.py

gpt_researcher/skills

Copy

asyncÂ defÂ get_similar_content_by_query(self,Â query,Â pages):

Â Â Â Â ifÂ self.researcher.verbose:

Â Â Â Â Â Â Â Â awaitÂ stream_output(

Â Â Â Â Â Â Â Â Â Â Â Â "logs",

Â Â Â Â Â Â Â Â Â Â Â Â "fetching_query_content",

Â Â Â Â Â Â Â Â Â Â Â Â f"ğŸ“šÂ GettingÂ relevantÂ contentÂ basedÂ onÂ query:Â 

Â Â Â Â Â Â Â Â Â Â Â Â {query}...",
<!--ID: 1761113422309-->


Â Â Â Â Â Â Â Â Â Â Â Â self.researcher.websocket,

Â Â Â Â Â Â Â Â )

Â Â Â Â context_compressorÂ =Â ContextCompressor(

Â Â Â Â Â Â Â Â documents=pages,Â embeddings=self.researcher.memory.

Â Â Â Â Â Â Â Â get_embeddings()

Â Â Â Â )

Â Â Â Â returnÂ awaitÂ context_compressor.async_get_context(

Â Â Â Â Â Â Â Â query=query,Â max_results=10,Â cost_callback=self.

Â Â Â Â Â Â Â Â researcher.add_costs

Â Â Â Â )

ä¸Šä¸‹æ–‡ç­›é€‰æœºåˆ¶ï¼š

1. ä½¿ç”¨Â `ContextCompressor`Â ç±»å¤„ç†æ–‡æ¡£
2. åˆ©ç”¨åµŒå…¥å‘é‡è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸ä¼¼åº¦
3. é€‰æ‹©æœ€ç›¸å…³çš„å†…å®¹ï¼ˆæœ€å¤š 10 ä¸ªç»“æœï¼‰
4. è·Ÿè¸ªæˆæœ¬å¹¶è¿”å›ç­›é€‰åçš„ä¸Šä¸‹æ–‡

### 4.4 æ·±åº¦ç ”ç©¶ä¸­çš„ä¸Šä¸‹æ–‡èšåˆ

æ·±åº¦ç ”ç©¶æ¨¡å¼ä½¿ç”¨æ›´å¤æ‚çš„ä¸Šä¸‹æ–‡èšåˆæœºåˆ¶ï¼š

deep_research.py

gpt_researcher/skills

Copy

asyncÂ defÂ deep_research(

Â Â Â Â Â Â Â Â self,

Â Â Â Â Â Â Â Â query:Â str,

Â Â Â Â Â Â Â Â breadth:Â int,

Â Â Â Â Â Â Â Â depth:Â int,

Â Â Â Â Â Â Â Â learnings:Â List[str]Â =Â None,

Â Â Â Â Â Â Â Â citations:Â Dict[str,Â str]Â =Â None,

Â Â Â Â Â Â Â Â visited_urls:Â Set[str]Â =Â None,

Â Â Â Â Â Â Â Â on_progress=None

)Â ->Â Dict[str,Â Any]:

Â Â Â Â """ConductÂ deepÂ iterativeÂ research"""

Â Â Â Â #Â ...Â åˆå§‹åŒ–

Â Â Â Â #Â GenerateÂ searchÂ queries

Â Â Â Â serp_queriesÂ =Â awaitÂ self.generate_search_queries(query,Â 

Â Â Â Â num_queries=breadth)

Â Â Â Â progress.total_queriesÂ =Â len(serp_queries)

Â Â Â Â all_learningsÂ =Â learnings.copy()

Â Â Â Â all_citationsÂ =Â citations.copy()

Â Â Â Â all_visited_urlsÂ =Â visited_urls.copy()

Â Â Â Â all_contextÂ =Â []

Â Â Â Â all_sourcesÂ =Â []

Â Â Â Â #Â ProcessÂ queriesÂ concurrentlyÂ withÂ limit

Â Â Â Â tasksÂ =Â [process_query(query)Â forÂ queryÂ inÂ serp_queries]

Â Â Â Â resultsÂ =Â awaitÂ asyncio.gather(*tasks)

Â Â Â Â resultsÂ =Â [rÂ forÂ rÂ inÂ resultsÂ ifÂ rÂ isÂ notÂ None]

Â Â Â Â #Â UpdateÂ breadthÂ progressÂ basedÂ onÂ successfulÂ queries

Â Â Â Â progress.current_breadthÂ =Â len(results)

Â Â Â Â ifÂ on_progress:

Â Â Â Â Â Â Â Â on_progress(progress)

Â Â Â Â #Â CollectÂ allÂ results

Â Â Â Â forÂ resultÂ inÂ results:

Â Â Â Â Â Â Â Â all_learnings.extend(result['learnings'])

Â Â Â Â Â Â Â Â all_visited_urls.update(result['visited_urls'])

Â Â Â Â Â Â Â Â all_citations.update(result['citations'])

Â Â Â Â Â Â Â Â ifÂ result['context']:

Â Â Â Â Â Â Â Â Â Â Â Â all_context.append(result['context'])

Â Â Â Â Â Â Â Â ifÂ result['sources']:

Â Â Â Â Â Â Â Â Â Â Â Â all_sources.extend(result['sources'])

Â Â Â Â Â Â Â Â #Â ContinueÂ deeperÂ ifÂ needed

Â Â Â Â Â Â Â Â ifÂ depthÂ >Â 1:

Â Â Â Â Â Â Â Â Â Â Â Â #Â ...Â é€’å½’ç ”ç©¶

æ·±åº¦ç ”ç©¶çš„ä¸Šä¸‹æ–‡èšåˆï¼š

1. å¹¶è¡Œå¤„ç†å¤šä¸ªæŸ¥è¯¢
2. æ”¶é›†æ‰€æœ‰ç»“æœï¼ˆå­¦ä¹ ã€å¼•ç”¨ã€URLã€ä¸Šä¸‹æ–‡ã€æ¥æºï¼‰
3. å¦‚æœæ·±åº¦ > 1ï¼Œåˆ™é€’å½’è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„ç ”ç©¶
4. æœ€ç»ˆèšåˆæ‰€æœ‰å±‚æ¬¡çš„ç»“æœ

## 5. å…³é”®æœºåˆ¶æ€»ç»“

### 5.1 æŸ¥è¯¢å¤„ç†æœºåˆ¶

1. **åˆ†å±‚æŸ¥è¯¢ç»“æ„**ï¼š
    - åŸå§‹æŸ¥è¯¢ â†’ å­æŸ¥è¯¢ â†’ æœç´¢æŸ¥è¯¢
    - æ¯ä¸€å±‚éƒ½å¢åŠ ç‰¹å¼‚æ€§å’Œè¦†ç›–èŒƒå›´
2. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢ç”Ÿæˆ**ï¼š
    - ä½¿ç”¨åˆå§‹æœç´¢ç»“æœæŒ‡å¯¼å­æŸ¥è¯¢ç”Ÿæˆ
    - è€ƒè™‘æŠ¥å‘Šç±»å‹å’Œçˆ¶æŸ¥è¯¢
3. **å¹¶è¡ŒæŸ¥è¯¢å¤„ç†**ï¼š
    - ä½¿ç”¨Â `asyncio.gather`Â å¹¶è¡Œå¤„ç†å¤šä¸ªå­æŸ¥è¯¢
    - æé«˜ç ”ç©¶æ•ˆç‡å’Œè¦†ç›–èŒƒå›´

### 5.2 ä¸Šä¸‹æ–‡ç®¡ç†æœºåˆ¶

1. **å¤šæºä¸Šä¸‹æ–‡è·å–**ï¼š
    - ç½‘ç»œæœç´¢
    - æä¾›çš„ URL
    - æœ¬åœ°æ–‡æ¡£
    - å‘é‡å­˜å‚¨
2. **ç›¸ä¼¼åº¦ç­›é€‰**ï¼š
    - ä½¿ç”¨åµŒå…¥å‘é‡è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„ç›¸ä¼¼åº¦
    - é€‰æ‹©æœ€ç›¸å…³çš„å†…å®¹
3. **ä¸Šä¸‹æ–‡å‹ç¼©å’Œèšåˆ**ï¼š
    - åˆå¹¶å¤šä¸ªæ¥æºçš„ä¿¡æ¯
    - å»é™¤é‡å¤å’Œä¸ç›¸å…³å†…å®¹
    - ç¡®ä¿ä¸Šä¸‹æ–‡åœ¨ LLM å¤„ç†é™åˆ¶å†…

### 5.3 æ·±åº¦ç ”ç©¶ç‰¹æ€§

1. **å¹¿åº¦å’Œæ·±åº¦å‚æ•°**ï¼š
    - æ§åˆ¶ç ”ç©¶çš„å¹¿åº¦ï¼ˆå¹¶è¡ŒæŸ¥è¯¢æ•°ï¼‰
    - æ§åˆ¶ç ”ç©¶çš„æ·±åº¦ï¼ˆé€’å½’å±‚çº§ï¼‰
2. **å­¦ä¹ å’Œå¼•ç”¨è·Ÿè¸ª**ï¼š
    - ä»æ¯ä¸ªæ¥æºæå–å…³é”®å­¦ä¹ ç‚¹
    - ç»´æŠ¤å¼•ç”¨ä»¥ç¡®ä¿å¯è¿½æº¯æ€§
3. **è¿›åº¦è·Ÿè¸ª**ï¼š
    - ä½¿ç”¨Â `ResearchProgress`Â ç±»è·Ÿè¸ªç ”ç©¶è¿›åº¦
    - æ”¯æŒå®æ—¶è¿›åº¦æ›´æ–°

## 6. æ‰§è¡Œæµç¨‹å›¾

Copy

conduct_research
```
â”‚

â”œâ”€â”€Â å¦‚æœæ˜¯æ·±åº¦ç ”ç©¶

â”‚Â Â Â â””â”€â”€Â _handle_deep_research

â”‚Â Â Â Â Â Â Â â”œâ”€â”€Â ç”Ÿæˆç ”ç©¶è®¡åˆ’å’Œé—®é¢˜

â”‚Â Â Â Â Â Â Â â”œâ”€â”€Â æ‰§è¡Œæ·±åº¦ç ”ç©¶ï¼ˆbreadthÂ Ã—Â depthï¼‰

â”‚Â Â Â Â Â Â Â â””â”€â”€Â å¤„ç†ç»“æœå¹¶è¿”å›ä¸Šä¸‹æ–‡

â”‚

â””â”€â”€Â æ ‡å‡†ç ”ç©¶

Â Â Â Â â”œâ”€â”€Â é€‰æ‹©ä»£ç†å’Œè§’è‰²ï¼ˆå¦‚æœéœ€è¦ï¼‰

Â Â Â Â â””â”€â”€Â research_conductor.conduct_research

Â Â Â Â Â Â Â Â â”œâ”€â”€Â æ ¹æ®æ¥æºé€‰æ‹©ç ”ç©¶æ–¹æ³•

Â Â Â Â Â Â Â Â â”‚Â Â Â â”œâ”€â”€Â _get_context_by_urlsï¼ˆæä¾›çš„Â URLï¼‰

Â Â Â Â Â Â Â Â â”‚Â Â Â â”œâ”€â”€Â _get_context_by_web_searchï¼ˆç½‘ç»œæœç´¢ï¼‰

Â Â Â Â Â Â Â Â â”‚Â Â Â â””â”€â”€Â _get_context_by_vectorstoreï¼ˆå‘é‡å­˜å‚¨ï¼‰

Â Â Â Â Â Â Â Â â”‚

Â Â Â Â Â Â Â Â â”œâ”€â”€Â å¯¹äºç½‘ç»œæœç´¢

Â Â Â Â Â Â Â Â â”‚Â Â Â â”œâ”€â”€Â plan_researchÂ ç”Ÿæˆå­æŸ¥è¯¢

Â Â Â Â Â Â Â Â â”‚Â Â Â â”œâ”€â”€Â å¹¶è¡Œå¤„ç†æ¯ä¸ªå­æŸ¥è¯¢

Â Â Â Â Â Â Â Â â”‚Â Â Â â”‚Â Â Â â””â”€â”€Â _process_sub_query

Â Â Â Â Â Â Â Â â”‚Â Â Â â”‚Â Â Â Â Â Â Â â”œâ”€â”€Â _scrape_data_by_urlsÂ æŠ“å–æ•°æ®

Â Â Â Â Â Â Â Â â”‚Â Â Â â”‚Â Â Â Â Â Â Â â””â”€â”€Â context_manager.

Â Â Â Â Â Â Â Â get_similar_content_by_queryÂ ç­›é€‰å†…å®¹

Â Â Â Â Â Â Â Â â”‚Â Â Â â””â”€â”€Â èšåˆæ‰€æœ‰å­æŸ¥è¯¢ç»“æœ

Â Â Â Â Â Â Â Â â”‚

Â Â Â Â Â Â Â Â â”œâ”€â”€Â å¯é€‰åœ°ç­›é€‰å’Œ
Â Â Â Â Â Â Â Â 
```